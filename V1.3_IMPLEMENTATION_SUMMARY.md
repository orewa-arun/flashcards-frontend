# v1.3 Postgres Content Pipeline - Implementation Summary

## Overview

Successfully implemented the v1.3 automated content pipeline using PostgreSQL as the single source of truth. The system provides a modular, loosely-coupled architecture for managing course content generation with sequential manual triggers.

## Architecture

### Database Layer

**Location**: `backend/app/db/postgres.py`
- Async SQLAlchemy engine with connection pooling
- Async session management with dependency injection
- Automatic table creation on startup

**Models**: `backend/app/models/content.py`
- `Course` table: Stores course metadata with auto-incrementing ID
  - `course_code` as unique identifier (foreign key reference)
  - JSONB for flexible reference_textbooks storage
  - Timestamps for created_at and updated_at
  
- `Lecture` table: Stores lecture PDFs and generated content
  - Auto-incrementing ID as primary key
  - `course_code` as foreign key
  - JSONB columns for: structured_analysis, flashcards, quizzes
  - Status columns: analysis_status, flashcard_status, quiz_status, qdrant_status
  - JSONB error_log for detailed error tracking
  - Indexed status columns for efficient querying

### Services Layer

**Directory**: `backend/app/services/content_pipeline/`

All services are loosely coupled and can be run independently:

1. **ingestion.py**: R2 upload and database record creation
   - Handles course creation/updates
   - Uploads PDFs to Cloudflare R2 with sanitized filenames
   - Creates lecture records with 'pending' statuses

2. **structured_analysis.py**: Gemini 2.0 Pro analysis
   - Model configurable via `MODEL_ANALYSIS` env var
   - Fetches PDF from R2
   - Generates structured analysis
   - Updates status and handles errors

3. **flashcard_generation.py**: Claude 3 Haiku flashcard generation
   - Model configurable via `MODEL_FLASHCARDS` env var
   - Requires completed analysis_status
   - Generates flashcards from structured analysis
   - Updates status and handles errors

4. **quiz_generation.py**: Gemini 2.0 Flash quiz generation
   - Model configurable via `MODEL_QUIZ` env var
   - Requires completed flashcard_status
   - Generates quizzes in flexible array format: `[{"level": 1, "questions": [...]}, ...]`
   - Updates status and handles errors

5. **vector_indexing.py**: Qdrant vector indexing
   - Requires completed flashcard_status
   - Indexes flashcards to Qdrant
   - Updates status and handles errors

6. **orchestrator.py**: High-level facade
   - Provides unified interface for all pipeline actions
   - Routes requests to appropriate service
   - Centralizes action handling logic

### API Layer

**Router**: `backend/app/routers/content.py`

Endpoints:
- `POST /api/v1/content/ingest`: Upload course + PDFs
- `POST /api/v1/content/{action}/{lecture_id}`: Trigger pipeline actions
  - Actions: analyze, flashcards, quiz, index
- `GET /api/v1/content/courses`: List all courses
- `GET /api/v1/content/lectures`: List lectures with statuses (filterable by course_code)
- `GET /api/v1/content/lectures/{lecture_id}`: Get specific lecture details

### Integration

**Updated**: `backend/app/main.py`
- Added Postgres initialization on startup
- Added Postgres cleanup on shutdown
- Included content router in application

## Configuration

All AI models are configurable via environment variables in `backend/app/config.py`:
- `MODEL_ANALYSIS`: Default "gemini-2.0-pro-exp"
- `MODEL_FLASHCARDS`: Default "claude-3-haiku-20240307"
- `MODEL_QUIZ`: Default "gemini-2.0-flash-exp"

Cloudflare R2 configuration:
- `R2_ACCOUNT_ID`
- `R2_ACCESS_KEY_ID`
- `R2_SECRET_ACCESS_KEY`
- `R2_BUCKET_NAME`
- `R2_ENDPOINT_URL`

PostgreSQL configuration:
- `POSTGRES_URL`: Default "postgresql+asyncpg://postgres:postgres@localhost:5432/self_learning_ai"

## Workflow

1. **Ingestion** (Step 1):
   - User submits course metadata + PDF files
   - System creates course record (or updates if exists)
   - PDFs uploaded to R2: `docs/course_slides_pdf/{course_code}/{filename}`
   - Lecture records created with all statuses = 'pending'

2. **Structured Analysis** (Step 2):
   - Frontend button active when analysis_status = 'pending' or 'failed'
   - Calls `POST /api/v1/content/analyze/{lecture_id}`
   - Status updates: pending → in_progress → completed/failed

3. **Flashcard Generation** (Step 3):
   - Frontend button active when analysis_status = 'completed'
   - Calls `POST /api/v1/content/flashcards/{lecture_id}`
   - Status updates: pending → in_progress → completed/failed

4. **Quiz Generation** (Step 4):
   - Frontend button active when flashcard_status = 'completed'
   - Calls `POST /api/v1/content/quiz/{lecture_id}`
   - Status updates: pending → in_progress → completed/failed

5. **Vector Indexing** (Step 5):
   - Frontend button active when flashcard_status = 'completed'
   - Calls `POST /api/v1/content/index/{lecture_id}`
   - Status updates: pending → in_progress → completed/failed

## Error Handling

All services implement comprehensive error handling:
- Try/catch blocks wrap all operations
- On error: status set to 'failed', full traceback logged to error_log column
- Errors don't stop other lectures from processing
- Frontend can display errors and allow retry

## Next Steps

### TODO: Implement AI Model Integrations

The following placeholder implementations need actual API integrations:

1. **structured_analysis.py**: 
   - Implement `call_analysis_model()` with Gemini 2.0 Pro API
   - Handle PDF to model input conversion
   - Parse model response into structured JSON

2. **flashcard_generation.py**:
   - Implement `call_flashcard_model()` with Claude 3 Haiku API
   - Pass structured analysis as context
   - Parse model response into flashcard JSON

3. **quiz_generation.py**:
   - Implement `call_quiz_model()` with Gemini 2.0 Flash API
   - Generate questions for 4 difficulty levels
   - Parse model response into flexible quiz array format

4. **vector_indexing.py**:
   - Initialize Qdrant client with proper configuration
   - Implement embedding generation
   - Implement `index_to_qdrant()` with batch processing

### Frontend Implementation

Frontend needs to implement:
- Content Manager page
- Course ingestion form (metadata + PDF upload)
- Lectures dashboard with status indicators
- Action buttons that trigger appropriate endpoints based on status
- Error display and retry functionality

## File Structure

```
backend/app/
├── db/
│   └── postgres.py              # PostgreSQL connection
├── models/
│   └── content.py               # Course & Lecture models
├── routers/
│   └── content.py               # API endpoints
├── services/
│   └── content_pipeline/
│       ├── __init__.py
│       ├── orchestrator.py      # Facade service
│       ├── ingestion.py         # R2 upload & DB
│       ├── structured_analysis.py  # Gemini analysis
│       ├── flashcard_generation.py # Claude flashcards
│       ├── quiz_generation.py      # Gemini quizzes
│       └── vector_indexing.py      # Qdrant indexing
├── config.py                    # Configuration (updated)
└── main.py                      # Application (updated)
```

## Dependencies

Already present in `requirements.txt`:
- asyncpg>=0.29.0
- sqlalchemy>=2.0.0
- psycopg2-binary>=2.9.9
- boto3>=1.28.0 (for R2)
- anthropic>=0.34.0 (for Claude)

## Testing

To test the implementation:

1. Ensure PostgreSQL is running and accessible
2. Configure environment variables in `.env`
3. Start the backend: `python -m app.main`
4. Test ingestion: `POST /api/v1/content/ingest` with form data
5. Test pipeline actions: `POST /api/v1/content/{action}/{lecture_id}`
6. Monitor status: `GET /api/v1/content/lectures`

## Benefits

1. **Modular**: Each service can be tested and updated independently
2. **Observable**: Status tracking at each pipeline stage
3. **Resilient**: Errors don't cascade; can retry failed steps
4. **Scalable**: Database-backed with efficient indexing
5. **Flexible**: JSONB allows schema evolution without migrations
6. **Configurable**: All AI models configurable via environment

