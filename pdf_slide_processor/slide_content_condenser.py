"""
Slide Content Condenser
-----------------------

This optional pipeline consumes the structured JSON generated by
`pdf_slide_processor.main` and condenses redundant slide content while keeping
the output schema identical so downstream flashcard/quiz tooling continues to
work without changes.
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from textwrap import dedent
from typing import Any, Dict, List, Optional, Sequence, Tuple

from config import Config
from .utils import get_course_by_id, load_courses


class SlideContentCondenser:
    """Uses Gemini to condense verbose slide analyses into focused sections."""

    def __init__(self, api_key: str, model: str, temperature: float = 0.2):
        try:
            import google.generativeai as genai  # type: ignore
        except ImportError as exc:  # pragma: no cover - import guard
            raise ImportError(
                "google-generativeai is required. Install with pip install google-generativeai"
            ) from exc

        genai.configure(api_key=api_key)
        self._genai = genai
        self._model = genai.GenerativeModel(
            model,
            generation_config={
                "temperature": temperature,
                "top_p": 0.9,
                "response_mime_type": "application/json",
                "max_output_tokens": 4096,
            },
        )

    def condense(
        self,
        slides: Sequence[Dict[str, Any]],
        course_context: Dict[str, Any],
        lecture_context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Condense a complete set of slide analyses into a smaller, cleaner set.

        Returns a dictionary that mirrors the original structured_analysis
        schema: {"total_slides": N, "slides": [...]}
        """
        if not slides:
            raise ValueError("No slides provided for condensation.")

        prompt = self._build_prompt(slides, course_context, lecture_context)
        response = self._model.generate_content(prompt)

        result_text = (response.text or "").strip()
        if not result_text:
            raise RuntimeError("Received empty response from Gemini.")

        parsed = self._parse_response(result_text)
        normalized = self._normalize_output(parsed, lecture_context)
        return normalized

    def _build_prompt(
        self,
        slides: Sequence[Dict[str, Any]],
        course_context: Dict[str, Any],
        lecture_context: Dict[str, Any],
    ) -> str:
        """Create the condensation prompt with full course/lecture context."""
        course_lines = []
        if course_context.get("course_name"):
            course_lines.append(f"Course: {course_context['course_name']}")
        if course_context.get("course_code"):
            course_lines.append(f"Course Code: {course_context['course_code']}")
        if course_context.get("course_description"):
            course_lines.append(
                f"Course Description: {course_context['course_description']}"
            )
        textbooks = course_context.get("reference_textbooks") or []
        if textbooks:
            course_lines.append(
                "Reference Textbooks:\n  - " + "\n  - ".join(textbooks)
            )

        lecture_lines = [
            f"Lecture: {lecture_context.get('lecture_name', 'Unknown Lecture')}",
            f"Lecture Number: {lecture_context.get('lecture_number', 'N/A')}",
        ]
        topics = lecture_context.get("topics") or []
        if topics:
            lecture_lines.append(
                "Lecture Topics:\n  - " + "\n  - ".join(topics[:15])
            )

        formatted_slides = self._format_slides_for_prompt(slides)
        prompt = dedent(
            f"""
            You are an expert academic editor tasked with condensing verbose lecture slides.

            ### Course Context
            {'\n'.join(course_lines) if course_lines else 'N/A'}

            ### Lecture Context
            {'\n'.join(lecture_lines)}

            ### Objectives
            - Merge slides that cover the same concept or repeat information.
            - Remove redundant phrasing but keep precise facts, definitions, formulas, data points, and nuanced explanations.
            - Preserve critical minute details (terminology, caveats, edge cases, numeric values, step-by-step logic).
            - Group content by coherent concept/theme; each condensed section should feel like a meaningful study unit.
            - Track which original slide numbers contributed to each condensed section.

            ### Output Requirements
            Output valid JSON with this exact shape:
            {{
              "total_slides": <integer>,
              "slides": [
                {{
                  "page_number": <1-based index of the condensed section>,
                  "filename": "condensed_slide_<3-digit-index>.png",
                  "path": "",
                  "width": 0,
                  "height": 0,
                  "analysis": {{
                    "title": "Concise yet specific heading",
                    "main_text": "Well-structured paragraph(s) that synthesize all relevant insights.",
                    "key_concepts": ["Concept 1", "Concept 2"],
                    "diagrams": [
                      {{
                        "type": "flowchart/table/etc",
                        "description": "Describe visuals in natural language",
                        "key_points": ["Point A", "Point B"]
                      }}
                    ],
                    "examples": ["Short but concrete examples"],
                    "definitions": [
                      {{
                        "term": "Important term",
                        "definition": "Verbatim or precise definition"
                      }}
                    ],
                    "formulas": ["Any formulas or equations"],
                    "notes": "Nuances, caveats, or contextual remarks",
                    "source_slide_numbers": [list of original slide numbers that fed this section]
                  }}
                }}
              ]
            }}

            - Always include every key shown above (use empty arrays/strings when necessary).
            - Do NOT invent new facts. Use only what is present in the input slides.
            - Maintain chronological logic when possible.

            ### Input Slides
            {formatted_slides}
            """
        ).strip()

        return prompt

    def _format_slides_for_prompt(
        self, slides: Sequence[Dict[str, Any]]
    ) -> str:
        """Flatten slide analyses into a readable text block for the model."""
        segments: List[str] = []
        for idx, slide in enumerate(slides, 1):
            analysis = slide.get("analysis", {})
            lines = [
                f"Slide {slide.get('page_number', idx)}:",
            ]
            title = (analysis.get("title") or "").strip()
            if title:
                lines.append(f"Title: {title}")

            main_text = (analysis.get("main_text") or "").strip()
            if main_text:
                lines.append(f"Main Text: {main_text}")

            key_concepts = analysis.get("key_concepts") or []
            if key_concepts:
                lines.append(
                    "Key Concepts: " + "; ".join(str(c).strip() for c in key_concepts)
                )

            definitions = analysis.get("definitions") or []
            if definitions:
                def_lines = []
                for definition in definitions:
                    if isinstance(definition, dict):
                        term = definition.get("term", "")
                        definition_text = definition.get("definition", "")
                        def_lines.append(f"{term}: {definition_text}".strip(": "))
                    else:
                        def_lines.append(str(definition))
                lines.append("Definitions: " + "; ".join(def_lines))

            examples = analysis.get("examples") or []
            if examples:
                lines.append(
                    "Examples: " + "; ".join(str(example).strip() for example in examples)
                )

            diagrams = analysis.get("diagrams") or []
            if diagrams:
                diag_lines = []
                for diag in diagrams:
                    if isinstance(diag, dict):
                        diag_lines.append(
                            f"{diag.get('type', 'diagram')}: {diag.get('description', '')}"
                        )
                    else:
                        diag_lines.append(str(diag))
                lines.append("Diagrams: " + "; ".join(diag_lines))

            formulas = analysis.get("formulas") or []
            if formulas:
                lines.append("Formulas: " + "; ".join(formulas))

            notes = (analysis.get("notes") or "").strip()
            if notes:
                lines.append(f"Notes: {notes}")

            segments.append("\n".join(lines))

        divider = "\n\n---\n\n"
        return divider.join(segments)

    def _parse_response(self, raw_text: str) -> Dict[str, Any]:
        """Strip code fences (if any) and parse JSON."""
        cleaned = raw_text
        if cleaned.startswith("```"):
            parts = cleaned.split("```")
            if len(parts) >= 2:
                cleaned = parts[1]
                if cleaned.startswith("json"):
                    cleaned = cleaned[4:]
        cleaned = cleaned.strip()
        return json.loads(cleaned)

    def _normalize_output(
        self, data: Dict[str, Any], lecture_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Ensure the output strictly matches the expected schema."""
        slides_data = data.get("slides")
        if isinstance(slides_data, list):
            raw_slides = slides_data
        elif isinstance(data, list):
            raw_slides = data
        else:
            raise ValueError("Model response missing 'slides' array.")

        normalized_slides: List[Dict[str, Any]] = []
        pdf_stem = lecture_context.get("pdf_stem", "condensed_slide")

        for idx, raw_slide in enumerate(raw_slides, 1):
            normalized_slides.append(
                self._normalize_slide(raw_slide, idx, pdf_stem)
            )

        return {
            "total_slides": len(normalized_slides),
            "slides": normalized_slides,
        }

    def _normalize_slide(
        self, raw_slide: Dict[str, Any], idx: int, pdf_stem: str
    ) -> Dict[str, Any]:
        """Normalize a single slide entry and guarantee required fields."""
        metadata_defaults = {
            "page_number": idx,
            "filename": f"{pdf_stem}_condensed_{idx:03d}.png",
            "path": "",
            "width": 0,
            "height": 0,
        }

        metadata = {
            **metadata_defaults,
            **{k: raw_slide.get(k, metadata_defaults[k]) for k in metadata_defaults},
        }

        analysis = raw_slide.get("analysis")
        if analysis is None:
            # Assume slide-level keys are analysis content
            analysis = {k: raw_slide.get(k) for k in raw_slide.keys() if k not in metadata}

        normalized_analysis = self._normalize_analysis(analysis or {})
        return {**metadata, "analysis": normalized_analysis}

    def _normalize_analysis(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Guarantee that every analysis key exists with the proper type."""
        def _ensure_list(value: Any) -> List[Any]:
            if value is None:
                return []
            if isinstance(value, list):
                return value
            return [value]

        diagrams_value = analysis.get("diagrams")
        if diagrams_value is None:
            diagrams: List[Any] = []
        elif isinstance(diagrams_value, list):
            diagrams = diagrams_value
        else:
            diagrams = [diagrams_value]

        normalized = {
            "title": (analysis.get("title") or "").strip(),
            "main_text": (analysis.get("main_text") or "").strip(),
            "key_concepts": _ensure_list(analysis.get("key_concepts")),
            "diagrams": diagrams,
            "examples": _ensure_list(analysis.get("examples")),
            "definitions": _ensure_list(analysis.get("definitions")),
            "formulas": _ensure_list(analysis.get("formulas")),
            "notes": (analysis.get("notes") or "").strip(),
            "source_slide_numbers": _ensure_list(
                analysis.get("source_slide_numbers")
            ),
        }
        return normalized


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Condense slide structured_analysis files without changing the downstream JSON schema.",
    )
    parser.add_argument(
        "course_id",
        help="Course ID from courses.json (e.g., MS5260).",
    )
    parser.add_argument(
        "lecture",
        nargs="?",
        help="Lecture number (1-based index as listed in courses.json). Omit to condense all lectures.",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite the original structured_analysis.json instead of writing *_structured_analysis_condensed.json.",
    )
    return parser.parse_args(argv)


def resolve_lecture_targets(
    lectures: Sequence[Dict[str, Any]], lecture_identifier: Optional[str]
) -> List[Tuple[int, Dict[str, Any]]]:
    """Return (1-based-index, lecture_dict) pairs to process."""
    if not lectures:
        return []

    if lecture_identifier is None:
        return [(idx + 1, lecture) for idx, lecture in enumerate(lectures)]

    identifier = lecture_identifier.strip()

    # Try 1-based numeric index from array order
    if identifier.isdigit():
        idx = int(identifier)
        if idx < 1 or idx > len(lectures):
            raise ValueError(
                f"Lecture number {idx} is out of range (1-{len(lectures)})."
            )
        return [(idx, lectures[idx - 1])]

    matches: List[Tuple[int, Dict[str, Any]]] = []

    # Match against lecture_number field
    for idx, lecture in enumerate(lectures, 1):
        lecture_number = str(lecture.get("lecture_number", "")).lower()
        if lecture_number == identifier.lower():
            matches.append((idx, lecture))

    if matches:
        return matches

    # Match against pdf stem as last resort
    for idx, lecture in enumerate(lectures, 1):
        pdf_path = lecture.get("pdf_path", "")
        pdf_stem = Path(pdf_path).stem.lower()
        if pdf_stem == identifier.lower():
            matches.append((idx, lecture))
    if matches:
        return matches

    raise ValueError(
        f"Lecture identifier '{lecture_identifier}' did not match any lecture number or PDF."
    )


def load_structured_slides(path: Path) -> List[Dict[str, Any]]:
    """Load the slides array from a structured_analysis JSON file."""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, dict) and "slides" in data:
        slides = data.get("slides") or []
    elif isinstance(data, list):
        slides = data
    else:
        raise ValueError(f"Unrecognized JSON structure in {path}")

    if not isinstance(slides, list):
        raise ValueError(f"'slides' is not a list in {path}")

    return slides


def save_structured_slides(path: Path, payload: Dict[str, Any]) -> None:
    """Persist condensed structured analysis JSON to disk."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ Condensed structured analysis saved to: {path}")


def condense_lecture(
    condenser: SlideContentCondenser,
    course: Dict[str, Any],
    lecture_index: int,
    lecture: Dict[str, Any],
    overwrite: bool,
) -> None:
    course_id = course["course_id"]
    pdf_path = lecture.get("pdf_path")
    if not pdf_path:
        print(f"‚ö†Ô∏è  Lecture #{lecture_index} is missing 'pdf_path'. Skipping.")
        return

    pdf_stem = Path(pdf_path).stem
    course_base = Path("courses") / course_id
    analysis_dir = course_base / "slide_analysis"
    source_path = analysis_dir / f"{pdf_stem}_structured_analysis.json"

    if not source_path.exists():
        print(f"‚ö†Ô∏è  Structured analysis not found for lecture #{lecture_index}: {source_path}")
        print("    Run the PDF slide processor first: python -m pdf_slide_processor.main "
              f"{course_id} {pdf_stem}")
        return

    try:
        original_slides = load_structured_slides(source_path)
    except Exception as exc:
        print(f"‚ùå Failed to load structured analysis for {pdf_stem}: {exc}")
        return

    course_context = {
        "course_name": course.get("course_name"),
        "course_code": course.get("course_code"),
        "course_description": course.get("course_description"),
        "reference_textbooks": course.get("reference_textbooks", []),
    }
    lecture_context = {
        "lecture_name": lecture.get("lecture_name"),
        "lecture_number": lecture.get("lecture_number", lecture_index),
        "topics": lecture.get("topics", []),
        "pdf_stem": pdf_stem,
    }

    print(f"\n{'-'*70}")
    print(f"üìÑ Condensing Lecture #{lecture_index}: {lecture_context['lecture_name']}")
    print(f"{'-'*70}")
    print(f"  ‚Ä¢ Slides in source file: {len(original_slides)}")

    try:
        condensed_payload = condenser.condense(
            original_slides, course_context, lecture_context
        )
    except Exception as exc:
        print(f"‚ùå Condensation failed for {pdf_stem}: {exc}")
        return

    if not isinstance(condensed_payload.get("slides"), list):
        print(f"‚ùå Model output missing slides array for {pdf_stem}.")
        return

    target_filename = (
        f"{pdf_stem}_structured_analysis.json"
        if overwrite
        else f"{pdf_stem}_structured_analysis_condensed.json"
    )
    target_path = analysis_dir / target_filename

    if overwrite:
        backup_path = target_path.with_suffix(target_path.suffix + ".bak")
        try:
            if source_path.exists():
                source_path.replace(backup_path)
                print(f"  ‚Ä¢ Original file backed up to {backup_path}")
        except Exception:
            print("  ‚ö†Ô∏è  Failed to create backup before overwriting.")

    save_structured_slides(target_path, condensed_payload)
    print(f"  ‚Ä¢ Condensed slide count: {condensed_payload.get('total_slides', 0)}")


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = parse_args(argv)

    print("ü™Ñ Slide Content Condenser\n")

    is_valid, error_msg = Config.validate()
    if not is_valid:
        print(f"‚ùå Configuration Error: {error_msg}")
        sys.exit(1)

    courses = load_courses()
    if not courses:
        print("‚ùå No courses found in courses_resources/courses.json")
        sys.exit(1)

    course = get_course_by_id(args.course_id, courses)
    if not course:
        print(f"‚ùå Course '{args.course_id}' not found in courses.json")
        available = ", ".join(c["course_id"] for c in courses)
        print(f"   Available course IDs: {available}")
        sys.exit(1)

    lectures = course.get("lecture_slides") or []
    if not lectures:
        print(f"‚ö†Ô∏è  Course '{args.course_id}' has no lecture_slides entries.")
        sys.exit(0)

    try:
        targets = resolve_lecture_targets(lectures, args.lecture)
    except ValueError as exc:
        print(f"‚ùå {exc}")
        sys.exit(1)

    condenser = SlideContentCondenser(
        api_key=Config.GEMINI_API_KEY,
        model=Config.GEMINI_MODEL,
    )

    for lecture_index, lecture in targets:
        condense_lecture(condenser, course, lecture_index, lecture, args.overwrite)

    print("\n‚úÖ Slide condensation complete.")


if __name__ == "__main__":
    main()

