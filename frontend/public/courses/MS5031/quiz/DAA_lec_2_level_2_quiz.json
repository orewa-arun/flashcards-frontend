{
  "questions": [
    {
      "type": "mcq",
      "question_text": "A tech startup wants to predict the number of daily active users (Y) based on its daily marketing spend (X). They gather data over several months and plan to use a Simple Regression Model. In this context, what do Y and X primarily represent?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Y represents the independent variable, and X represents the dependent variable.",
        "B": "Y represents the outcome to be predicted, and X represents the predictor variable.",
        "C": "Y and X both represent independent variables.",
        "D": "Y represents a causal factor, and X represents a correlation."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "In a Simple Regression Model, the dependent variable (Y) is the outcome or response variable that we want to predict or explain, and the independent variable (X) is the predictor or explanatory variable used to make that prediction. So, daily active users (Y) is the outcome, and daily marketing spend (X) is the predictor. Option A incorrectly swaps the roles of Y and X. Options C and D misrepresent the basic definitions of dependent and independent variables or prematurely assign causation.",
      "difficulty_level": 2,
      "source_flashcard_id": "simple_regression_model",
      "tags": [
        "Simple Regression",
        "Dependent Variable",
        "Independent Variable",
        "Application"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A retail company is analyzing its sales data to understand the relationship between the number of sales associates on the floor (X) and total hourly sales (Y). They aim to use a Simple Regression Model. What is the primary objective of applying this model in this business scenario?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To prove that more sales associates directly cause higher sales.",
        "B": "To identify all factors influencing hourly sales.",
        "C": "To quantify the linear association between sales associates and hourly sales, and to predict sales for different staffing levels.",
        "D": "To determine the optimal number of sales associates that guarantees maximum sales."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "The primary objective of a Simple Regression Model is to analyze and quantify the linear relationship between a single dependent variable (Y) and a single independent variable (X), enabling prediction of Y based on X and understanding the strength of their association. In this context, it quantifies the relationship between sales associates and hourly sales, allowing the company to predict sales at various staffing levels. Option A incorrectly infers causation. Option B is too broad, as simple regression only considers one independent variable. Option D implies optimization or guarantee, which goes beyond the model's direct objective without further analysis.",
      "difficulty_level": 2,
      "source_flashcard_id": "simple_regression_model",
      "tags": [
        "Simple Regression",
        "Prediction",
        "Linear Relationship",
        "Business Applications"
      ]
    },
    {
      "type": "mca",
      "question_text": "A logistics firm uses a simple regression model to analyze the relationship between the number of packages delivered per day (X) and daily fuel consumption (Y) for its delivery fleet. After running the model, they find a strong positive linear relationship. Which of the following conclusions are contextually appropriate based on the model's primary objectives and common pitfalls? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model helps predict daily fuel consumption based on the number of packages delivered.",
        "B": "An increase in packages delivered per day is the sole cause of increased fuel consumption.",
        "C": "The model quantifies the average change in fuel consumption for each additional package delivered.",
        "D": "The firm can use the model to understand the strength of the association between packages and fuel consumption."
      },
      "correct_answer": [
        "A",
        "C",
        "D"
      ],
      "explanation": "The Simple Regression Model aims to predict Y based on X and understand the nature and strength of their association. Option A is correct as prediction is a core objective. Option C is correct as the slope coefficient in the model quantifies the average change in the dependent variable for a one-unit change in the independent variable. Option D is correct as the model provides measures (like R-squared) to understand the strength of the relationship. Option B is incorrect because simple regression quantifies association, not causation. Other unmeasured factors (like traffic, route efficiency, vehicle maintenance) could also influence fuel consumption, and inferring causation from correlation is a common pitfall.",
      "difficulty_level": 2,
      "source_flashcard_id": "simple_regression_model",
      "tags": [
        "Simple Regression",
        "Prediction",
        "Linear Relationship",
        "Correlation vs Causation",
        "Business Applications"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A food delivery service uses a simple regression model to examine the relationship between the number of active delivery drivers (X) and the average customer waiting time (Y) in a specific city. The model indicates a negative linear relationship. What does this model primarily allow the company to understand?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The exact causal mechanism by which drivers affect waiting times.",
        "B": "The specific thresholds at which adding more drivers no longer reduces waiting times.",
        "C": "The direction and strength of the linear association between the number of drivers and average waiting time, and to predict waiting times.",
        "D": "All potential factors, both observed and unobserved, that influence customer waiting times."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "The Simple Regression Model's main objective is to quantify the linear relationship between a single dependent variable (Y) and a single independent variable (X) to predict Y and understand the nature and strength of their association. In this case, it helps understand how waiting time changes with the number of drivers and allows for predictions. Option A is incorrect because regression models show association, not necessarily causation. Option B suggests identifying thresholds, which is beyond the scope of a simple linear model. Option D is too broad, as the model only accounts for the specified independent variable and an error term for unobserved factors.",
      "difficulty_level": 2,
      "source_flashcard_id": "simple_regression_model",
      "tags": [
        "Simple Regression",
        "Linear Relationship",
        "Association",
        "Prediction"
      ]
    },
    {
      "type": "mca",
      "question_text": "A financial advisory firm uses a simple regression model to predict client investment growth (Y) based on the number of personalized financial planning sessions provided per year (X). They observe a strong positive correlation and a statistically significant slope. Which of the following are potential pitfalls or limitations they should be cautious about when interpreting these results for business strategy? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Assuming that increased financial planning sessions *cause* higher investment growth.",
        "B": "Using the model to predict growth for clients receiving a number of sessions far outside the range of their observed data.",
        "C": "Ignoring that the model only accounts for a linear relationship, which might not hold true at all levels.",
        "D": "Concluding that the model is useless if the R-squared value is not 100%."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": "Option A is a common pitfall; correlation does not imply causation. Other factors (e.g., initial wealth, risk tolerance) could influence both. Option B describes extrapolation, which leads to unreliable predictions because the linear relationship might not hold outside the observed data range. Option C is also a limitation; simple regression assumes a linear relationship, and failing to acknowledge this can lead to misinterpretation if the true relationship is non-linear. Option D is incorrect; a model can be useful for prediction and understanding relationships even if it doesn't explain all variability (R-squared < 100%).",
      "difficulty_level": 2,
      "source_flashcard_id": "simple_regression_model",
      "tags": [
        "Simple Regression",
        "Correlation vs Causation",
        "Extrapolation",
        "Model Limitations",
        "Common Mistakes"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A global research team is studying the true underlying relationship between a country's average education level (X) and its long-term economic growth rate (Y). They conceptualize this relationship using the Population Regression Function (PRF): `Y = β₀ + β₁X + ε`. In this context, what does `β₁` specifically represent?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The estimated change in economic growth for a one-unit increase in education level, based on a sample.",
        "B": "The true, average economic growth rate when the education level is zero.",
        "C": "The random variation in economic growth not explained by education level.",
        "D": "The true, average change in economic growth rate for a one-unit increase in education level across the entire population."
      },
      "correct_answer": [
        "D"
      ],
      "explanation": "In the Population Regression Function, `β₁` is the population slope coefficient. It represents the true, average change in the dependent variable (Y) for a one-unit increase in the independent variable (X) across the entire population. Option A describes a sample estimate (`b₁`), not the true population parameter (`β₁`). Option B describes the population Y-intercept (`β₀`). Option C describes the error term (`ε`).",
      "difficulty_level": 2,
      "source_flashcard_id": "population_regression_function",
      "tags": [
        "Population Regression Function",
        "Beta-one",
        "Slope Coefficient",
        "Population Parameters"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A public health researcher is modeling the true relationship between average daily air pollution levels (X) and the incidence of respiratory illnesses (Y) in a large metropolitan area using the Population Regression Function (PRF): `Y = β₀ + β₁X + ε`. What aspect of this relationship does the `ε` (error term) primarily capture?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The average incidence of respiratory illnesses when air pollution is zero.",
        "B": "The true causal effect of air pollution on respiratory illnesses.",
        "C": "The unobserved factors and random variation influencing respiratory illnesses beyond what is explained by air pollution.",
        "D": "The estimated incidence of respiratory illnesses from a sample of data."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "In the PRF, `ε` (epsilon) represents the error term. It accounts for all other unobserved factors, measurement errors, and inherent random variation in the dependent variable (Y) that are not captured by the independent variable (X). Option A describes `β₀`. Option B incorrectly implies causation and is too specific for the error term. Option D describes a sample prediction (`ŷ`), not the population error term.",
      "difficulty_level": 2,
      "source_flashcard_id": "population_regression_function",
      "tags": [
        "Population Regression Function",
        "Error Term",
        "Unobserved Factors",
        "Random Variation"
      ]
    },
    {
      "type": "mca",
      "question_text": "An agricultural scientist is developing a theoretical model for the *true* relationship between the amount of fertilizer applied (X) and crop yield (Y) for a specific plant species globally. They use the Population Regression Function (PRF): `Y = β₀ + β₁X + ε`. Which of the following components of this formula represent parameters that describe the entire population and are typically unknown? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Y (Crop Yield)",
        "B": "β₀ (Population Y-intercept)",
        "C": "X (Fertilizer Amount)",
        "D": "β₁ (Population Slope Coefficient)",
        "E": "ε (Error Term)"
      },
      "correct_answer": [
        "B",
        "D",
        "E"
      ],
      "explanation": "The PRF describes the true, underlying linear relationship in the entire population. `β₀` is the population Y-intercept, representing the true average Y when X is zero. `β₁` is the population slope coefficient, representing the true average change in Y for a one-unit change in X. `ε` is the error term, representing the unobserved factors and random variation in the population. These are all unknown population parameters. `Y` and `X` are variables, not parameters; `Y` is the dependent variable and `X` is the independent variable, both of which can be observed.",
      "difficulty_level": 2,
      "source_flashcard_id": "population_regression_function",
      "tags": [
        "Population Regression Function",
        "Beta-naught",
        "Beta-one",
        "Error Term",
        "Population Parameters"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A government agency is theorizing the true relationship between the number of police officers deployed (X) and the crime rate (Y) in a city, using the Population Regression Function (PRF): `Y = β₀ + β₁X + ε`. What does `β₀` signify in this theoretical model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The estimated crime rate when police officers deployed is zero, derived from a sample.",
        "B": "The true average crime rate when the number of police officers deployed is zero, for the entire population.",
        "C": "The random, unexplained fluctuations in the crime rate.",
        "D": "The true change in crime rate for each additional police officer deployed."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "In the Population Regression Function, `β₀` is the population Y-intercept. It represents the true, average value of the dependent variable (Y) when the independent variable (X) is zero, for the entire population. Option A describes a sample estimate (`b₀`). Option C describes the error term (`ε`). Option D describes the population slope coefficient (`β₁`).",
      "difficulty_level": 2,
      "source_flashcard_id": "population_regression_function",
      "tags": [
        "Population Regression Function",
        "Beta-naught",
        "Y-intercept",
        "Population Parameters"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A business school professor is explaining the difference between the theoretical Population Regression Function and a model estimated from data. She emphasizes that `Y = β₀ + β₁X + ε` uses Greek letters (β₀, β₁) while `ŷ = b₀ + b₁x` uses Latin letters (b₀, b₁). What is the primary reason for this distinction?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Greek letters represent parameters that are known, while Latin letters represent parameters that are unknown.",
        "B": "Greek letters denote the true, unknown population parameters, whereas Latin letters denote sample estimates of those parameters.",
        "C": "Greek letters are used for the dependent variable, and Latin letters for the independent variable.",
        "D": "Greek letters are used in simple regression, and Latin letters in multiple regression."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "This distinction is a fundamental concept in statistics. Greek letters (like β₀, β₁) are conventionally used to denote true, unknown population parameters, which describe the entire population. Latin letters (like b₀, b₁) are used to denote sample statistics or estimates, which are calculated from a sample of data and are used to approximate the population parameters. Option A is incorrect as population parameters are unknown. Option C is incorrect as Y and X are variables, not parameters represented by β or b. Option D is incorrect as the notation applies to both simple and multiple regression.",
      "difficulty_level": 2,
      "source_flashcard_id": "population_regression_function",
      "tags": [
        "Population Regression Function",
        "Sample Regression Function",
        "Population Parameters",
        "Sample Estimates",
        "Common Mistakes"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst collects data on monthly advertising spend (X) and corresponding sales revenue (Y) for a product. To estimate the relationship, they use Ordinary Least Squares (OLS) to derive an Estimated Regression Line. What is the primary principle OLS uses to determine the coefficients (b₀ and b₁)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Maximizing the correlation between advertising spend and sales revenue.",
        "B": "Minimizing the sum of the absolute differences between observed and predicted sales revenue.",
        "C": "Minimizing the sum of the squared differences between observed and predicted sales revenue.",
        "D": "Ensuring that the estimated regression line passes through as many observed data points as possible."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "The Ordinary Least Squares (OLS) method is designed to find the line that best fits the data by minimizing the sum of the squared differences between the actual observed values (Yᵢ) and the values predicted by the regression line (ŷᵢ). These differences are known as residuals. Option A describes correlation, not the OLS estimation method. Option B mentions absolute differences, which is a different estimation method (Least Absolute Deviations, LAD), not OLS. Option D is generally not the objective; OLS aims to minimize overall error, not necessarily pass through specific points.",
      "difficulty_level": 2,
      "source_flashcard_id": "estimated_regression_line",
      "tags": [
        "Estimated Regression Line",
        "OLS",
        "Ordinary Least Squares",
        "Coefficients"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A production manager is using an Estimated Regression Line to predict the number of defective units per batch (Y) based on the machine's operating temperature (X). The estimated equation is `ŷ = 15 - 0.5x`. If a machine operates at 200 degrees Celsius, what does the calculated `ŷ` value primarily represent?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The actual number of defective units observed at 200 degrees Celsius.",
        "B": "The true average number of defective units for all machines operating at 200 degrees Celsius in the population.",
        "C": "The predicted average number of defective units for a batch produced at 200 degrees Celsius, based on the sample data.",
        "D": "The error or residual for a machine operating at 200 degrees Celsius."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "The `ŷ` (Y-hat) in the Estimated Regression Line represents the predicted value of the dependent variable for a given value of the independent variable, based on the sample data. It is an estimate of the average Y for a specific X. Option A is incorrect as ŷ is a prediction, not an actual observed value. Option B refers to the true population mean, which is estimated by ŷ but is not ŷ itself. Option D describes the residual (Y - ŷ).",
      "difficulty_level": 2,
      "source_flashcard_id": "estimated_regression_line",
      "tags": [
        "Estimated Regression Line",
        "Predicted Value",
        "Sample Regression Function"
      ]
    },
    {
      "type": "mca",
      "question_text": "A data analyst is tasked with building a model to predict employee satisfaction (Y) based on employee tenure (X) using historical survey data. They choose to employ the Ordinary Least Squares (OLS) method. Which of the following statements accurately describe the primary objectives and characteristics of the OLS method in this scenario? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "OLS minimizes the total vertical distances between the observed satisfaction scores and the estimated regression line.",
        "B": "OLS aims to find the line that best approximates the true, unknown population relationship.",
        "C": "OLS determines the coefficients (b₀ and b₁) by minimizing the sum of the squared differences between actual and predicted satisfaction scores.",
        "D": "OLS ensures that the estimated coefficients (b₀ and b₁) are the true population parameters (β₀ and β₁)."
      },
      "correct_answer": [
        "B",
        "C"
      ],
      "explanation": "Option B is correct because the Estimated Regression Line derived from OLS is intended to approximate the true, underlying population relationship. Option C is the defining characteristic of OLS: it minimizes the sum of the squared residuals. Option A is incorrect because OLS minimizes the *sum of the squared* vertical distances, not the sum of the absolute vertical distances. Option D is a common mistake; OLS provides *estimates* (b₀, b₁) of the true population parameters (β₀, β₁), but they are not guaranteed to be identical due to sampling variability.",
      "difficulty_level": 2,
      "source_flashcard_id": "estimated_regression_line",
      "tags": [
        "Estimated Regression Line",
        "OLS",
        "Ordinary Least Squares",
        "Sample Regression Function"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An HR department uses OLS to estimate a regression model predicting employee performance ratings (Y) based on hours of training (X). They obtain coefficients `b₀ = 50` and `b₁ = 0.8`. What do these coefficients primarily represent in the context of this model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The true, fixed population parameters for employee performance and training impact.",
        "B": "Estimates of the population parameters, derived from the sample data, and subject to sampling variability.",
        "C": "The guaranteed performance rating with zero training and the exact increase per hour of training.",
        "D": "The maximum possible performance rating and the minimum training required."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "The coefficients `b₀` and `b₁` derived from the Estimated Regression Line (`ŷ = b₀ + b₁x`) are *sample estimates* of the true, unknown population parameters (`β₀` and `β₁`). They are based on the specific sample data and are therefore subject to sampling variability. Option A is incorrect as `b₀` and `b₁` are estimates, not the true population parameters. Option C is incorrect as estimates are not guarantees, and `b₀` is the predicted baseline, not necessarily a 'guaranteed' performance. Option D misinterprets the meaning of the coefficients.",
      "difficulty_level": 2,
      "source_flashcard_id": "estimated_regression_line",
      "tags": [
        "Estimated Regression Line",
        "Coefficients",
        "Sample Estimates",
        "Sampling Variability"
      ]
    },
    {
      "type": "mca",
      "question_text": "A startup uses a regression model to estimate the relationship between weekly customer support tickets (X) and average resolution time in minutes (Y). Their Estimated Regression Line is `ŷ = 10 + 0.5x`. Which of the following statements correctly describe what this estimated line allows the startup to do, and what limitations it inherently has? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "It provides a point estimate for the average resolution time given a specific number of support tickets.",
        "B": "It proves that an increase in support tickets *causes* an increase in resolution time.",
        "C": "It reflects an approximation of the true population relationship based on their collected sample data.",
        "D": "It can be reliably used for predictions even if the number of support tickets is significantly higher than any observed in their data."
      },
      "correct_answer": [
        "A",
        "C"
      ],
      "explanation": "Option A is correct; the estimated line allows for point predictions of the dependent variable (`ŷ`) for given values of the independent variable (`x`). Option C is correct; the Estimated Regression Line is derived from sample data to approximate the true population relationship, acknowledging that it's an estimate. Option B is incorrect; regression models show association, not causation. Other factors might influence resolution time. Option D is incorrect; predicting outside the observed range of X (extrapolation) is a common pitfall that can lead to unreliable forecasts.",
      "difficulty_level": 2,
      "source_flashcard_id": "estimated_regression_line",
      "tags": [
        "Estimated Regression Line",
        "Prediction",
        "Sample Regression Function",
        "Extrapolation",
        "Correlation vs Causation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A company predicts employee commute time (Y) based on distance from office (X). For one employee, the actual commute time was 45 minutes, but the model predicted 38 minutes. What is the residual for this specific observation?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "-7 minutes",
        "B": "7 minutes",
        "C": "38 minutes",
        "D": "45 minutes"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "A residual (`eᵢ`) is the difference between the actual observed value (`Yᵢ`) and the predicted value (`ŷᵢ`). The formula is `eᵢ = Yᵢ - ŷᵢ`. In this case, `eᵢ = 45 minutes - 38 minutes = 7 minutes`. A positive residual indicates that the model under-predicted the actual value.",
      "difficulty_level": 2,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Prediction Error",
        "Calculation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A regression model is used by an airline to predict the number of delayed flights per day based on weather conditions. On a particular day with moderate weather, the model predicted 5 delayed flights, but the actual number was 12. What does this large positive residual primarily suggest?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model perfectly captured all factors influencing flight delays on that day.",
        "B": "The model over-predicted the number of delayed flights for that day.",
        "C": "There were significant unmodeled factors or random events that led to more delays than predicted by the weather conditions alone.",
        "D": "The regression model is fundamentally flawed and should be discarded immediately."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "A residual is the difference between the observed and predicted values. A large positive residual (actual > predicted) indicates that the model significantly under-predicted the outcome for that observation. This suggests that there were other important factors at play (unmodeled factors or random variation) that influenced the actual number of delayed flights, beyond what the independent variable (weather conditions) could explain. Option A is incorrect as a large residual means the model did not perfectly capture all factors. Option B is incorrect as the model under-predicted (12 actual vs 5 predicted). Option D is too extreme; a single large residual doesn't necessarily mean the model is fundamentally flawed, but it warrants investigation.",
      "difficulty_level": 2,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Unexplained Variation",
        "Model Fit",
        "Interpretation"
      ]
    },
    {
      "type": "mca",
      "question_text": "A logistics company uses a regression model to predict package delivery times (Y) based on distance (X). After analyzing the residuals from their model, they observe several patterns. Which of the following insights can be gained from a careful analysis of these residuals? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Identification of specific deliveries where the model significantly over- or under-predicted the time.",
        "B": "Detection of potential violations of OLS assumptions, such as non-linearity or heteroscedasticity.",
        "C": "The precise values of the true population error terms (`εᵢ`).",
        "D": "Suggestions for unmodeled factors (e.g., traffic, road closures) that might be influencing delivery times."
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": "Option A is correct; residuals directly show the prediction error for each observation, allowing identification of specific instances of over- or under-prediction. Option B is correct; residual plots are a crucial diagnostic tool for detecting violations of OLS assumptions like non-linearity (e.g., a curved pattern in residuals) or heteroscedasticity (e.g., a funnel shape). Option D is correct; large or patterned residuals can point towards systematic influences that the current model does not account for, suggesting new variables to include. Option C is incorrect; residuals (`eᵢ`) are *sample estimates* of the true population error terms (`εᵢ`), not the true values themselves.",
      "difficulty_level": 2,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Model Fit",
        "OLS Assumptions",
        "Model Diagnostics",
        "Unexplained Variation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A financial analyst builds a model to predict daily stock returns (Y) based on changes in a market index (X). When discussing the errors in the model, they differentiate between `eᵢ` and `εᵢ`. What is the key distinction between a residual (`eᵢ`) and a population error term (`εᵢ`)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "`eᵢ` represents the true, unobservable error, while `εᵢ` is its sample estimate.",
        "B": "`eᵢ` is the difference between observed and predicted values from the sample, while `εᵢ` is the true, unobservable deviation from the population regression line.",
        "C": "`eᵢ` applies to simple regression, and `εᵢ` applies to multiple regression.",
        "D": "`eᵢ` always sums to zero, while `εᵢ` can be any value."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "Residuals (`eᵢ`) are the observable differences between the actual observed values (Yᵢ) and the values predicted by the *estimated* regression line (ŷᵢ) for each observation in the sample. They are *sample estimates* of the errors. The population error term (`εᵢ`) represents the *true*, unobservable deviation of an individual observation from the *true* population regression line. Option A incorrectly swaps the definitions. Option C is incorrect; both terms apply to simple and multiple regression. Option D is incorrect; while OLS ensures the sum of residuals is zero, this is a property of the estimation, not the definition distinguishing it from the population error term.",
      "difficulty_level": 2,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Error Term",
        "Population Parameters",
        "Sample Estimates",
        "Common Mistakes"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A product manager wants to model the relationship between the number of features in a product (X) and its customer satisfaction score (Y). For a product with 5 features, the actual satisfaction score was 75. The regression model predicted a score of 72. How is the residual for this observation calculated?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "72 - 75 = -3",
        "B": "75 + 72 = 147",
        "C": "75 - 72 = 3",
        "D": "5 - (75 - 72) = 2"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "A residual (`eᵢ`) is defined as the difference between the actual observed value (`Yᵢ`) and the predicted value (`ŷᵢ`). Therefore, the calculation is `Yᵢ - ŷᵢ`. In this scenario, `Yᵢ = 75` and `ŷᵢ = 72`. So, the residual is `75 - 72 = 3`. Options A, B, and D perform incorrect calculations or use the wrong values.",
      "difficulty_level": 2,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Calculation",
        "Prediction Error"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A retail chain is modeling daily store sales (Y) based on the number of marketing promotions run (X). They observe that the errors (residuals) in their model are consistently higher on weekends than on weekdays, indicating a systematic pattern where errors from one day are correlated with errors from the next. Which OLS assumption is most likely being violated in this scenario?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Linearity",
        "B": "Normality of Errors",
        "C": "Independence of Errors",
        "D": "Homoscedasticity"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "The scenario describes errors that are correlated over time (errors from one day influencing the next), which is a direct violation of the Independence of Errors assumption. This assumption states that the error terms for any two observations should be uncorrelated. Linearity refers to a straight-line relationship. Normality of Errors refers to the distribution of errors. Homoscedasticity refers to the constant variance of errors.",
      "difficulty_level": 2,
      "source_flashcard_id": "ols_assumptions_for_errors",
      "tags": [
        "OLS Assumptions",
        "Independence of Errors",
        "Autocorrelation",
        "Model Diagnostics"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate company uses a linear regression model to predict house prices (Y) based on square footage (X). After plotting the residuals against the predicted values, they notice a 'funnel shape' where the spread of residuals increases significantly for larger houses. Which OLS assumption is primarily violated by this observation?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Linearity",
        "B": "Independence of Errors",
        "C": "Normality of Errors",
        "D": "Homoscedasticity (Equal Variance)"
      },
      "correct_answer": [
        "D"
      ],
      "explanation": "The 'funnel shape' in a residual plot, where the spread of residuals changes with the predicted (or independent) values, is a classic indicator of heteroscedasticity. This violates the Homoscedasticity (Equal Variance) assumption, which states that the variance of the error term should be constant across all levels of the independent variable. Linearity refers to the overall shape of the relationship, Independence to correlation between errors, and Normality to the distribution of the errors.",
      "difficulty_level": 2,
      "source_flashcard_id": "ols_assumptions_for_errors",
      "tags": [
        "OLS Assumptions",
        "Homoscedasticity",
        "Heteroscedasticity",
        "Residual Plots",
        "Model Diagnostics"
      ]
    },
    {
      "type": "mca",
      "question_text": "A business analyst develops a regression model to predict customer churn (Y) based on their monthly service usage (X). If the assumption of Normality of Errors is violated, which of the following implications could arise for the model's statistical inference and business decisions? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The OLS coefficient estimates (b₀, b₁) might still be unbiased, but their standard errors could be incorrect.",
        "B": "Confidence intervals for the coefficients may be inaccurate, leading to incorrect assessments of parameter precision.",
        "C": "Hypothesis tests (e.g., t-tests for coefficients) might yield incorrect p-values, leading to erroneous conclusions about statistical significance.",
        "D": "The relationship between customer churn and service usage must be non-linear."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": "Option A is correct; OLS estimates can still be unbiased under non-normal errors (by Gauss-Markov theorem if other assumptions hold), but their standard errors (and thus t-statistics and p-values) will be unreliable in smaller samples. Option B is correct; inaccurate standard errors directly lead to inaccurate confidence intervals, misrepresenting the precision of the estimates. Option C is correct; incorrect standard errors and confidence intervals mean that hypothesis tests (which rely on these) will produce unreliable p-values, potentially leading to incorrect conclusions about the statistical significance of predictors. Option D is incorrect; non-normality of errors does not directly imply non-linearity of the relationship. Linearity is a separate assumption.",
      "difficulty_level": 2,
      "source_flashcard_id": "ols_assumptions_for_errors",
      "tags": [
        "OLS Assumptions",
        "Normality of Errors",
        "Statistical Inference",
        "Confidence Intervals",
        "Hypothesis Testing"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A restaurant chain models daily customer foot traffic (Y) based on the daily temperature (X). After fitting a linear regression model, they examine a residual plot which shows a clear U-shaped pattern, with residuals being mostly positive at low and high temperatures, and mostly negative in the middle range. Which of the four key OLS assumptions about errors is this pattern primarily indicating a violation of?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Independence of Errors",
        "B": "Normality of Errors",
        "C": "Linearity",
        "D": "Homoscedasticity"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "A U-shaped pattern in a residual plot (residuals systematically positive at extremes and negative in the middle, or vice-versa) is a strong indicator that the underlying relationship between Y and X is non-linear, but a linear model has been fitted. This violates the Linearity assumption, which states that the relationship between the dependent and independent variables is linear. Independence relates to serial correlation, Normality to the distribution shape of errors, and Homoscedasticity to constant error variance.",
      "difficulty_level": 2,
      "source_flashcard_id": "ols_assumptions_for_errors",
      "tags": [
        "OLS Assumptions",
        "Linearity",
        "Residual Plots",
        "Model Diagnostics"
      ]
    },
    {
      "type": "mca",
      "question_text": "A management consultant is evaluating a client's business strategy, which relies heavily on predictions from a simple linear regression model. The consultant stresses the importance of checking the Ordinary Least Squares (OLS) assumptions about the error term. Which of the following statements accurately explain why these assumption checks are critical for valid business decision-making? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Violations can lead to OLS estimates that are biased, meaning they systematically over- or under-estimate the true population parameters.",
        "B": "Meeting these assumptions ensures that the OLS estimates are BLUE (Best Linear Unbiased Estimators), which is desirable for efficiency and unbiasedness.",
        "C": "If assumptions are violated, the standard errors of the coefficients can be incorrect, leading to invalid confidence intervals and p-values.",
        "D": "Checking assumptions allows for the identification of a perfectly linear relationship, which is always present in real-world data."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": "Option A is correct; violations of assumptions like linearity can indeed lead to biased OLS estimates. Option B is correct; the Gauss-Markov theorem states that if the OLS assumptions are met, the OLS estimators are BLUE, meaning they are the most efficient among all linear unbiased estimators. Option C is correct; incorrect standard errors directly impact the validity of confidence intervals and p-values, making statistical inference unreliable and potentially leading to poor business decisions (e.g., investing in a strategy based on a 'significant' coefficient that isn't truly significant). Option D is incorrect; perfectly linear relationships are rare in real-world data, and checking assumptions helps detect deviations from linearity, not confirm its perfect presence.",
      "difficulty_level": 2,
      "source_flashcard_id": "ols_assumptions_for_errors",
      "tags": [
        "OLS Assumptions",
        "BLUE",
        "Statistical Inference",
        "Business Decision Making",
        "Common Mistakes"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A startup has developed a new app and uses a linear regression model to predict user engagement (Y) based on the number of in-app tutorials completed (X). Their historical data shows users completing between 1 and 5 tutorials. If they use this model to predict engagement for a user who has completed 20 tutorials, what common pitfall are they engaging in?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Confusing correlation with causation.",
        "B": "Ignoring the normality of errors.",
        "C": "Extrapolation.",
        "D": "Overfitting the model."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "Extrapolation is the act of making predictions outside the range of observed data. In this scenario, the startup is predicting for 20 tutorials, which is far beyond their observed range of 1 to 5 tutorials. This assumes the linear relationship will hold true far outside the observed data, which is often not the case and leads to unreliable predictions. Option A is about inferring causation from correlation. Option B is an assumption about the error distribution. Option D refers to a model that performs well on training data but poorly on new data within the observed range, not necessarily outside it.",
      "difficulty_level": 2,
      "source_flashcard_id": "pitfalls_extrapolation_causation",
      "tags": [
        "Regression Pitfalls",
        "Extrapolation",
        "Unreliable Predictions"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A city council observes a strong positive correlation between the number of public parks (X) and the reported happiness levels of its residents (Y). Based on this, a council member proposes allocating a large budget to build many more parks, asserting that more parks will directly cause residents to be happier. What common pitfall is the council member making in their interpretation?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Ignoring the linearity assumption.",
        "B": "Extrapolating beyond observed data.",
        "C": "Confusing correlation with causation.",
        "D": "Overlooking the presence of outliers."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "The council member is observing a correlation (parks and happiness increase together) and then incorrectly assuming that one *causes* the other. This is the classic pitfall of confusing correlation with causation. There could be confounding variables, such as wealthier cities having both more resources for parks and generally happier residents due to better infrastructure, healthcare, etc. Options A and B are distinct pitfalls (linearity assumption and extrapolation). Option D refers to data quality issues, not directly the interpretation of the relationship itself.",
      "difficulty_level": 2,
      "source_flashcard_id": "pitfalls_extrapolation_causation",
      "tags": [
        "Regression Pitfalls",
        "Correlation vs Causation",
        "Confounding Variables",
        "Business Decision Making"
      ]
    }
  ]
}