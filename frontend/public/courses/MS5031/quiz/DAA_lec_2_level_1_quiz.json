{
  "questions": [
    {
      "type": "mcq",
      "question_text": "Which of the following best describes the primary objective of a Simple Regression Model in a business context?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To prove that changes in the independent variable *cause* changes in the dependent variable.",
        "B": "To analyze the linear relationship between a single dependent variable and a single independent variable for prediction and understanding.",
        "C": "To identify all possible factors influencing a dependent variable, regardless of their relationship type.",
        "D": "To calculate the average value of a single variable over time."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct because the Simple Regression Model specifically aims to analyze and quantify the linear relationship between one dependent (Y) and one independent (X) variable, primarily for prediction and understanding their association. Option A is incorrect because regression quantifies association, not necessarily causation. Options C and D describe objectives for other analytical methods, not simple regression.",
      "difficulty_level": 1,
      "source_flashcard_id": "simple_regression_model_definition",
      "tags": [
        "Simple Regression",
        "Objective",
        "Linear Relationship",
        "Prediction"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A retail company wants to predict its monthly sales revenue. They believe that their monthly advertising spend is the most significant factor. In a Simple Regression Model, what would 'monthly sales revenue' represent?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The independent variable (X)",
        "B": "The dependent variable (Y)",
        "C": "The error term (ε)",
        "D": "The slope coefficient (β₁)"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. In a regression model, the variable being predicted or explained is the dependent variable, typically denoted as Y. Since the company wants to predict 'monthly sales revenue', it is the dependent variable. Advertising spend would be the independent variable (X).",
      "difficulty_level": 1,
      "source_flashcard_id": "simple_regression_model_definition",
      "tags": [
        "Dependent Variable",
        "Simple Regression",
        "Business Applications"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Using a Simple Regression Model, a company explores the relationship between employee training hours and job performance scores. If 'job performance scores' is the dependent variable, what would 'employee training hours' represent?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The dependent variable (Y)",
        "B": "The error term (ε)",
        "C": "The independent variable (X)",
        "D": "The Y-intercept (β₀)"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. In a Simple Regression Model, the variable used to predict or explain the dependent variable is called the independent variable, typically denoted as X. Since job performance scores (Y) are being predicted based on employee training hours, training hours represent the independent variable (X).",
      "difficulty_level": 1,
      "source_flashcard_id": "simple_regression_model_definition",
      "tags": [
        "Independent Variable",
        "Simple Regression",
        "Business Applications"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common misconception when interpreting the results of a Simple Regression Model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "That a strong correlation implies a weak linear relationship.",
        "B": "That the model can only be used for forecasting, not understanding relationships.",
        "C": "That correlation between X and Y automatically proves X *causes* Y.",
        "D": "That the model is robust to all data outliers."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. A common mistake is to assume that a strong relationship found by the regression model means X *causes* Y. Regression quantifies association, but it does not prove causation, as other unmeasured factors could be at play. Option A is incorrect because a strong correlation suggests a strong linear relationship. Option B is incorrect as understanding the relationship is a key objective. Option D is incorrect as regression models can be sensitive to outliers.",
      "difficulty_level": 1,
      "source_flashcard_id": "simple_regression_model_definition",
      "tags": [
        "Correlation vs Causation",
        "Common Mistakes",
        "Simple Regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing team uses a Simple Regression Model to predict customer engagement (Y) based on social media ad spend (X). Which statement accurately reflects the model's function?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "It quantifies the linear impact of Y on X.",
        "B": "It helps determine if X and Y are related, and allows prediction of Y from X.",
        "C": "It measures the total number of customers engaged, ignoring ad spend.",
        "D": "It proves that increasing ad spend will always directly cause engagement to rise."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The Simple Regression Model is designed to analyze the relationship between an independent variable (X) and a dependent variable (Y) and to use X to predict Y. It quantifies the association, but does not prove causation (Option D). Option A is incorrect as it quantifies the impact of X on Y, not Y on X. Option C is incorrect as it specifically considers ad spend.",
      "difficulty_level": 1,
      "source_flashcard_id": "simple_regression_model_definition",
      "tags": [
        "Simple Regression",
        "Prediction",
        "Association",
        "Business Applications"
      ]
    },
    {
      "type": "mcq",
      "question_text": "In the Population Regression Function (PRF) `Y = β₀ + β₁X + ε`, what does the term `β₀` represent?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The estimated slope coefficient from a sample.",
        "B": "The population Y-intercept, representing the true mean value of Y when X is zero.",
        "C": "The error term, capturing unobserved factors.",
        "D": "The independent variable."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. In the PRF, `β₀` is the population Y-intercept, which represents the true average value of the dependent variable (Y) when the independent variable (X) is zero. Option A describes `b₁`, the sample slope. Option C describes `ε`. Option D describes `X`.",
      "difficulty_level": 1,
      "source_flashcard_id": "population_regression_function_components",
      "tags": [
        "Population Regression Function",
        "PRF",
        "Beta-naught",
        "Y-intercept"
      ]
    },
    {
      "type": "mcq",
      "question_text": "For the Population Regression Function (PRF) `Y = β₀ + β₁X + ε`, what does `β₁` signify?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The observed value of the dependent variable.",
        "B": "The population slope coefficient, representing the true change in Y for a one-unit change in X.",
        "C": "The predicted value of the dependent variable.",
        "D": "The random error for each observation."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. `β₁` is the population slope coefficient in the PRF. It represents the true average change in the dependent variable (Y) for every one-unit increase in the independent variable (X), assuming all other factors are constant. Option A is `Y`. Option C is `ŷ`. Option D is `ε`.",
      "difficulty_level": 1,
      "source_flashcard_id": "population_regression_function_components",
      "tags": [
        "Population Regression Function",
        "PRF",
        "Beta-one",
        "Slope Coefficient"
      ]
    },
    {
      "type": "mcq",
      "question_text": "In the context of the Population Regression Function `Y = β₀ + β₁X + ε`, what is the role of the `ε` (epsilon) term?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "It represents the true Y-intercept.",
        "B": "It is the predicted value of Y.",
        "C": "It captures the unobserved factors and random variation affecting Y, not explained by X.",
        "D": "It is the independent variable."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. The error term (`ε`) in the PRF accounts for all other factors influencing Y that are not included in the model, as well as random noise. It represents the unexplained variation in the dependent variable. Option A is `β₀`. Option B is `ŷ`. Option D is `X`.",
      "difficulty_level": 1,
      "source_flashcard_id": "population_regression_function_components",
      "tags": [
        "Population Regression Function",
        "PRF",
        "Error Term",
        "Unobserved Factors"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is the key difference between population parameters (e.g., β₀, β₁) and sample estimates (e.g., b₀, b₁) in regression analysis?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Population parameters are always known, while sample estimates are unknown.",
        "B": "Population parameters refer to the true, unknown values for the entire population, while sample estimates are calculated from observed data.",
        "C": "Sample estimates are represented by Greek letters, while population parameters use Roman letters.",
        "D": "Population parameters are used for prediction, while sample estimates are used for description."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. Population parameters (β₀, β₁) represent the true, fixed, but unknown values describing the entire population. Sample estimates (b₀, b₁) are calculated from a sample of data and are used to approximate these unknown population parameters. Option A is incorrect as population parameters are typically unknown. Option C reverses the notation. Option D is incorrect; both are involved in prediction and description.",
      "difficulty_level": 1,
      "source_flashcard_id": "population_regression_function_components",
      "tags": [
        "Population Parameters",
        "Sample Estimates",
        "Common Mistakes"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which of the following formulas represents the Population Regression Function (PRF)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "ŷ = b₀ + b₁x",
        "B": "Y = β₀ + β₁X + ε",
        "C": "eᵢ = Yᵢ - ŷᵢ",
        "D": "Y = X + ε"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The Population Regression Function (PRF) describes the true, underlying relationship in the entire population using population parameters (β₀, β₁) and includes an error term (ε). Option A is the Estimated Regression Line (SRF). Option C is the formula for a residual. Option D is an incomplete and incorrect representation of a regression model.",
      "difficulty_level": 1,
      "source_flashcard_id": "population_regression_function_components",
      "tags": [
        "Population Regression Function",
        "PRF",
        "Formula"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is the primary purpose of the Estimated Regression Line (or Sample Regression Function, SRF)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To describe the true, unknown relationship in the entire population.",
        "B": "To approximate the true population relationship using coefficients derived from sample data.",
        "C": "To precisely measure the error term (ε) for each observation.",
        "D": "To determine if two variables are causally linked."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The Estimated Regression Line (`ŷ = b₀ + b₁x`) is derived from sample data and serves as an approximation of the true, underlying population relationship. Option A describes the Population Regression Function (PRF). Option C describes residuals. Option D is incorrect as regression shows association, not necessarily causation.",
      "difficulty_level": 1,
      "source_flashcard_id": "estimated_regression_line_ols",
      "tags": [
        "Estimated Regression Line",
        "Sample Regression Function",
        "Purpose"
      ]
    },
    {
      "type": "mcq",
      "question_text": "How are the coefficients `b₀` and `b₁` for the Estimated Regression Line typically determined?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "By visually drawing a line through the data points.",
        "B": "Using the Ordinary Least Squares (OLS) method.",
        "C": "By randomly selecting values for b₀ and b₁.",
        "D": "By setting b₀ and b₁ equal to the population parameters β₀ and β₁."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The coefficients `b₀` and `b₁` of the Estimated Regression Line are typically determined using the Ordinary Least Squares (OLS) method, which mathematically minimizes the sum of squared residuals. Option A is a subjective and imprecise method. Options C and D are incorrect approaches to estimation.",
      "difficulty_level": 1,
      "source_flashcard_id": "estimated_regression_line_ols",
      "tags": [
        "OLS",
        "Coefficients",
        "Estimated Regression Line"
      ]
    },
    {
      "type": "mcq",
      "question_text": "The Ordinary Least Squares (OLS) method aims to minimize which of the following?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The sum of the observed Y values.",
        "B": "The sum of the squared differences between observed and predicted Y values.",
        "C": "The number of independent variables.",
        "D": "The variance of the independent variable (X)."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The core principle of Ordinary Least Squares (OLS) is to find the line that minimizes the sum of the squared differences (residuals) between the actual observed values (Yᵢ) and the values predicted by the model (ŷᵢ). This ensures the 'best fit' linear line. Option A, C, and D are not the objective of OLS.",
      "difficulty_level": 1,
      "source_flashcard_id": "estimated_regression_line_ols",
      "tags": [
        "OLS",
        "Sum of Squared Residuals",
        "Objective"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which of the following equations represents the Estimated Regression Line (or Sample Regression Function)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Y = β₀ + β₁X + ε",
        "B": "eᵢ = Yᵢ - ŷᵢ",
        "C": "ŷ = b₀ + b₁x",
        "D": "σ = √(Σ(Xᵢ - μ)² / N)"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. The Estimated Regression Line, also known as the Sample Regression Function (SRF), is represented by `ŷ = b₀ + b₁x`, where `ŷ` is the predicted dependent variable, `b₀` and `b₁` are the estimated coefficients, and `x` is the independent variable. Option A is the Population Regression Function (PRF). Option B is the formula for a residual. Option D is the formula for population standard deviation.",
      "difficulty_level": 1,
      "source_flashcard_id": "estimated_regression_line_ols",
      "tags": [
        "Estimated Regression Line",
        "Sample Regression Function",
        "Formula"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A common mistake when working with the Estimated Regression Line is assuming that the estimated coefficients (`b₀`, `b₁`) are equivalent to which of the following?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The R-squared value.",
        "B": "The true population parameters (`β₀`, `β₁`).",
        "C": "The standard errors of the estimates.",
        "D": "The number of observations in the sample."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. A common mistake is confusing the sample estimates (`b₀`, `b₁`) with the true, unknown population parameters (`β₀`, `β₁`). The estimated coefficients are only approximations based on a sample and are subject to sampling variability. Options A, C, and D are related concepts but do not represent the true population parameters.",
      "difficulty_level": 1,
      "source_flashcard_id": "estimated_regression_line_ols",
      "tags": [
        "Common Mistakes",
        "Sample Estimates",
        "Population Parameters"
      ]
    },
    {
      "type": "mcq",
      "question_text": "In regression analysis, what does a residual (`eᵢ`) represent for a given observation?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The value of the independent variable (Xᵢ).",
        "B": "The difference between the actual observed value (Yᵢ) and the predicted value (ŷᵢ).",
        "C": "The estimated slope coefficient (b₁).",
        "D": "The true, unobservable error term (εᵢ)."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. A residual (`eᵢ`) is defined as the difference between the actual observed value (`Yᵢ`) of the dependent variable and the value predicted by the estimated regression line (`ŷᵢ`) for that specific observation. Option D is incorrect because `εᵢ` is the true population error, while `eᵢ` is its sample estimate.",
      "difficulty_level": 1,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Definition",
        "Prediction Error"
      ]
    },
    {
      "type": "mcq",
      "question_text": "The formula for a residual in regression analysis is `eᵢ = Yᵢ - ŷᵢ`. What does `Yᵢ` represent in this formula?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The predicted value of the dependent variable for observation i.",
        "B": "The actual observed value of the dependent variable for observation i.",
        "C": "The independent variable for observation i.",
        "D": "The population error term for observation i."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. In the residual formula `eᵢ = Yᵢ - ŷᵢ`, `Yᵢ` represents the actual, observed value of the dependent variable for the i-th observation. `ŷᵢ` is the predicted value. Option A describes `ŷᵢ`. Option C describes `Xᵢ`. Option D describes `εᵢ`.",
      "difficulty_level": 1,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Formula",
        "Observed Value"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What does a residual primarily signify in the context of a regression model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The strength of the linear relationship.",
        "B": "The total variation in the dependent variable.",
        "C": "The unexplained variation or error in the model's prediction for an individual data point.",
        "D": "The average value of the independent variable."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. Residuals represent the portion of the dependent variable's variation that the regression model, using the independent variable(s), could not explain for a particular observation. It is the 'error' in the model's prediction for that specific point. Option A is related to R-squared. Option B is related to Total Sum of Squares. Option D is unrelated to residuals.",
      "difficulty_level": 1,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Unexplained Variation",
        "Prediction Error"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Residuals (`eᵢ`) are considered to be sample estimates of which population concept?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The population slope (β₁).",
        "B": "The population Y-intercept (β₀).",
        "C": "The population error terms (εᵢ).",
        "D": "The population mean of Y (μY)."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. Residuals (`eᵢ`) are the sample estimates of the true, unobservable population error terms (`εᵢ`). The error terms represent the true random disturbances in the population relationship, while residuals are what we observe in our sample. Options A, B, and D are other population parameters or statistics.",
      "difficulty_level": 1,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "Error Term",
        "Sample Estimates"
      ]
    },
    {
      "type": "mcq",
      "question_text": "When using the Ordinary Least Squares (OLS) method, what is the primary role of residuals?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To maximize the difference between observed and predicted values.",
        "B": "To ensure that all observed values fall exactly on the regression line.",
        "C": "To be minimized in their squared sum to find the best-fitting line.",
        "D": "To act as the independent variable in the model."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. The Ordinary Least Squares (OLS) method works by finding the regression line that minimizes the sum of the squared residuals. This means the coefficients `b₀` and `b₁` are chosen to make the total 'error' (as represented by the squared residuals) as small as possible. Option A is the opposite of OLS's goal. Option B is unrealistic for real-world data. Option D is incorrect; residuals are an outcome of the model.",
      "difficulty_level": 1,
      "source_flashcard_id": "residuals_definition",
      "tags": [
        "Residuals",
        "OLS",
        "Model Fit"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which OLS assumption states that the variance of the error term (`ε`) is constant across all values of the independent variable (X)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Linearity",
        "B": "Independence of Errors",
        "C": "Normality of Errors",
        "D": "Homoscedasticity"
      },
      "correct_answer": [
        "D"
      ],
      "explanation": "D is correct. Homoscedasticity is the assumption that the variance of the error term (`ε`) is constant across all levels of the independent variable (X). If the variance of errors changes with X, it's called heteroscedasticity. Option A refers to the functional form. Option B refers to errors not being correlated. Option C refers to the distribution of errors.",
      "difficulty_level": 1,
      "source_flashcard_id": "ols_assumptions",
      "tags": [
        "OLS Assumptions",
        "Homoscedasticity",
        "Equal Variance"
      ]
    },
    {
      "type": "mcq",
      "question_text": "The OLS assumption of 'Independence of Errors' means that:",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The error terms are normally distributed.",
        "B": "The error terms for one observation are not correlated with the error terms for any other observation.",
        "C": "The independent variable is not correlated with the error term.",
        "D": "The variance of the error terms is constant."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The assumption of Independence of Errors means that the error term for one observation does not systematically influence or correlate with the error term of any other observation. This is particularly important in time series data, where autocorrelation can be an issue. Option A is Normality of Errors. Option C is part of the Linearity assumption. Option D is Homoscedasticity.",
      "difficulty_level": 1,
      "source_flashcard_id": "ols_assumptions",
      "tags": [
        "OLS Assumptions",
        "Independence of Errors"
      ]
    },
    {
      "type": "mca",
      "question_text": "Which of the following are key assumptions about the error term (ε) that must hold for Ordinary Least Squares (OLS) estimates to be valid for statistical inference? (Select all that apply)",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The relationship between X and Y is linear.",
        "B": "The error terms are normally distributed.",
        "C": "The error terms have a constant variance (Homoscedasticity).",
        "D": "The independent variable (X) must be categorical."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": "A, B, and C are correct. Linearity (A) ensures the chosen model form is correct. Normality of Errors (B) is crucial for valid hypothesis testing and confidence intervals. Homoscedasticity (C), or equal variance, ensures that the precision of the estimates is consistent across the range of X. Option D is incorrect; the independent variable can be continuous or categorical, but it does not have to be categorical.",
      "difficulty_level": 1,
      "source_flashcard_id": "ols_assumptions",
      "tags": [
        "OLS Assumptions",
        "Linearity",
        "Normality of Errors",
        "Homoscedasticity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is the primary reason why it is crucial to check the assumptions of Ordinary Least Squares (OLS) regression?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To ensure the model has the highest possible R-squared value.",
        "B": "To guarantee that the independent variable causes the dependent variable.",
        "C": "To ensure the OLS estimates are BLUE (Best Linear Unbiased Estimators) and for valid statistical inference.",
        "D": "To simplify the interpretation of the regression coefficients."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. Meeting OLS assumptions ensures that the estimated coefficients are BLUE (Best Linear Unbiased Estimators) and that statistical inferences (like p-values and confidence intervals) drawn from the model are valid and reliable. Violations can lead to incorrect conclusions. Option A is incorrect as a high R-squared does not guarantee valid inference. Option B is incorrect as regression does not prove causation. Option D is incorrect as assumptions do not primarily simplify interpretation but rather validate it.",
      "difficulty_level": 1,
      "source_flashcard_id": "ols_assumptions",
      "tags": [
        "OLS Assumptions",
        "BLUE",
        "Statistical Inference"
      ]
    },
    {
      "type": "mcq",
      "question_text": "The OLS assumption of 'Normality of Errors' states that the error terms (`ε`) should follow which distribution?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Uniform distribution",
        "B": "Chi-squared distribution",
        "C": "Normal distribution",
        "D": "Binomial distribution"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. The assumption of 'Normality of Errors' states that the error terms (`ε`) are normally distributed. This assumption is important for the validity of hypothesis tests and confidence intervals, particularly in small samples. Options A, B, and D are other types of distributions not typically assumed for OLS errors.",
      "difficulty_level": 1,
      "source_flashcard_id": "ols_assumptions",
      "tags": [
        "OLS Assumptions",
        "Normality of Errors",
        "Normal Distribution"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is a Prediction Interval (PI) in regression analysis used for?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To estimate the range for the true population mean of Y at a given X.",
        "B": "To estimate the range for the true population slope coefficient (β₁).",
        "C": "To estimate a range within which a single, new individual observation of Y is expected to fall.",
        "D": "To determine if there is a linear relationship between X and Y."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. A Prediction Interval (PI) provides an estimated range for a *single, new, individual observation* of the dependent variable (Y) at a specific value of the independent variable (X). Option A describes a Confidence Interval for the Mean Response. Option B describes a Confidence Interval for a Regression Coefficient. Option D is an objective of hypothesis testing.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_definition",
      "tags": [
        "Prediction Interval",
        "Definition",
        "Individual Observation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A pharmaceutical company uses a regression model to predict the blood pressure reduction (Y) for a patient given a specific drug dosage (X). They want to provide a range for the expected reduction for a *new patient*. Which statistical tool should they use?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "A Confidence Interval for the mean response.",
        "B": "A Prediction Interval.",
        "C": "A p-value for the slope coefficient.",
        "D": "The R-squared value."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. Since the company wants to forecast the range for a *single, new patient* (an individual observation), a Prediction Interval is the appropriate tool. A Confidence Interval for the mean response (Option A) would be for the average reduction for all patients at that dosage. Options C and D are measures of statistical significance and model fit, respectively, not prediction ranges for individuals.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_definition",
      "tags": [
        "Prediction Interval",
        "Real-world Use Case",
        "Forecasting"
      ]
    },
    {
      "type": "mcq",
      "question_text": "How does a Prediction Interval (PI) generally compare in width to a Confidence Interval for the mean response (CI_Mean) at the same confidence level and for the same X value?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The PI is always narrower than the CI_Mean.",
        "B": "The PI is always wider than the CI_Mean.",
        "C": "The PI and CI_Mean are typically the same width.",
        "D": "The width comparison depends entirely on the sample size."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. Prediction Intervals are always wider than Confidence Intervals for the mean response. This is because PIs account for two sources of variability: the uncertainty in estimating the mean response (like the CI_Mean) AND the inherent, irreducible variability of individual observations around that mean. Option A and C are incorrect. Option D is a factor, but the inherent difference in width remains.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_definition",
      "tags": [
        "Prediction Interval",
        "Confidence Interval",
        "Width Comparison"
      ]
    },
    {
      "type": "mcq",
      "question_text": "If a 95% Prediction Interval for a new observation is calculated as [20, 30], what does this mean?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "There is a 95% chance that the true mean of Y lies within this range.",
        "B": "We are 95% confident that a single, new observation of Y will fall between 20 and 30.",
        "C": "The probability that the true population slope is within this range is 95%.",
        "D": "95% of the existing data points fall within this range."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. A 95% Prediction Interval means that we are 95% confident that a *single, new individual observation* of the dependent variable will fall within the calculated range. Option A describes a confidence interval for the mean. Option C describes a confidence interval for a coefficient. Option D refers to the fit of the existing data, not a prediction for a new point.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_definition",
      "tags": [
        "Prediction Interval",
        "Interpretation",
        "Confidence Level"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is the primary factor that causes Prediction Intervals to be wider than Confidence Intervals for the mean response?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The smaller sample size used for prediction intervals.",
        "B": "The inclusion of the independent variable in PI calculations.",
        "C": "Accounting for the inherent variability of individual observations around the mean.",
        "D": "The use of a lower confidence level for prediction intervals."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. Prediction intervals are wider because they must account for two sources of uncertainty: the uncertainty in estimating the mean response (which confidence intervals for the mean also capture) AND the additional, inherent variability of a single, individual observation around that estimated mean. Options A and B are incorrect. Option D would make the interval narrower, not wider.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_definition",
      "tags": [
        "Prediction Interval",
        "Variability",
        "Uncertainty"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a critical assumption for the validity of Prediction Intervals, in addition to the OLS assumptions?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The independent variable (X) must be perfectly correlated with Y.",
        "B": "The new X value for prediction must fall within the range of the observed X data.",
        "C": "The sample size must be exactly 30 observations.",
        "D": "The dependent variable (Y) must be a categorical variable."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. A critical assumption for valid Prediction Intervals is that you should not extrapolate; meaning, the new X value for which you are making a prediction must be within the range of the X values used to build the model. Predicting outside this range (extrapolation) can lead to unreliable intervals. Option A is an unrealistic and unnecessary assumption. Options C and D are incorrect; sample size can vary, and Y is typically continuous in linear regression.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_assumptions",
      "tags": [
        "Prediction Interval Assumptions",
        "No Extrapolation",
        "Validity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "If the assumption of linearity is violated (i.e., the true relationship is non-linear) when constructing a Prediction Interval, what is a likely consequence?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The prediction interval will become infinitely wide.",
        "B": "The prediction interval will be unreliable and likely biased, consistently over or under-predicting.",
        "C": "The prediction interval will be identical to a confidence interval for the mean.",
        "D": "The independent variable will no longer be considered significant."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. If the true relationship is non-linear but a linear model is used, the model will systematically misrepresent the relationship. This will lead to prediction intervals that are unreliable and biased, as the model's predictions will consistently be off in certain regions. Option A is an exaggeration. Option C is incorrect as they are distinct concepts. Option D is a possible outcome but not the most direct consequence on the PI itself.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_assumptions",
      "tags": [
        "Prediction Interval Assumptions",
        "Linearity",
        "Violation Consequences"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which of the following OLS assumptions is also crucial for the validity of Prediction Intervals?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The dependent variable must be normally distributed.",
        "B": "The error terms must be homoscedastic (have equal variance).",
        "C": "There must be perfect multicollinearity among independent variables.",
        "D": "The sample size must be very small."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. Homoscedasticity (equal variance of errors) is a key OLS assumption that is also crucial for valid prediction intervals. If errors are heteroscedastic, the width of the prediction interval will not be consistent across all X values, leading to misrepresentation of uncertainty. Option A is a common confusion; it's normality of errors, not the dependent variable itself. Option C is a violation of an assumption (no perfect multicollinearity). Option D is incorrect; larger sample sizes generally improve interval validity.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_assumptions",
      "tags": [
        "Prediction Interval Assumptions",
        "Homoscedasticity",
        "OLS Assumptions"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing team builds a model to predict sales (Y) based on advertising spend (X) and generates a prediction interval. If they use an ad spend value (X_new) that is far outside the range of their historical data, what assumption are they violating?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Normality of errors",
        "B": "Independence of errors",
        "C": "No extrapolation",
        "D": "Homoscedasticity"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. Using a new X value (advertising spend) that is far outside the range of the observed data used to build the model constitutes extrapolation. The 'no extrapolation' assumption is critical for the validity of prediction intervals, as the linear relationship may not hold beyond the observed range. Options A, B, and D are other OLS assumptions, but 'no extrapolation' specifically addresses predicting outside the data range.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_assumptions",
      "tags": [
        "Prediction Interval Assumptions",
        "Extrapolation",
        "Violation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is a primary consequence of ignoring the key assumptions when constructing a Prediction Interval?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The point estimate (ŷ) will always be zero.",
        "B": "The model will become a multiple regression model.",
        "C": "The prediction interval will be unreliable, potentially too narrow or too wide, misrepresenting the true uncertainty.",
        "D": "The R-squared value will automatically increase."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. Ignoring the assumptions for Prediction Intervals (like linearity, homoscedasticity, or no extrapolation) can lead to intervals that are either too narrow or too wide, thereby misrepresenting the true level of uncertainty in future predictions. This can lead to poor decision-making. Option A, B, and D are incorrect consequences.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_assumptions",
      "tags": [
        "Prediction Interval Assumptions",
        "Consequences",
        "Unreliability"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common pitfall when interpreting a Prediction Interval (PI)?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Assuming the PI is for the average response at a given X.",
        "B": "Recognizing that the PI is for a single, new observation.",
        "C": "Understanding that the PI becomes narrower for X values far from the mean.",
        "D": "Using the PI to prove causation between X and Y."
      },
      "correct_answer": [
        "A"
      ],
      "explanation": "A is correct. A common pitfall is confusing a Prediction Interval with a Confidence Interval for the *mean* response. The PI is specifically for a *single, new observation*, not an average, and is therefore wider. Option B is a correct understanding, not a pitfall. Option C is incorrect; PIs become *wider* for X values far from the mean. Option D is incorrect; regression (including PIs) shows association, not causation.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_pitfalls",
      "tags": [
        "Prediction Interval",
        "Common Mistakes",
        "Confidence Interval"
      ]
    },
    {
      "type": "mcq",
      "question_text": "How does the distance of a new independent variable value (`x_new`) from the mean of the observed independent variable values (`x_bar`) typically affect the width of a Prediction Interval?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The PI becomes narrower the further `x_new` is from `x_bar`.",
        "B": "The PI becomes wider the further `x_new` is from `x_bar`.",
        "C": "The distance of `x_new` from `x_bar` has no impact on the PI width.",
        "D": "The PI width is only affected by the sample size, not `x_new`'s distance."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. Both Prediction Intervals and Confidence Intervals for the mean become wider as the new X value (`x_new`) moves further away from the mean of the observed X values (`x_bar`). This reflects increased uncertainty in predictions at the extremes of the data range or beyond. Options A and C are incorrect. Option D is incorrect; while sample size is a factor, the distance of `x_new` is also critical.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_pitfalls",
      "tags": [
        "Prediction Interval",
        "Extrapolation",
        "Uncertainty"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Over-relying on the point estimate (ŷ) for a new prediction without considering its Prediction Interval can lead to which of the following?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "An accurate assessment of the true variability.",
        "B": "Underestimating the true uncertainty and risk associated with the prediction.",
        "C": "A narrower prediction interval than is actually appropriate.",
        "D": "The discovery of new causal relationships."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. If you only look at the point estimate (ŷ) and ignore the Prediction Interval, you are not accounting for the inherent variability of individual observations. This leads to underestimating the true uncertainty and risk, which can result in poor decisions. Option A is the opposite. Option C refers to the interval itself, not the consequence of ignoring it. Option D is incorrect as regression does not prove causation.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_pitfalls",
      "tags": [
        "Prediction Interval",
        "Point Estimate",
        "Uncertainty",
        "Decision Making"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A retail company forecasts monthly sales for a new product launch in a specific store using a Prediction Interval. Which statement describes a common pitfall they should avoid?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Confusing the PI with a confidence interval for the average sales of all new products.",
        "B": "Using the PI to communicate a range of possible sales to stakeholders.",
        "C": "Acknowledging that the PI will be wider for new products compared to established ones.",
        "D": "Ensuring their new product's advertising spend is within the range of historical data."
      },
      "correct_answer": [
        "A"
      ],
      "explanation": "A is correct. A common pitfall is confusing a Prediction Interval (for a single new product launch) with a Confidence Interval for the *average* sales of all new products. The PI is wider and reflects the higher uncertainty of a single, unique event. Options B, C, and D are good practices or correct understandings, not pitfalls.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_pitfalls",
      "tags": [
        "Prediction Interval",
        "Common Mistakes",
        "Business Decision Making"
      ]
    },
    {
      "type": "mcq",
      "question_text": "Why is a Prediction Interval for a single new observation generally wider than a Confidence Interval for the mean response at the same level of X?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Because the Prediction Interval does not account for sampling variability.",
        "B": "Because the Prediction Interval must account for the inherent variability of individual data points in addition to the uncertainty in the mean.",
        "C": "Because the Confidence Interval for the mean only considers the independent variable.",
        "D": "Because the Prediction Interval uses a different statistical distribution."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": "B is correct. The key reason Prediction Intervals are wider is that they include two components of uncertainty: the uncertainty in the estimated mean response (captured by the Confidence Interval for the mean) and the additional, inherent, unpredictable variation of a single new observation around that mean. Option A is incorrect; it accounts for both. Option C is incorrect as CI for the mean also considers dependent and independent variables. Option D is incorrect; both typically use the t-distribution.",
      "difficulty_level": 1,
      "source_flashcard_id": "prediction_interval_pitfalls",
      "tags": [
        "Prediction Interval",
        "Confidence Interval",
        "Variability",
        "Uncertainty"
      ]
    },
    {
      "type": "mcq",
      "question_text": "What is the core purpose of a Confidence Interval in regression analysis?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "To provide a single best guess (point estimate) for a parameter.",
        "B": "To determine the exact true value of a population parameter.",
        "C": "To quantify the uncertainty of a point estimate by providing a range of plausible values for an unknown population parameter.",
        "D": "To predict the value of a single future observation."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": "C is correct. A Confidence Interval quantifies the uncertainty of a point estimate (derived from sample data) by providing a range of values within which the true, unknown population parameter is estimated to lie, with a specified level of confidence. Option A describes a point estimate. Option B is incorrect as population parameters are generally unknown and cannot be 'exactly' determined from a sample. Option D describes a Prediction Interval.",
      "difficulty_level": 1,
      "source_flashcard_id": "confidence_interval_definition",
      "tags": [
        "Confidence Interval",
        "Purpose",
        "Uncertainty",
        "Population Parameter"
      ]
    }
  ]
}