{
  "metadata": {
    "generated_at": "2025-11-04T10:24:09.595712",
    "total_questions": 37,
    "course_name": "Data Analysis Applications",
    "course_id": "MS5031",
    "course_code": "DAA",
    "textbook_reference": "Statistics for Business: Decision Making and Analysis by Robert E Stine and Dean Foster, Pearson (ISBN: 978-81-317-3347-9)",
    "lecture": "DAA_lec_4",
    "difficulty_level": 3,
    "source_flashcards": 13
  },
  "questions": [
    {
      "type": "mcq",
      "question_text": "A real estate company is using Multiple Linear Regression (MLR) to predict property prices in a city. They've collected data on various features, including square footage, number of bedrooms, distance to the city center, and age of the property. After building the model, they notice that the coefficient for 'number of bedrooms' is negative, which contradicts their initial expectation that more bedrooms would increase the price. Further investigation reveals a high correlation between 'square footage' and 'number of bedrooms'. Why did this unexpected result occur?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model is overfitting the data due to the inclusion of irrelevant variables.",
        "B": "The 'number of bedrooms' is negatively correlated with property prices in this specific city.",
        "C": "Multicollinearity between 'square footage' and 'number of bedrooms' is affecting the coefficient estimates.",
        "D": "The relationship between 'number of bedrooms' and property price is non-linear and not captured by the linear model."
      },
      "correct_answer": "Multicollinearity between 'square footage' and 'number of bedrooms' is affecting the coefficient estimates.",
      "explanation": "The trick here is recognizing the impact of multicollinearity. The negative coefficient for 'number of bedrooms' is likely due to its high correlation with 'square footage'. In the presence of multicollinearity, the coefficients can be unstable and have unexpected signs. The model is essentially trying to disentangle the effects of two highly related variables. While overfitting (A) and non-linearity (D) could be issues, the high correlation strongly suggests multicollinearity. Option B is unlikely to be universally true.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_1",
      "tags": [
        "multiple linear regression",
        "multicollinearity",
        "regression analysis"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An online retailer wants to predict daily sales using MLR, considering advertising spend, website traffic, and seasonality. They observe that during promotional periods, both advertising spend and website traffic increase significantly. After building the model, they find that the coefficients for both advertising spend and website traffic are smaller than expected. What is the most likely error in this analysis?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model is not capturing the non-linear relationship between advertising spend and sales.",
        "B": "The model is suffering from heteroscedasticity due to the varying sales volume.",
        "C": "Multicollinearity between advertising spend and website traffic is attenuating the coefficient estimates.",
        "D": "The model is missing an interaction term between advertising spend and seasonality."
      },
      "correct_answer": "Multicollinearity between advertising spend and website traffic is attenuating the coefficient estimates.",
      "explanation": "The scenario highlights a temporal dependency: promotional periods influence both advertising spend and website traffic, creating multicollinearity. The trick is recognizing that the *smaller* coefficients are a sign of this. Option C correctly identifies this issue. While non-linearity (A) and heteroscedasticity (B) might be present, multicollinearity is the most direct explanation for the smaller coefficients. Option D, while plausible, is not the *most likely* initial error.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_1",
      "tags": [
        "multiple linear regression",
        "multicollinearity",
        "regression analysis"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building an MLR model to predict customer churn for a telecom company. They include features like monthly bill amount, number of calls made, data usage, and contract length. After building the model, they discover that the R-squared value is very high (0.95), but when they use the model on a new dataset, the predictive performance is poor. Which of the following is the most likely reason for this discrepancy?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model is underfitting the data due to the exclusion of important variables.",
        "B": "The model is overfitting the data due to the inclusion of irrelevant variables and noise.",
        "C": "The data violates the assumption of independence of errors.",
        "D": "The data violates the assumption of normality of errors."
      },
      "correct_answer": "The model is overfitting the data due to the inclusion of irrelevant variables and noise.",
      "explanation": "The high R-squared on the training data combined with poor performance on new data is a classic sign of overfitting. The trick is connecting high R-squared with poor generalization. Option B is the most likely explanation. While underfitting (A) is the opposite problem, violating independence (C) or normality (D) of errors would typically lead to less dramatic degradation in performance on new datasets. Overfitting is a direct consequence of including too many predictors, especially when some are noise.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_1",
      "tags": [
        "multiple linear regression",
        "overfitting",
        "regression analysis"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A financial analyst is using MLR to predict stock prices based on several economic indicators (interest rates, inflation, unemployment rate, and GDP growth). The analyst wants to understand the impact of a 1% increase in interest rates on stock prices. However, they are concerned that the relationship between interest rates and stock prices might be different during periods of economic recession compared to periods of economic growth. Which of the following approaches would be most appropriate to address this concern?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Remove the 'GDP growth' variable from the model to reduce multicollinearity.",
        "B": "Transform the 'interest rates' variable using a logarithmic transformation to ensure linearity.",
        "C": "Include an interaction term between 'interest rates' and a binary variable indicating whether the economy is in a recession or growth phase.",
        "D": "Fit two separate MLR models, one for recession periods and one for growth periods."
      },
      "correct_answer": "Include an interaction term between 'interest rates' and a binary variable indicating whether the economy is in a recession or growth phase.",
      "explanation": "The key here is recognizing the potential for interaction effects based on the economic climate. Option C is the most appropriate solution. Creating an interaction term allows the model to estimate different slopes for 'interest rates' depending on whether the economy is in a recession or growth phase. Removing 'GDP growth' (A) might address multicollinearity but doesn't directly address the interaction concern. Logarithmic transformation (B) addresses non-linearity, which isn't the primary issue here. Fitting separate models (D) is an option, but using an interaction term in a single model is often more efficient and allows for direct comparison of the coefficients.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_1",
      "tags": [
        "multiple linear regression",
        "interaction effects",
        "regression analysis"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing team uses MLR to predict sales based on TV advertising spend, online advertising spend, and print advertising spend. They find a statistically significant positive coefficient for TV advertising spend. However, when they increase TV advertising spend by 10%, sales only increase by 2%. Which of the following is the most likely explanation for this discrepancy?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model is overfitting the data due to the inclusion of irrelevant variables.",
        "B": "The model is not capturing the non-linear relationship between TV advertising spend and sales.",
        "C": "The partial regression coefficient for TV advertising spend represents the change in sales holding other variables constant, which may not reflect the real-world scenario.",
        "D": "The assumptions of MLR are violated, leading to biased coefficient estimates."
      },
      "correct_answer": "The partial regression coefficient for TV advertising spend represents the change in sales holding other variables constant, which may not reflect the real-world scenario.",
      "explanation": "The discrepancy arises because the partial slope isolates the effect of TV advertising *while holding other variables constant*. The trick is understanding the 'holding all other variables constant' clause. In reality, increasing TV advertising might influence other factors (e.g., online advertising increases due to a coordinated campaign). Option C accurately explains this. Overfitting (A), non-linearity (B), and violated assumptions (D) are possible, but the core issue is misinterpreting the meaning of the partial slope in a dynamic marketing environment.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_1",
      "tags": [
        "multiple linear regression",
        "partial slope",
        "regression analysis"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data analyst is building an MLR model to predict customer satisfaction based on product quality, customer service, and price. The partial slope for 'price' is -0.5. A senior manager interprets this as: \"For every $1 increase in price, customer satisfaction decreases by 0.5 units.\" Under what condition would this interpretation be most accurate?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "If the model has a high R-squared value.",
        "B": "If there is no multicollinearity among the independent variables.",
        "C": "If the product quality and customer service remain constant.",
        "D": "If the data satisfies the assumption of normality of errors."
      },
      "correct_answer": "If the product quality and customer service remain constant.",
      "explanation": "The core concept tested is the meaning of the partial slope: the change in the dependent variable for a one-unit change in the independent variable, *holding all other independent variables constant*. The trick is understanding this 'ceteris paribus' condition. Option C explicitly states this condition. A high R-squared (A), lack of multicollinearity (B), and normality of errors (D) are desirable properties of the model, but they don't guarantee the accuracy of the interpretation *unless* the other variables are held constant.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_2",
      "tags": [
        "partial slope",
        "regression coefficient",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A company is analyzing the impact of advertising spend on sales using MLR. The partial slope for advertising spend is $10. A marketing manager argues that for every additional dollar spent on advertising, sales will increase by $10. However, the company's sales data shows that the relationship between advertising spend and sales is subject to diminishing returns (i.e., the increase in sales for each additional dollar spent decreases as advertising spend increases). What is the most critical trade-off the manager is overlooking?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The trade-off between the cost of advertising and the revenue generated by increased sales.",
        "B": "The trade-off between different advertising channels (e.g., TV vs. online).",
        "C": "The trade-off between short-term sales increases and long-term brand building.",
        "D": "The trade-off between the linear approximation of the MLR model and the true non-linear relationship between advertising spend and sales."
      },
      "correct_answer": "The trade-off between the linear approximation of the MLR model and the true non-linear relationship between advertising spend and sales.",
      "explanation": "The question focuses on the limitations of the linear model when the true relationship is non-linear. The trick is recognizing that a linear model provides a *local* approximation. Option D is the most critical trade-off. The manager is assuming the partial slope holds true even at high levels of advertising spend, ignoring the diminishing returns. While cost-revenue (A), channel selection (B), and short-term vs. long-term (C) are important, the model's *inherent limitation* due to linearity is the primary oversight.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_2",
      "tags": [
        "partial slope",
        "regression coefficient",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A pharmaceutical company is using MLR to predict the effectiveness of a new drug based on dosage and patient weight. The partial slope for dosage is 0.8 (meaning that for each additional milligram of dosage, the effectiveness score increases by 0.8, holding patient weight constant). However, the drug has a known toxicity threshold: exceeding a certain dosage can lead to adverse effects and reduce effectiveness. What is the most significant limitation of using the partial slope to guide dosage recommendations?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The partial slope does not account for the cost of producing the drug.",
        "B": "The partial slope does not account for potential interactions between dosage and patient weight.",
        "C": "The partial slope does not reflect the drug's effectiveness beyond the toxicity threshold.",
        "D": "The partial slope only represents the average effect of dosage across all patients and may not be accurate for individual patients."
      },
      "correct_answer": "The partial slope does not reflect the drug's effectiveness beyond the toxicity threshold.",
      "explanation": "The key is the toxicity threshold. The partial slope is a *linear* approximation that doesn't capture the *non-linear* effect of toxicity. The trick is the business requirement contradicting the theory. Option C is the most significant limitation. While cost (A), interactions (B), and individual variation (D) are relevant, the toxicity threshold directly limits the *applicability* of the partial slope for dosage guidance. The model is valid only within the safe dosage range.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_2",
      "tags": [
        "partial slope",
        "regression coefficient",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An HR department is using MLR to predict employee performance based on years of experience and training hours. The partial slope for years of experience is 5 (meaning that for each additional year of experience, the performance score increases by 5, holding training hours constant). They want to use this information to justify higher salaries for experienced employees. However, they also know that employee performance plateaus after a certain number of years, and some experienced employees become resistant to new training methods. Which factor would most invalidate the use of the partial slope for salary justification?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The potential for multicollinearity between years of experience and training hours.",
        "B": "The possibility that some employees with fewer years of experience are more productive due to innate talent.",
        "C": "The non-linear relationship between years of experience and employee performance, particularly the plateau effect.",
        "D": "The fact that the partial slope only represents the average effect of experience across all employees."
      },
      "correct_answer": "The non-linear relationship between years of experience and employee performance, particularly the plateau effect.",
      "explanation": "The problem highlights a situation where the assumption of linearity is violated due to the plateau effect. The trick is linking the plateau effect to the violation of linearity. Option C is the most invalidating factor. While multicollinearity (A), individual talent (B), and average effect (D) are relevant, the *non-linearity* directly undermines the validity of using a *linear* partial slope to justify salaries across all experience levels. The linear model doesn't capture the diminishing returns of experience.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_2",
      "tags": [
        "partial slope",
        "regression coefficient",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A retail chain uses MLR to predict sales based on advertising spend, store size, and number of employees. The model has a high R-squared value and statistically significant coefficients for all variables. However, when they implement the model's recommendations, sales do not increase as predicted. Further investigation reveals that the store size and number of employees are highly correlated, and the company has a policy of staffing larger stores with more employees. What is the most likely statistical issue contributing to this problem?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Heteroscedasticity in the error terms.",
        "B": "Non-normality of the error terms.",
        "C": "Multicollinearity between store size and number of employees.",
        "D": "Violation of the linearity assumption."
      },
      "correct_answer": "Multicollinearity between store size and number of employees.",
      "explanation": "The scenario describes a situation where a business policy creates multicollinearity. The trick is connecting the policy to the statistical problem. Option C is the most likely issue. The high correlation between store size and number of employees makes it difficult to isolate the individual effects of each variable. While heteroscedasticity (A), non-normality (B), and non-linearity (D) could be present, the explicitly stated correlation points directly to multicollinearity as the primary driver of the prediction failure.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_2",
      "tags": [
        "partial slope",
        "regression coefficient",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An analyst is building an MLR model to predict customer spending based on income, age, and education level. After running the model, they observe the following residual plot (see below), which shows a clear pattern of increasing variance as the predicted spending increases. Which assumption of MLR is most likely violated, and what is a potential consequence of this violation?",
      "visual_type": "Plotly",
      "visual_code": "import plotly.graph_objects as go\nimport numpy as np\n\n# Generate dummy data with heteroscedasticity\nnp.random.seed(42)\nx = np.linspace(0, 10, 50)\ny_hat = 2 * x + 1\nerrors = np.random.normal(0, x, 50)  # Standard deviation increases with x\ny = y_hat + errors\n\nfig = go.Figure(data=go.Scatter(x=y_hat, y=errors, mode='markers'))\n\nfig.update_layout(title='Residual Plot', xaxis_title='Predicted Spending', yaxis_title='Residuals')\n\nfig.show()",
      "alt_text": "Scatter plot showing a residual plot where the spread of residuals increases as predicted spending increases, indicating heteroscedasticity.",
      "options": {
        "A": "Linearity; biased coefficient estimates.",
        "B": "Independence of Errors; inflated standard errors.",
        "C": "Normality of Errors; unreliable p-values.",
        "D": "Equal Variance (Homoscedasticity); inaccurate hypothesis tests."
      },
      "correct_answer": "Equal Variance (Homoscedasticity); inaccurate hypothesis tests.",
      "explanation": "The residual plot clearly shows heteroscedasticity (unequal variance). The trick is recognizing the visual pattern and its consequence. Option D is the correct answer. Heteroscedasticity violates the assumption of equal variance, leading to inaccurate standard errors and, consequently, unreliable hypothesis tests. Linearity violation (A) would show a curved pattern in the residual plot. Independence violation (B) would show correlation between residuals. Non-normality (C) is less evident from this specific plot pattern.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_3",
      "tags": [
        "mlr assumptions",
        "equal variance",
        "heteroscedasticity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building an MLR model to predict house prices. They collect data on house size, number of bedrooms, location, and age. After building the model, they perform residual analysis and find that the residuals are clustered together based on location (i.e., houses in the same neighborhood tend to have similar prediction errors). Which assumption of MLR is most likely violated, and what is a potential solution?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Linearity; transform the location variable using dummy variables.",
        "B": "Independence of Errors; include a spatial autocorrelation term in the model.",
        "C": "Normality of Errors; use a robust regression technique.",
        "D": "Equal Variance; transform the dependent variable (house price) using a logarithmic transformation."
      },
      "correct_answer": "Independence of Errors; include a spatial autocorrelation term in the model.",
      "explanation": "The clustering of residuals by location indicates a violation of the independence of errors assumption. The trick is recognizing the spatial dependency. Option B correctly identifies the violated assumption and a potential solution. Spatial autocorrelation means that errors are correlated based on their proximity in space. A spatial autocorrelation term accounts for this dependency. Linearity violation (A) would show non-linear patterns in the residual plot. Non-normality (C) and unequal variance (D) are less directly related to the spatial clustering of residuals.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_3",
      "tags": [
        "mlr assumptions",
        "independence",
        "spatial autocorrelation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A company is using MLR to predict sales based on advertising spend. They observe that the sales data is highly skewed, with a few very large sales values and many smaller sales values. After building the model, they perform residual analysis and find that the residuals are not normally distributed. Which assumption of MLR is most likely violated, and what is a potential approach to address this issue?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Linearity; add a quadratic term for advertising spend.",
        "B": "Independence of Errors; use time series regression.",
        "C": "Normality of Errors; transform the dependent variable (sales) using a logarithmic transformation.",
        "D": "Equal Variance; use weighted least squares regression."
      },
      "correct_answer": "Normality of Errors; transform the dependent variable (sales) using a logarithmic transformation.",
      "explanation": "The skewed sales data and non-normal residuals suggest a violation of the normality of errors assumption. The trick is linking skewness to non-normality. Option C is the best approach. A logarithmic transformation can often reduce skewness and improve the normality of the residuals. While adding a quadratic term (A) addresses non-linearity, it doesn't directly fix the non-normality. Time series regression (B) addresses dependence in time series data. Weighted least squares (D) addresses heteroscedasticity.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_3",
      "tags": [
        "mlr assumptions",
        "normality",
        "data transformation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst creates a scatterplot matrix (Scatmat) before building an MLR model to predict sales based on advertising spend, price, and competitor price. The Scatmat reveals a strong negative linear relationship between price and sales and a weak positive relationship between advertising spend and sales. However, the correlation between advertising spend and competitor price is very high (close to 1). What is the most critical issue the analyst should address before building the MLR model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The weak relationship between advertising spend and sales.",
        "B": "The strong negative relationship between price and sales.",
        "C": "The potential for non-linear relationships between the variables.",
        "D": "The high correlation between advertising spend and competitor price."
      },
      "correct_answer": "The high correlation between advertising spend and competitor price.",
      "explanation": "The high correlation between advertising spend and competitor price suggests multicollinearity. The trick is identifying the *most critical* issue. Option D is the most critical issue. Multicollinearity can lead to unstable coefficient estimates and make it difficult to determine the individual effects of advertising spend and competitor price. The weak relationship between advertising spend and sales (A) and the strong negative relationship between price and sales (B) are informative, but they are less critical than addressing the multicollinearity. Non-linear relationships (C) are also a concern, but multicollinearity often takes precedence in initial model building.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_4",
      "tags": [
        "EDA",
        "scatterplot matrix",
        "multicollinearity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is using a scatterplot matrix (Scatmat) to explore the relationships between various features and house prices before building an MLR model. The Scatmat shows a strong positive correlation between square footage and price, and a moderately positive correlation between the number of bathrooms and price. However, the scatterplot between square footage and the number of bathrooms shows a distinct non-linear, U-shaped pattern. What does this pattern suggest, and what action should the data scientist take?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "It suggests multicollinearity; remove either square footage or the number of bathrooms from the model.",
        "B": "It suggests a non-linear relationship; transform either square footage or the number of bathrooms to improve linearity.",
        "C": "It suggests heteroscedasticity; use weighted least squares regression.",
        "D": "It suggests a coding error; review the data for inconsistencies."
      },
      "correct_answer": "It suggests a non-linear relationship; transform either square footage or the number of bathrooms to improve linearity.",
      "explanation": "The non-linear U-shaped pattern in the Scatmat indicates a non-linear relationship between square footage and the number of bathrooms. The trick is interpreting the shape of the relationship in the Scatmat. Option B is the most appropriate action. Transforming one or both variables can help linearize the relationship. Multicollinearity (A) is possible, but the U-shaped pattern suggests a more fundamental issue of non-linearity. Heteroscedasticity (C) is not directly indicated by this pattern. A coding error (D) is possible, but the pattern is too structured to be random noise.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_4",
      "tags": [
        "EDA",
        "scatterplot matrix",
        "non-linearity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate analyst uses a Scatterplot Matrix (Scatmat) to examine the relationships between house price (Y), square footage (X1), number of bedrooms (X2), and distance to the city center (X3). The Scatmat reveals a strong positive linear relationship between square footage and price. However, when the analyst builds an MLR model, the coefficient for square footage is much smaller than expected, and the overall model fit (R-squared) is lower than anticipated. What is the most likely reason for this discrepancy between the Scatmat and the MLR results?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The Scatmat is misleading because it only shows marginal relationships, not partial relationships.",
        "B": "The MLR model is overfitting the data due to the inclusion of irrelevant variables.",
        "C": "The assumptions of MLR are violated, leading to biased coefficient estimates.",
        "D": "The analyst has incorrectly specified the MLR model."
      },
      "correct_answer": "The Scatmat is misleading because it only shows marginal relationships, not partial relationships.",
      "explanation": "The discrepancy arises because the Scatmat shows marginal relationships, while MLR models *partial* relationships. The trick is understanding the difference between marginal and partial effects. Option A is the most likely reason. The Scatmat doesn't account for the effects of other variables. The strong marginal relationship between square footage and price might be diminished when other variables are included in the model. Overfitting (B), violated assumptions (C), and model misspecification (D) are possible, but the core issue is the difference in what the Scatmat and MLR model are showing.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_4",
      "tags": [
        "EDA",
        "scatterplot matrix",
        "marginal effects",
        "partial effects"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing team uses a Scatterplot Matrix (Scatmat) to explore the relationships between sales (Y) and various marketing channels (X1: TV ads, X2: Online ads, X3: Print ads). They observe strong positive correlations between sales and each of the marketing channels. Based solely on this information from the Scatmat, which of the following conclusions is most justified?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Increasing spending on any of the marketing channels will lead to a significant increase in sales.",
        "B": "All marketing channels are equally effective at driving sales.",
        "C": "There is a positive association between spending on each marketing channel and sales.",
        "D": "An MLR model using these marketing channels as predictors will have a high R-squared value."
      },
      "correct_answer": "There is a positive association between spending on each marketing channel and sales.",
      "explanation": "The Scatmat only shows associations, not causal relationships or relative effectiveness. The trick is not over-interpreting the Scatmat. Option C is the most justified conclusion. The strong positive correlations indicate a positive association between spending on each channel and sales. Claiming a significant increase (A), equal effectiveness (B), or a high R-squared (D) goes beyond what can be inferred solely from the Scatmat. The Scatmat doesn't account for other factors or the relationships between the marketing channels themselves (e.g., multicollinearity).",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_4",
      "tags": [
        "EDA",
        "scatterplot matrix",
        "correlation",
        "association"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate company is analyzing housing prices in a city using several factors, including square footage, number of bedrooms, age of the house, and proximity to schools. They generate a correlation matrix and observe a high correlation (0.85) between square footage and the number of bedrooms. However, the model they build using these two variables shows inflated standard errors for their coefficients and unstable coefficient estimates when new data is added. Why did this problem likely occur, and what action should the company take given that both square footage and number of bedrooms are theoretically important predictors of housing price?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The high correlation indicates perfect multicollinearity. One of the variables (either square footage or number of bedrooms) should be removed from the model immediately to avoid biased coefficient estimates.",
        "B": "The high correlation suggests a non-linear relationship between housing price and the predictors. A non-linear regression model should be used instead of a multiple linear regression.",
        "C": "The high correlation suggests potential multicollinearity. Variance Inflation Factors (VIFs) should be calculated to quantify the severity. If VIFs are high, consider centering the variables, collecting more data, or combining the variables into a single composite variable.",
        "D": "The high correlation is not a problem because correlation matrices only show linear relationships, and multicollinearity only affects non-linear relationships. The model is correctly specified and no action is necessary."
      },
      "correct_answer": "The high correlation suggests potential multicollinearity. Variance Inflation Factors (VIFs) should be calculated to quantify the severity. If VIFs are high, consider centering the variables, collecting more data, or combining the variables into a single composite variable.",
      "explanation": "The question focuses on diagnosing and addressing multicollinearity. The trick is that a high correlation in the correlation matrix is a *warning sign*, not a definitive diagnosis. Option A is incorrect because it suggests removing a variable without further investigation, which might discard valuable information. Option B is incorrect because multicollinearity is related to linear dependencies, not necessarily non-linear relationships with the response. Option D is incorrect because it incorrectly states that multicollinearity only affects non-linear relationships. Option C correctly identifies the next steps: calculating VIFs and considering appropriate remedies like centering, more data, or combining variables.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "multicollinearity",
        "EDA",
        "VIF"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An analyst is using a correlation matrix to identify potential predictors for a customer churn model. The matrix reveals a very weak correlation (close to 0) between customer service call frequency and churn rate. The analyst concludes that customer service call frequency is not a useful predictor and removes it from the model. However, after building the model with other predictors, they discover that adding customer service call frequency significantly improves the model's predictive accuracy when combined with customer tenure. Why did the initial correlation matrix mislead the analyst, and what does this tell us about the limitations of using a correlation matrix in isolation?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The correlation matrix is flawed and should not be used for predictor selection. Using domain expertise and business logic is always a better approach.",
        "B": "The correlation matrix only captures linear relationships. Customer service call frequency likely has a non-linear relationship with churn, which was missed by the correlation matrix but captured by the model.",
        "C": "The correlation matrix only shows bivariate relationships. Customer service call frequency might have a significant *partial* effect on churn when considered in conjunction with other variables, even if its direct correlation is weak.",
        "D": "The model is overfitting the data. Including a variable with a weak correlation is always a bad practice and leads to poor generalization."
      },
      "correct_answer": "The correlation matrix only shows bivariate relationships. Customer service call frequency might have a significant *partial* effect on churn when considered in conjunction with other variables, even if its direct correlation is weak.",
      "explanation": "The question highlights the limitation of correlation matrices in capturing multivariate relationships. The trick is that a weak bivariate correlation doesn't mean a variable is useless in a multivariate model. Option A is wrong because correlation matrices *are* useful, just not in isolation. Option B is plausible but not the primary reason; while non-linear relationships are a concern, the more important factor is the partial effect. Option D is incorrect because including a weakly correlated variable is not *always* bad; it can improve the model if it contributes additional information when combined with other predictors. Option C correctly identifies the issue: customer service calls have a 'partial effect' only evident when combined with tenure.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "EDA",
        "Partial Correlation",
        "Multivariate Relationships"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is exploring a dataset of hospital patient records. They create a correlation matrix to understand the relationships between various patient characteristics (age, BMI, blood pressure, cholesterol levels) and the length of hospital stay. The matrix reveals several strong correlations (above 0.8) between different patient characteristics but relatively weak correlations between these characteristics and the length of stay. The data scientist concludes there's no strong evidence that these characteristics are useful predictors of length of stay. What is the most likely error in this conclusion, and how should the data scientist proceed?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The error is relying on a correlation matrix, which is only suitable for linear relationships. The data scientist should use a scatterplot matrix to identify non-linear relationships between patient characteristics and length of stay.",
        "B": "The error is assuming that strong correlations between predictors are beneficial. The data scientist should remove the highly correlated patient characteristics to simplify the model and avoid overfitting.",
        "C": "The error is focusing solely on bivariate correlations. The data scientist should explore interaction effects between patient characteristics. For example, the effect of high blood pressure on length of stay might be different for elderly patients compared to younger patients.",
        "D": "The error is not considering the sample size. With a large enough sample, even weak correlations can be statistically significant. The data scientist should perform hypothesis tests to determine if the correlations are statistically significant."
      },
      "correct_answer": "The error is focusing solely on bivariate correlations. The data scientist should explore interaction effects between patient characteristics. For example, the effect of high blood pressure on length of stay might be different for elderly patients compared to younger patients.",
      "explanation": "This question focuses on the importance of considering interaction effects. The trick is the phrase \"relatively weak correlations between these characteristics and the length of stay.\" Option A is incorrect because even if there are non-linear relationships, interaction effects could still be present. Option B is incorrect because the focus should be on the relationship with the outcome, not just between predictors. Option D is partially correct; statistical significance is important, but doesn't address the *lack* of strong individual correlations. Option C correctly identifies that the effect of one predictor might depend on the value of another (interaction), which is missed by the correlation matrix.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "EDA",
        "Interaction Effects",
        "Multiple Linear Regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An e-commerce company is investigating factors influencing customer spending. They create a correlation matrix and a scatterplot matrix. The correlation matrix shows a moderate positive correlation (0.55) between the number of products viewed and total spending. However, the scatterplot matrix reveals a distinct cluster of customers who view a large number of products but have very low total spending. Why did the correlation matrix fail to capture this pattern, and what implications does this have for the company's marketing strategy?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The correlation matrix is more reliable than the scatterplot matrix. The cluster observed in the scatterplot matrix is likely due to random noise and should be ignored.",
        "B": "The correlation matrix is only valid for normally distributed data. The presence of the cluster suggests that the data is not normally distributed, invalidating the correlation coefficient.",
        "C": "The correlation matrix only captures linear relationships. The cluster likely represents a subgroup of customers with a different spending behavior that deviates from the overall linear trend.",
        "D": "The correlation matrix is influenced by outliers. The cluster of low-spending customers is pulling down the overall correlation, making it appear weaker than it actually is."
      },
      "correct_answer": "The correlation matrix only captures linear relationships. The cluster likely represents a subgroup of customers with a different spending behavior that deviates from the overall linear trend.",
      "explanation": "This question directly compares the correlation matrix and scatterplot matrix. The trick is the phrase \"a distinct cluster of customers who view a large number of products but have very low total spending.\" Option A is incorrect because the scatterplot provides valuable visual information. Option B, while technically true that Pearson correlation assumes normality, is not the *primary* reason for the discrepancy in this scenario. Option D is plausible (outliers can influence correlation), but Option C is a better explanation because it focuses on the linear nature of the correlation matrix and the existence of a subgroup with different behavior, evident in the scatterplot. Therefore, Option C is the correct answer.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_6",
      "tags": [
        "scatterplot matrix",
        "correlation matrix",
        "EDA",
        "Linearity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A financial analyst is investigating the relationship between several economic indicators (inflation rate, unemployment rate, interest rates) and stock market returns. They create both a scatterplot matrix and a correlation matrix. The scatterplot matrix reveals a complex pattern where stock market returns tend to increase with inflation up to a certain point, after which they start to decline. The correlation matrix, however, shows a weak positive correlation overall between inflation and stock market returns. Given this discrepancy, what is the most critical trade-off the analyst must consider when choosing between a linear regression model based on the correlation matrix versus a more complex, potentially non-linear model suggested by the scatterplot matrix?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The trade-off is between model simplicity and interpretability. A linear model is easier to understand and explain, while a non-linear model is more complex and harder to interpret.",
        "B": "The trade-off is between model accuracy on the training data and generalization to new data. A non-linear model might fit the training data better, but it could also overfit and perform poorly on new data.",
        "C": "The trade-off is between computational cost and statistical power. A non-linear model is computationally more expensive to estimate, but it might also have more statistical power to detect significant relationships.",
        "D": "The trade-off is between using a correlation matrix for initial screening and relying solely on the scatterplot matrix for model selection. The scatterplot is subjective, and the correlation matrix provides objective numbers."
      },
      "correct_answer": "The trade-off is between model accuracy on the training data and generalization to new data. A non-linear model might fit the training data better, but it could also overfit and perform poorly on new data.",
      "explanation": "This question tests the understanding of overfitting and generalization. The trick is the phrase \"stock market returns tend to increase with inflation up to a certain point, after which they start to decline.\" This describes a non-linear relationship that a linear model will miss. Option A is a general trade-off but doesn't address the specific problem of a non-linear relationship. Option C is less relevant to the core issue of model accuracy vs. generalization. Option D is incorrect because the scatterplot provides *valuable* information, even if it is visual. Option B correctly identifies the core trade-off: a more complex model might fit the *training* data better, but it risks overfitting and performing poorly on *new* data.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_6",
      "tags": [
        "scatterplot matrix",
        "correlation matrix",
        "EDA",
        "Overfitting",
        "Generalization"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst is building a multiple linear regression model to predict sales based on advertising spend (TV, radio, and online). After running the model, they observe that the coefficient for TV advertising spend is negative and statistically significant. This contradicts the analyst's prior belief and general marketing theory, which suggests a positive relationship between advertising and sales. Assuming the model is correctly specified and there are no data errors, what is the most likely reason for this unexpected negative coefficient, and what action should the analyst take?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The negative coefficient indicates a coding error in the data. The analyst should carefully review the data for TV advertising spend to identify and correct any errors.",
        "B": "The negative coefficient indicates a violation of the 'ceteris paribus' condition. The analyst should add interaction terms to the model to account for the combined effects of advertising spend across different channels.",
        "C": "The negative coefficient indicates multicollinearity. The analyst should examine the correlations between advertising spend across different channels and consider removing the TV advertising variable from the model.",
        "D": "The negative coefficient indicates that TV advertising is ineffective. The analyst should advise the company to discontinue TV advertising and allocate the budget to other channels."
      },
      "correct_answer": "The negative coefficient indicates multicollinearity. The analyst should examine the correlations between advertising spend across different channels and consider removing the TV advertising variable from the model.",
      "explanation": "This question focuses on interpreting coefficients in the context of multicollinearity and 'ceteris paribus'. The trick is the phrase \"This contradicts the analyst's prior belief and general marketing theory, which suggests a positive relationship between advertising and sales.\" Option A is a plausible initial check, but less likely given the problem states 'Assuming the model is correctly specified and there are no data errors'. Option B is plausible in some cases, but the *negative* coefficient is a strong indicator of multicollinearity. Option D is a premature conclusion. Option C correctly identifies multicollinearity as the likely cause and suggests appropriate actions. The negative coefficient arises because TV ad spend is highly correlated with another ad channel, and its effect is being 'suppressed'.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_7",
      "tags": [
        "MLR",
        "Ceteris Paribus",
        "Partial Slope",
        "Multicollinearity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building a multiple linear regression model to predict customer satisfaction scores based on several factors, including product quality, price, and customer service rating. The initial model has an $R^2$ of 0.75. To improve the model's predictive power, the data scientist adds several new predictor variables related to website usability and social media engagement. The $R^2$ increases to 0.85. However, upon closer inspection, the data scientist notices that the adjusted $R^2$ has slightly decreased. Why did the adjusted $R^2$ decrease despite the increase in $R^2$, and what does this indicate about the addition of the new predictor variables?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The adjusted $R^2$ decreased because the new predictor variables are irrelevant and do not contribute to explaining the variation in customer satisfaction scores. The data scientist should remove these variables from the model.",
        "B": "The adjusted $R^2$ decreased because the new predictor variables are highly correlated with each other, leading to multicollinearity and inflated standard errors. The data scientist should address the multicollinearity issue before interpreting the model.",
        "C": "The adjusted $R^2$ decreased because the sample size is too small. The data scientist should collect more data to improve the reliability of the model and increase the adjusted $R^2$.",
        "D": "The adjusted $R^2$ decreased because the increase in $R^2$ was not large enough to offset the penalty for adding more variables. The new variables do not add enough explanatory power to justify their inclusion in the model."
      },
      "correct_answer": "The adjusted $R^2$ decreased because the increase in $R^2$ was not large enough to offset the penalty for adding more variables. The new variables do not add enough explanatory power to justify their inclusion in the model.",
      "explanation": "This question focuses on the difference between $R^2$ and adjusted $R^2$. The trick is understanding why adjusted R-squared *decreases* even when R-squared increases. Option A is the most direct answer, but B is a close second. However, the question doesn't mention inflated standard errors, so multicollinearity isn't directly supported. Option C is incorrect because the adjusted R-squared adjusts for sample size. Option D accurately explains the adjusted R-squared's purpose: it penalizes the model for adding variables that don't significantly improve the fit. The improvement in $R^2$ was smaller than the penalty for adding more predictors.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_8",
      "tags": [
        "R-squared",
        "Coefficient of Determination",
        "Adjusted R-squared",
        "Model Fit"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An analyst builds a multiple linear regression model to predict monthly sales for a retail store. The model includes variables such as advertising spend, price, competitor promotions, and seasonality (using dummy variables). The model has a high $R^2$ (0.90), suggesting a good fit. However, when the model is used to forecast sales for the next month, the predicted sales are significantly higher than the actual sales. What is the most likely reason for this discrepancy, and what should the analyst do to improve the model's forecasting accuracy?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The high $R^2$ indicates overfitting. The analyst should simplify the model by removing some of the predictor variables and using techniques like cross-validation to evaluate the model's performance on unseen data.",
        "B": "The high $R^2$ is misleading because the model is likely capturing spurious correlations due to the inclusion of irrelevant variables. The analyst should use a feature selection technique like stepwise regression to identify the most important predictors.",
        "C": "The high $R^2$ suggests that the model is too sensitive to changes in the predictor variables. The analyst should regularize the model using techniques like Ridge regression or Lasso to reduce the impact of individual predictors.",
        "D": "The high $R^2$ is irrelevant to forecasting accuracy. The analyst should focus on improving the model's residuals by transforming the dependent variable or adding interaction terms."
      },
      "correct_answer": "The high $R^2$ indicates overfitting. The analyst should simplify the model by removing some of the predictor variables and using techniques like cross-validation to evaluate the model's performance on unseen data.",
      "explanation": "This question tests understanding of overfitting and its impact on forecasting. The trick is the phrase \"the predicted sales are significantly higher than the actual sales\" which indicates the model is not generalizing well to new data. Option A correctly identifies overfitting as the likely cause and suggests appropriate remedies: simplifying the model and using cross-validation. Option B is plausible (spurious correlations), but overfitting is the more direct explanation for poor forecasting. Option C (regularization) is a valid technique, but again, overfitting is the primary issue. Option D is incorrect; while residual analysis is important, the primary problem is overfitting, not just poor residuals. A model with high R-squared on the training data may not perform well on new data due to overfitting, especially if it is overly complex.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_8",
      "tags": [
        "R-squared",
        "Coefficient of Determination",
        "Model Fit",
        "Overfitting",
        "Forecasting"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building a multiple regression model to predict customer churn for a telecommunications company. They build three models: Model A with 3 predictors ($R^2$ = 0.65), Model B with 7 predictors ($R^2$ = 0.72), and Model C with 12 predictors ($R^2$ = 0.75). The company's business requirement is to minimize model complexity due to computational constraints for real-time predictions. \n\nWhy might choosing Model C based solely on its highest $R^2$ be a problematic decision in this context?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Model C is guaranteed to have the lowest Mean Squared Error (MSE) on unseen data due to its highest $R^2$.",
        "B": "The increase in $R^2$ from Model B to Model C might not be statistically significant, indicating diminishing returns from the additional predictors, and violating the business requirement.",
        "C": "Model C is less likely to be affected by multicollinearity compared to Model A and Model B.",
        "D": "The higher $R^2$ of Model C ensures that all predictors are statistically significant at the 0.05 level."
      },
      "correct_answer": "The increase in $R^2$ from Model B to Model C might not be statistically significant, indicating diminishing returns from the additional predictors, and violating the business requirement.",
      "explanation": "The question tests understanding of $R^2$'s limitations and the importance of considering business constraints. The trick is that simply choosing the highest $R^2$ ignores the trade-off between model complexity and predictive power, especially given the computational constraints. While Model C has the highest $R^2$, the increase from Model B might be marginal and not justify the added complexity. Therefore, the most crucial trade-off is between predictive accuracy and computational efficiency. Option A is incorrect because a higher $R^2$ on training data doesn't guarantee a lower MSE on unseen data (overfitting). Option C is wrong because adding predictors can *increase* multicollinearity. Option D is wrong; a high R-squared does not guarantee that all predictors are significant.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_9",
      "tags": [
        "R-squared",
        "Model Comparison",
        "Overfitting",
        "Business Constraints"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An analyst is building a model to predict sales performance based on advertising spend in three different channels. The initial model includes TV and Radio advertising, resulting in an $R^2$ of 0.75. Adding Social Media advertising increases the $R^2$ to 0.78. However, the marketing team notices that the coefficient for Radio advertising changes sign (positive to negative) after adding Social Media. \n\nWhat is the most likely reason for the coefficient change, and why is relying solely on $R^2$ misleading in this scenario?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The change in sign indicates that the model is now perfectly calibrated, and the increased $R^2$ confirms the improved accuracy.",
        "B": "The change in sign suggests potential multicollinearity between Social Media and Radio advertising, making the individual coefficients unstable and $R^2$ a poor indicator of model quality. This creates a new problem: unreliable coefficient interpretation.",
        "C": "The increased $R^2$ proves that Social Media advertising is a more effective channel than Radio advertising.",
        "D": "The change in sign is expected because the model is now capturing non-linear relationships between the predictors and the response variable."
      },
      "correct_answer": "The change in sign suggests potential multicollinearity between Social Media and Radio advertising, making the individual coefficients unstable and $R^2$ a poor indicator of model quality. This creates a new problem: unreliable coefficient interpretation.",
      "explanation": "This question tests the understanding of multicollinearity and the limitations of $R^2$. The trick is the coefficient sign change, indicating a potential multicollinearity issue masked by the increased $R^2$. While $R^2$ increased, the model's interpretability is compromised. Option A is incorrect because a sign change doesn't indicate perfect calibration. Option C is wrong; it makes a direct channel effectiveness comparison based solely on the increased $R^2$, ignoring the multicollinearity issue. Option D is incorrect; while non-linear relationships can exist, multicollinearity is a more likely explanation for a coefficient sign change.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_9",
      "tags": [
        "R-squared",
        "Multicollinearity",
        "Model Comparison",
        "Coefficient Interpretation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An analyst is tasked with building a model to predict customer spending based on demographics and purchase history. They build two models: Model X with 5 predictors and an $R^2$ of 0.68, and Model Y with 10 predictors and an $R^2$ of 0.72. However, a senior analyst points out that Model Y has a significantly lower adjusted $R^2$ than Model X. The business requirement is to create a parsimonious model which is easy to explain to non-technical stakeholders.\n\nWhy is the lower adjusted $R^2$ of Model Y a concern, and what trade-off is most critical in this scenario?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The lower adjusted $R^2$ indicates that Model Y will likely have a lower Mean Absolute Error (MAE) on the training data.",
        "B": "The lower adjusted $R^2$ suggests that the additional predictors in Model Y do not significantly improve the model's predictive power and might lead to overfitting, making model explainability the critical trade-off.",
        "C": "The adjusted $R^2$ is irrelevant because the $R^2$ of Model Y is higher, indicating better overall fit.",
        "D": "The lower adjusted $R^2$ implies that Model Y is less sensitive to outliers in the data."
      },
      "correct_answer": "The lower adjusted $R^2$ suggests that the additional predictors in Model Y do not significantly improve the model's predictive power and might lead to overfitting, making model explainability the critical trade-off.",
      "explanation": "The question tests the understanding of adjusted $R^2$ and its role in preventing overfitting, along with the business requirement of model explainability. The trick is the conflict between a higher $R^2$ and a lower adjusted $R^2$. The lower adjusted $R^2$ signals that the extra predictors in Model Y are not adding much value and could lead to overfitting, which also hurts model explainability. Option A is incorrect because adjusted $R^2$ is a better indicator of generalization performance than MAE on the training data alone. Option C is incorrect; adjusted $R^2$ is crucial for comparing models with different numbers of predictors. Option D is wrong; adjusted R-squared does not directly indicate sensitivity to outliers.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_10",
      "tags": [
        "Adjusted R-squared",
        "Model Evaluation",
        "Overfitting",
        "Model Explainability"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building a model to predict customer satisfaction scores based on various customer service metrics. They compare two models: Model P with 4 predictors (adjusted $R^2$ = 0.55) and Model Q with 8 predictors (adjusted $R^2$ = 0.52). The company's objective is to maximize the predictive accuracy of the model on *future* customer data, even if it means sacrificing some model simplicity.\n\nWhy might choosing Model P based on its higher adjusted $R^2$ be detrimental to achieving the company's objective?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Model P is guaranteed to have lower bias than Model Q.",
        "B": "The slightly higher adjusted $R^2$ of Model P might be offset by the potential for Model Q to capture more complex relationships and generalize better to future data, despite the penalty for complexity, especially considering the objective.",
        "C": "Model P is less likely to suffer from multicollinearity due to having fewer predictors.",
        "D": "The adjusted $R^2$ is not relevant because the company's objective is to maximize predictive accuracy on *training* data, not future data."
      },
      "correct_answer": "The slightly higher adjusted $R^2$ of Model P might be offset by the potential for Model Q to capture more complex relationships and generalize better to future data, despite the penalty for complexity, especially considering the objective.",
      "explanation": "This question tests the understanding of adjusted $R^2$ and its limitations, especially when the goal is to maximize prediction on *future* data. The trick lies in the business objective conflicting with the typical interpretation of adjusted R-squared. While Model P has a slightly higher adjusted $R^2$, the question highlights that model Q has the potential to perform better on future data, therefore the trade off is complexity vs better capture of relationships within the data. Option A is incorrect, it is not guaranteed to have lower bias. Option C is incorrect because it is less likely to have multicollinearity, however that is not the focus of the question. Option D is incorrect, the objective is to maximize predictive accuracy on future data.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_10",
      "tags": [
        "Adjusted R-squared",
        "Model Evaluation",
        "Overfitting",
        "Generalization"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing team develops a multiple regression model to predict sales based on advertising spend in different channels (TV, Radio, Online). The Global F-Test yields a p-value of 0.06, slightly above the significance level of 0.05. Despite this, the team proceeds to interpret the individual t-tests, finding that the coefficient for 'Online' advertising is statistically significant (p < 0.05). They decide to increase spending on Online advertising. \n\nWhat is the most likely error in this approach, and what potential problem could arise from this decision?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The team is correct to focus on the significant t-test for 'Online' advertising, as it indicates a real effect even if the Global F-Test is not significant.",
        "B": "The team is committing a Type II error by failing to reject the null hypothesis of the Global F-Test, and therefore missing a potentially significant overall model.",
        "C": "The team is potentially drawing incorrect conclusions because the non-significant Global F-Test suggests that the model as a whole does not have significant explanatory power, making the individual t-tests unreliable. Increasing spending on online advertising might not lead to the predicted sales increase.",
        "D": "The team should lower the significance level to 0.06 to make the Global F-Test significant and validate their approach."
      },
      "correct_answer": "The team is potentially drawing incorrect conclusions because the non-significant Global F-Test suggests that the model as a whole does not have significant explanatory power, making the individual t-tests unreliable. Increasing spending on online advertising might not lead to the predicted sales increase.",
      "explanation": "This question tests the understanding of the Global F-Test and its importance in validating individual t-tests. The trick is the non-significant Global F-Test combined with a significant individual t-test. Since the Global F-Test is not significant, the entire model lacks explanatory power, meaning that the significant t-test on the online is not reliable. Option A is incorrect because it ignores the primary role of the global F test. Option B is wrong because focusing on individual coefficients makes sense only if the global F test is significant. Option D is incorrect, lowering the significance level would be poor methodology.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_11",
      "tags": [
        "Global F-Test",
        "Hypothesis Testing",
        "t-test",
        "Model Validation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A researcher builds a multiple regression model to predict student performance on a standardized test based on study hours, attendance, and socioeconomic status (SES). The Global F-Test is significant (p < 0.05). The t-test for 'SES' is not significant (p = 0.20). The researcher concludes that SES has no impact on student performance and removes it from the model. \n\nWhat is the most likely error in the researcher's conclusion, and what alternative explanation should they consider before removing 'SES' from the model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The researcher is correct; a non-significant t-test definitively proves that SES has no impact on student performance.",
        "B": "The researcher is committing a Type II error by failing to reject the null hypothesis for SES, and should therefore increase the sample size.",
        "C": "The researcher is overlooking the possibility that SES might be correlated with other predictors in the model (e.g., attendance, study hours), and its effect is already captured by those variables. Removing SES could lead to omitted variable bias in the remaining coefficients.",
        "D": "The researcher should use a one-tailed t-test instead of a two-tailed t-test to increase the chances of finding a significant effect for SES."
      },
      "correct_answer": "The researcher is overlooking the possibility that SES might be correlated with other predictors in the model (e.g., attendance, study hours), and its effect is already captured by those variables. Removing SES could lead to omitted variable bias in the remaining coefficients.",
      "explanation": "This question tests the understanding of individual t-tests and the potential for multicollinearity or omitted variable bias. The trick is the non-significant t-test for SES, which doesn't necessarily mean it has *no* effect. It means it has no significant effect *given the other predictors*. If SES is correlated with other predictors, its effect might be masked, and removing it could bias the remaining coefficients. Option A is incorrect because a non-significant t-test doesn't prove no effect. Option B is incorrect, increasing the sample size won't solve the issue of SES not being significant *given other predictors*. Option D is incorrect, using a one tailed test would be inappropriate.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_12",
      "tags": [
        "t-test",
        "Multicollinearity",
        "Omitted Variable Bias",
        "multiple regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate company is building a multiple regression model to predict house prices based on square footage, number of bedrooms, location (represented by a neighborhood score), and the age of the house. The t-test for 'age of the house' has a high p-value (e.g., 0.6), indicating it's not a statistically significant predictor given the other variables in the model. However, the company's CEO insists on keeping 'age of the house' in the model because it is a common feature buyers consider, and they believe it provides important context. \n\nWhat is the most appropriate justification for keeping 'age of the house' in the model, despite its non-significant t-test, and what potential problem could arise if it is removed?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The 'age of the house' should be removed to improve the model's adjusted R-squared and prevent overfitting.",
        "B": "Keeping 'age of the house' is justified if it is theoretically important or serves as a control variable, even if not statistically significant. Removing it could lead to omitted variable bias or make the model less useful for practical decision-making.",
        "C": "The high p-value indicates that 'age of the house' is perfectly multicollinear with other predictors and must be removed to stabilize the model.",
        "D": "The company should perform a variable selection technique (e.g., stepwise regression) to automatically determine whether 'age of the house' should be included."
      },
      "correct_answer": "Keeping 'age of the house' is justified if it is theoretically important or serves as a control variable, even if not statistically significant. Removing it could lead to omitted variable bias or make the model less useful for practical decision-making.",
      "explanation": "This question tests the understanding of the t-test and the importance of theoretical considerations in model building. The trick is the CEO's insistence on keeping a non-significant variable. Even if 'age of the house' isn't statistically significant *given the other variables*, it might be theoretically important or act as a control variable. Removing it could lead to omitted variable bias or make the model less realistic from a business perspective. Option A is wrong, the variable should be kept. Option C is incorrect because it is not perfectly multicollinear. Option D is incorrect, while variable selection techniques are useful, the CEO's decision justifies keeping the variable regardless of model selection.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_12",
      "tags": [
        "t-test",
        "multiple regression",
        "Omitted Variable Bias",
        "Theoretical Importance"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate company uses multiple regression to predict house prices based on square footage, number of bedrooms, and location. They calculate a 95% Confidence Interval (CI) of $450,000 - $470,000 for houses with 2000 sq ft, 3 bedrooms, in a specific neighborhood. The company's sales team then uses this CI to tell a prospective buyer that '95% of houses like this will sell within this price range.' Why is this usage of the Confidence Interval potentially misleading to the buyer?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The Confidence Interval only estimates the range for the *average* price of similar houses, not the price of any *individual* house.",
        "B": "The sales team should have used a Prediction Interval (PI), which provides a range for a *single* house's selling price.",
        "C": "The Confidence Interval is only valid if the underlying data is perfectly normally distributed, which is unlikely for house prices.",
        "D": "The Confidence Interval only accounts for the variables included in the model, neglecting other factors that influence house prices, such as the condition of the house or current market trends."
      },
      "correct_answer": "B",
      "explanation": "The trick here is confusing the CI with the PI. The CI estimates the average selling price of *all* houses meeting the criteria, while the buyer is interested in the likely price of *one specific* house. Therefore, using the CI to predict the selling price of a single house is misleading. A Prediction Interval would be more appropriate. Option A is partially correct but misses the core issue of needing a PI for a single observation. Option C brings in a potentially irrelevant assumption about normality. Option D is true but doesn't address the CI vs. PI confusion.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An environmental agency develops a multiple regression model to predict air quality index (AQI) based on traffic volume, industrial emissions, and weather conditions. They calculate both a 99% Confidence Interval (CI) and a 99% Prediction Interval (PI) for the AQI on a specific day, given the predicted traffic, emissions, and weather. The CI is significantly narrower than the PI. The agency's spokesperson states: 'The narrow Confidence Interval proves our model is highly accurate in predicting air quality.' What is the most significant flaw in this statement?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The Confidence Interval only reflects the precision of the *average* AQI prediction, not the accuracy of any *single day's* AQI. The wider Prediction Interval is a better reflection of the model's real-world predictive power.",
        "B": "The high confidence level (99%) is unnecessary and may lead to overfitting, reducing the model's generalizability to new data.",
        "C": "The model's accuracy should be evaluated using metrics like R-squared and RMSE, not by comparing the widths of CI and PI.",
        "D": "The agency should validate the model using a holdout dataset before making any claims about its accuracy."
      },
      "correct_answer": "A",
      "explanation": "The trick lies in understanding what the CI represents. A narrow CI indicates a precise estimate of the *average* AQI, but it says nothing about the model's ability to accurately predict the AQI on a *specific* day. The wider PI, which accounts for individual variation, is more relevant to assessing the model's real-world predictive power. Option B introduces overfitting, which is not directly related to the CI/PI distinction. Option C mentions relevant metrics but doesn't address the core misunderstanding of CI's meaning. Option D is good practice, but again, doesn't highlight the CI/PI issue.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A hospital uses multiple regression to predict patient length of stay (LOS) based on age, number of pre-existing conditions, and severity score. They need to estimate the impact of a new treatment protocol on patient LOS. They calculate both a 95% Confidence Interval (CI) and a 95% Prediction Interval (PI) for LOS *after* the protocol is implemented. Which interval would be MOST appropriate to use if the hospital wants to ensure that at least 95% of *individual patients* will have a length of stay shorter than the upper bound of the calculated interval?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The Confidence Interval (CI), because it estimates the range for the *average* LOS, and the hospital wants to minimize the average LOS.",
        "B": "The Prediction Interval (PI), because it estimates the range for a *single patient's* LOS, and the hospital wants to ensure most individual patients benefit.",
        "C": "Either the Confidence Interval (CI) or the Prediction Interval (PI) can be used, as long as the confidence level is set at 95%.",
        "D": "Neither the Confidence Interval (CI) nor the Prediction Interval (PI) is suitable. They should use a tolerance interval instead as it is designed to contain a certain proportion of the population."
      },
      "correct_answer": "D",
      "explanation": "The trick here is understanding the subtle difference between prediction intervals and tolerance intervals, and that the hospital wants to contain a certain *proportion of the population*. While Prediction Intervals predict a range for a single new observation, they don't guarantee coverage of a specific proportion of the population. A tolerance interval is specifically designed for this purpose. Option A is incorrect because the hospital's goal is about individual patients, not the average. Option B is better, but still wrong because a PI doesn't guarantee coverage of 95% of the population. Option C is incorrect since the two intervals have different purposes.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A financial analyst builds a multiple regression model to predict stock prices based on market indices, company earnings, and interest rates. They calculate a 90% Confidence Interval (CI) for the predicted stock price of Company X next quarter. Subsequently, a major, unexpected event (e.g., a product recall) occurs that is *not* captured by the model's input variables. How does this event MOST likely affect the validity of the calculated Confidence Interval?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The Confidence Interval remains valid as long as the model's assumptions (linearity, independence, normality, equal variance) are still met, regardless of external events.",
        "B": "The Confidence Interval is no longer valid because the unexpected event introduces a source of error that is not accounted for in the model, potentially shifting the true mean outside the calculated range.",
        "C": "The Confidence Interval will become wider to reflect the increased uncertainty caused by the unexpected event.",
        "D": "The Confidence Interval will become narrower because the market will quickly adjust to the new information, reducing the variability in stock prices."
      },
      "correct_answer": "B",
      "explanation": "The trick is that real-world events violate model assumptions. While statistical assumptions are important, the *unforeseen event* is the key. The CI is based on the model's inputs. An external shock introduces bias that the CI cannot quantify, rendering it invalid. Option A is dangerously wrong; models are *always* simplifications. Option C is plausible, but the CI calculation is *fixed* based on the original data. Option D is wishful thinking and ignores the potential for negative impacts.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A supply chain manager uses multiple regression to predict delivery times based on distance, weather conditions, and traffic density. After building the model, they need to estimate the range of delivery times for *all* shipments on a specific route, given the predicted weather and traffic. They calculate both a 95% Confidence Interval (CI) and a 95% Prediction Interval (PI). However, they also know that 10% of shipments are handled by a new, less experienced team, which introduces additional variability *not captured by the model*. Which interval should the manager use, and how should they interpret it considering the unmodeled variability?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The Confidence Interval (CI), but acknowledge that the true average delivery time may fall slightly outside the calculated range due to the unmodeled variability.",
        "B": "The Prediction Interval (PI), but recognize that it may *underestimate* the actual range of delivery times because it does not account for the additional variability from the new team.",
        "C": "The Confidence Interval (CI), because it is more robust to outliers and unexpected variations in the data.",
        "D": "Neither the Confidence Interval (CI) nor the Prediction Interval (PI) are suitable. The manager should rebuild the model to include a variable representing the experience level of the delivery team."
      },
      "correct_answer": "B",
      "explanation": "The trick is recognizing the limitations of the model and the PI in the face of unmodeled variability. The manager wants to estimate the range for *all* shipments, making the PI the more appropriate choice. However, because the model *doesn't* account for the new team's variability, the calculated PI will likely be too narrow. Option A is wrong because it suggests using the CI for a single observation. Option C confuses robustness with accuracy. Option D is ideal, but not always feasible in the short term, so it's not the *most* appropriate choice given the immediate need for an estimate.",
      "difficulty_level": 3,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting"
      ]
    }
  ]
}