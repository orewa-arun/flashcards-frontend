{
  "metadata": {
    "generated_at": "2025-11-04T10:26:34.708110",
    "total_questions": 25,
    "course_name": "Data Analysis Applications",
    "course_id": "MS5031",
    "course_code": "DAA",
    "textbook_reference": "Statistics for Business: Decision Making and Analysis by Robert E Stine and Dean Foster, Pearson (ISBN: 978-81-317-3347-9)",
    "lecture": "DAA_lec_4",
    "difficulty_level": 4,
    "source_flashcards": 13
  },
  "questions": [
    {
      "type": "mcq",
      "question_text": "A data scientist is building a model to predict customer churn at a telecommunications company. They use Multiple Linear Regression (MLR) with several independent variables, including monthly bill amount, number of customer service calls, and contract length. Upon reviewing the model, they find that the coefficient for 'monthly bill amount' is positive and statistically significant, suggesting that higher bills lead to increased churn. However, they also observe a high degree of correlation between 'monthly bill amount' and 'contract length' (longer contracts often have lower monthly bills). Considering the concept of 'partial regression coefficients' and the challenges of multicollinearity, what is the MOST likely explanation for the unexpected positive coefficient for 'monthly bill amount' in this scenario?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The positive coefficient is accurate and reflects a genuine causal relationship; customers with higher bills are simply more likely to churn, regardless of their contract length.",
        "B": "Multicollinearity is suppressing the true effect of 'monthly bill amount'. The partial regression coefficient reflects the effect of bill amount *after* controlling for contract length, meaning that *for customers with the same contract length*, those with higher bills are more likely to churn.",
        "C": "The model is suffering from omitted variable bias. A critical variable related to customer satisfaction (e.g., network reliability) has been left out, leading to a spurious correlation between bill amount and churn.",
        "D": "The data contains outliers with extremely high bill amounts and short contract lengths, unduly influencing the regression results. Removing these outliers will likely eliminate the positive coefficient."
      },
      "correct_answer": "Multicollinearity is suppressing the true effect of 'monthly bill amount'. The partial regression coefficient reflects the effect of bill amount *after* controlling for contract length, meaning that *for customers with the same contract length*, those with higher bills are more likely to churn.",
      "explanation": "This question combines the concepts of multicollinearity (addressed implicitly in the 'common mistakes' of the MLR definition flashcard) and partial regression coefficients (DAA_lec_4_2). The key is understanding that the partial regression coefficient represents the effect of a variable *holding all others constant*. Multicollinearity inflates the variance of the coefficients, making interpretation difficult. Option A ignores the multicollinearity. Option C introduces an unrelated issue (omitted variable bias). Option D is a possible but less likely explanation, as multicollinearity is more fundamental here. The correct answer (B) highlights that the positive coefficient, while counterintuitive, reflects the effect of bill amount *after* controlling for contract length. Customers with the *same* contract length are more likely to churn if they have higher bills. The multicollinearity makes the overall interpretation complex, requiring careful consideration of the partial effect.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_1",
      "tags": [
        "multiple linear regression",
        "partial slope",
        "multicollinearity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A financial analyst is developing an MLR model to predict stock prices based on several factors, including the company's earnings per share (EPS), price-to-earnings (P/E) ratio, and debt-to-equity ratio. Before finalizing the model, the analyst examines a Scatterplot Matrix (Scatmat) of all the variables. The Scatmat reveals a strong linear relationship between EPS and stock price, as expected. However, it also shows a very strong positive relationship between P/E ratio and *analyst recommendations* (on a scale of 1 to 5, with 5 being a 'strong buy'). Furthermore, the analyst knows from prior experience (and external data) that analyst recommendations strongly influence stock price. Synthesizing the role of a Scatmat in EDA and the concept of partial regression coefficients, what is the MOST likely consequence of including both P/E ratio and analyst recommendations in the MLR model?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Including both variables will improve the model's predictive power because they both independently contribute to explaining stock price variation. The partial regression coefficients will accurately reflect each variable's unique contribution.",
        "B": "Including both variables will likely lead to multicollinearity issues, making it difficult to interpret the individual coefficients and potentially inflating their standard errors. However, the overall predictive power of the model will remain unaffected.",
        "C": "Including both variables will likely result in a spurious relationship between P/E ratio and stock price. The partial regression coefficient for P/E ratio will be biased because it doesn't account for the confounding effect of analyst recommendations.",
        "D": "The model will suffer from endogeneity bias. Analyst recommendations are likely influenced by the current stock price, creating a feedback loop that violates the assumptions of MLR and leads to inconsistent coefficient estimates."
      },
      "correct_answer": "Including both variables will likely lead to multicollinearity issues, making it difficult to interpret the individual coefficients and potentially inflating their standard errors. However, the overall predictive power of the model will remain unaffected.",
      "explanation": "This question combines the purpose of Scatmat (DAA_lec_4_4) with multicollinearity (addressed implicitly in the 'common mistakes' of DAA_lec_4_1). The Scatmat reveals a strong relationship between P/E and analyst recommendations. This indicates likely multicollinearity. Option A is incorrect because multicollinearity *does* impact the interpretability of individual coefficients. Option C describes a spurious relationship and confounding, but multicollinearity is the more direct consequence here. Option D introduces endogeneity, which is a more complex issue not directly indicated by the Scatmat alone. The correct answer (B) acknowledges the multicollinearity and its impact on coefficient interpretation and standard errors. The overall predictive power *might* be okay, but individual coefficient analysis becomes problematic. This is a trade-off: potentially better prediction vs. interpretable parameters.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_4",
      "tags": [
        "EDA",
        "scatterplot matrix",
        "multicollinearity",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An environmental scientist is building an MLR model to predict air quality (measured as an air quality index, AQI) based on several pollutant concentrations (PM2.5, PM10, Ozone, NO2) and meteorological factors (temperature, wind speed, humidity). After fitting the model, the scientist observes significant heteroscedasticity in the residuals â€“ the variance of the errors is much larger for high AQI values than for low AQI values. Knowing that heteroscedasticity violates one of the key assumptions of MLR and considering potential remedies, which of the following approaches is MOST appropriate to address this issue while still maintaining the interpretability of the model's coefficients?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Transform the independent variables (pollutant concentrations and meteorological factors) using a Box-Cox transformation to achieve normality. This will automatically correct for heteroscedasticity.",
        "B": "Apply a White's test to formally confirm the presence of heteroscedasticity. If confirmed, remove the independent variable with the highest variance inflation factor (VIF) to reduce multicollinearity and, consequently, heteroscedasticity.",
        "C": "Use Weighted Least Squares (WLS) regression, where observations with lower variance (lower AQI values) are given higher weights and observations with higher variance (higher AQI values) are given lower weights. This directly addresses the unequal variance.",
        "D": "Transform the dependent variable (AQI) using a logarithmic transformation. This often stabilizes the variance and reduces heteroscedasticity, while still allowing for a relatively straightforward interpretation of the coefficients (as percentage changes)."
      },
      "correct_answer": "Transform the dependent variable (AQI) using a logarithmic transformation. This often stabilizes the variance and reduces heteroscedasticity, while still allowing for a relatively straightforward interpretation of the coefficients (as percentage changes).",
      "explanation": "This question combines the assumptions of MLR (DAA_lec_4_3) with potential remedies for violating those assumptions. Heteroscedasticity violates the equal variance assumption. Option A is incorrect; Box-Cox transforms address non-normality, not heteroscedasticity directly. Option B suggests removing a variable based on VIF (multicollinearity), which is unrelated to heteroscedasticity. Option C, Weighted Least Squares, is a valid approach but requires careful estimation of the weights and can be less intuitive to interpret. The logarithmic transformation of the dependent variable (D) is often the *most* practical solution. It stabilizes the variance and allows for interpretation of the coefficients as approximate percentage changes, balancing correction with interpretability. The key is recognizing that transforming the *dependent* variable is often more effective and interpretable than transforming the *independent* variables in this scenario.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_3",
      "tags": [
        "mlr assumptions",
        "equal variance",
        "heteroscedasticity",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A retail company is using MLR to predict weekly sales at its various store locations based on advertising spend, local demographics, and promotional activities. After building the model, the analysts notice that the residuals exhibit a clear pattern of non-independence: weeks with higher-than-predicted sales tend to be followed by weeks with higher-than-predicted sales, and vice-versa. This suggests a violation of the 'Independence of Errors' assumption of MLR. Considering potential causes and solutions for this issue, which of the following is the MOST likely root cause of this violation, and what is the MOST appropriate remedy?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Root cause: Multicollinearity between advertising spend and promotional activities is causing the non-independence. Remedy: Remove one of the multicollinear variables from the model.",
        "B": "Root cause: Omitted variable bias due to unmeasured factors like seasonal trends or competitor actions influencing sales. Remedy: Incorporate lagged values of the dependent variable (previous week's sales) as an additional predictor in the model.",
        "C": "Root cause: Heteroscedasticity in the data is creating the illusion of non-independence. Remedy: Apply a logarithmic transformation to the dependent variable (sales) to stabilize the variance.",
        "D": "Root cause: Non-linearity in the relationship between advertising spend and sales. Remedy: Use a polynomial regression model or add interaction terms to capture the non-linear relationship."
      },
      "correct_answer": "Root cause: Omitted variable bias due to unmeasured factors like seasonal trends or competitor actions influencing sales. Remedy: Incorporate lagged values of the dependent variable (previous week's sales) as an additional predictor in the model.",
      "explanation": "This question combines the assumptions of MLR (DAA_lec_4_3) with potential causes and solutions for violating the independence assumption. The key is the *temporal* pattern in the residuals. Option A is incorrect because multicollinearity doesn't directly cause non-independence of *errors*. Option C is incorrect because heteroscedasticity (unequal variance) is a different issue than non-independence. Option D addresses non-linearity, which doesn't explain the temporal pattern in the residuals. The correct answer (B) identifies omitted variable bias as the most likely root cause. The temporal pattern suggests that unmeasured factors (like seasonality or competitor actions) are influencing sales and are correlated over time. Including lagged values of the dependent variable (previous week's sales) captures some of this temporal dependence, addressing the non-independence of errors. This introduces autocorrelation but mitigates the violation of the independence assumption.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_3",
      "tags": [
        "mlr assumptions",
        "independence",
        "omitted variable bias",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate firm is using MLR to predict property values. They have collected data on square footage, number of bedrooms, location (distance to city center), and age of the property. After building the model and examining a Scatterplot Matrix (Scatmat), they notice a very strong positive correlation (close to 1) between 'square footage' and 'number of bedrooms'. Furthermore, the model's variance inflation factors (VIFs) for both these variables are extremely high (above 10). The firm wants to provide easily interpretable results for their clients. Given the problem of multicollinearity and the need for clear interpretations, which of the following strategies offers the BEST balance between addressing multicollinearity and maintaining model interpretability for communicating insights to clients with varying levels of statistical understanding?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Remove 'number of bedrooms' from the model. This will eliminate the multicollinearity issue and simplify the model, making it easier to interpret. The slight loss in predictive power is acceptable.",
        "B": "Create an interaction term between 'square footage' and 'number of bedrooms'. This will capture the combined effect of these variables and eliminate the multicollinearity.",
        "C": "Use Principal Component Analysis (PCA) to create a new set of uncorrelated variables from the original predictors, including 'square footage' and 'number of bedrooms'. Then, use these principal components in the MLR model.",
        "D": "Combine 'square footage' and 'number of bedrooms' into a single composite variable (e.g., 'living space index') by averaging or weighting them based on their individual contributions to property value. Then, use this index in the MLR model."
      },
      "correct_answer": "Combine 'square footage' and 'number of bedrooms' into a single composite variable (e.g., 'living space index') by averaging or weighting them based on their individual contributions to property value. Then, use this index in the MLR model.",
      "explanation": "This question combines the use of Scatmat (DAA_lec_4_4) for detecting multicollinearity with strategies for addressing it, emphasizing interpretability. Option A, removing a variable, is a common approach but sacrifices information and might not be the *best* solution if both variables are relevant. Option B, creating an interaction term, can address non-linearity or synergistic effects but does not directly solve multicollinearity and can complicate interpretation. Option C, PCA, creates uncorrelated variables but makes the model *less* interpretable because the principal components are abstract combinations of the original variables. The correct answer (D), creating a composite variable, directly addresses multicollinearity *and* maintains interpretability. A 'living space index' is easily understood by clients and captures the combined effect of square footage and bedrooms. This approach balances the need to address multicollinearity with the need for clear, understandable results for a non-technical audience. The trade-off is potential information loss in creating the index, but this is acceptable given the strong multicollinearity and the emphasis on interpretability.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_4",
      "tags": [
        "EDA",
        "scatterplot matrix",
        "multicollinearity",
        "multiple linear regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building a predictive model for customer lifetime value (CLTV) using multiple linear regression. After initial analysis, they observe a correlation matrix showing a strong positive correlation (0.85) between 'customer tenure' and 'total purchases,' and a moderate positive correlation (0.6) between 'customer tenure' and CLTV. Further investigation using a scatterplot matrix reveals that the relationship between 'customer tenure' and CLTV plateaus after five years, suggesting a non-linear association. The data scientist decides to remove 'customer tenure' from the model to avoid multicollinearity issues, focusing solely on 'total purchases' and other independent variables. Synthesizing the concepts of correlation matrices, scatterplot matrices, and ceteris paribus, what is the most likely *unintended consequence* of this decision, and why?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model's $R^2$ will significantly increase, as removing a multicollinear variable always improves model fit, ceteris paribus.",
        "B": "The model will overestimate CLTV for short-tenured customers because the effect of 'total purchases' will now be partially capturing the 'customer tenure' effect, violating the ceteris paribus condition.",
        "C": "The model will underestimate CLTV for long-tenured customers, as the linear relationship between 'total purchases' and CLTV will fail to capture the non-linear plateau effect of 'customer tenure,' violating the ceteris paribus condition.",
        "D": "Removing 'customer tenure' will eliminate all multicollinearity issues, guaranteeing a more accurate and interpretable model, regardless of the observed non-linear relationship."
      },
      "correct_answer": "The model will underestimate CLTV for long-tenured customers, as the linear relationship between 'total purchases' and CLTV will fail to capture the non-linear plateau effect of 'customer tenure,' violating the ceteris paribus condition.",
      "explanation": "This question synthesizes correlation matrices (Lec 4.5), scatterplot matrices (Lec 4.6), and ceteris paribus (Lec 4.7). The data scientist's action *partially* addresses multicollinearity but overlooks the non-linear relationship revealed by the scatterplot matrix. Option A is incorrect because removing a variable doesn't *always* increase $R^2$, especially if it contains unique information. Option B is plausible because 'total purchases' might capture some of the tenure effect, but the *main* problem is the loss of the non-linear plateau. Option D is wrong because removing one variable doesn't guarantee perfect model accuracy. The correct answer (C) identifies that by removing 'customer tenure', the model loses its ability to account for the plateau in CLTV after five years. The ceteris paribus condition is violated because the effect of 'total purchases' is now being interpreted *without* accounting for the effect of customer tenure, especially at longer tenures.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "scatterplot matrix",
        "multicollinearity",
        "EDA",
        "MLR",
        "Ceteris Paribus"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst is tasked with identifying key factors influencing online sales conversions. They begin by creating a correlation matrix and scatterplot matrix using historical data, which includes variables like 'website traffic,' 'average order value,' 'customer reviews,' and 'promotion frequency.' The correlation matrix reveals a strong positive correlation (0.75) between 'promotion frequency' and 'website traffic,' and a weaker positive correlation (0.3) between 'promotion frequency' and 'online sales conversions.' The scatterplot matrix, however, shows that 'promotion frequency' initially increases conversions, but beyond a certain point, the relationship plateaus and even declines, showing diminishing returns. The analyst decides to build a multiple linear regression model, including only 'website traffic,' 'average order value,' and 'customer reviews,' citing concerns about multicollinearity between 'promotion frequency' and 'website traffic.' Synthesizing the concepts of correlation matrices, scatterplot matrices, and the coefficient of determination ($R^2$), what is the MOST likely consequence of the analyst's decision?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The $R^2$ of the model will significantly increase, as multicollinearity always reduces the explanatory power of a regression model.",
        "B": "The model will provide a more accurate prediction of online sales conversions, as excluding 'promotion frequency' will eliminate the confounding effect of multicollinearity.",
        "C": "The model will likely underestimate the impact of marketing efforts on online sales conversions, as the model fails to capture the initial positive effect of 'promotion frequency' on sales, despite the later diminishing returns.",
        "D": "The model will overestimate the impact of website traffic, due to the correlation with promotion frequency, but will still provide a better estimate of conversion than a model with promotion frequency."
      },
      "correct_answer": "The model will likely underestimate the impact of marketing efforts on online sales conversions, as the model fails to capture the initial positive effect of 'promotion frequency' on sales, despite the later diminishing returns.",
      "explanation": "This question synthesizes correlation matrices (Lec 4.5), scatterplot matrices (Lec 4.6), and the coefficient of determination ($R^2$) (Lec 4.8). The analyst's decision to exclude 'promotion frequency' addresses multicollinearity but ignores the insight from the scatterplot matrix that 'promotion frequency' *does* have an initial positive impact on conversions. Option A is incorrect because removing a correlated variable doesn't *always* increase $R^2$. Option B is tempting because of the multicollinearity concern, but the loss of information about the initial positive effect outweighs this benefit. Option D is incorrect because while there might be some overestimation of the isolated impact of website traffic, the larger issue is the overall underestimation of marketing efforts. The correct answer (C) highlights that the model will underestimate marketing effectiveness because it completely misses the initial positive impact of promotions, even though the effect diminishes later. The scatterplot matrix shows that there is *some* predictive power in promotion frequency, even if it is non-linear and has diminishing returns.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "scatterplot matrix",
        "multicollinearity",
        "EDA",
        "R-squared",
        "Coefficient of Determination"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate analyst is building a model to predict housing prices in a metropolitan area. They collect data on several variables, including 'square footage,' 'number of bedrooms,' 'location score' (a composite index), and 'year built.' The analyst observes a strong positive correlation (0.8) between 'square footage' and 'number of bedrooms' in the correlation matrix. The scatterplot matrix shows a generally linear relationship between 'square footage' and 'price,' but reveals a cluster of new, small, expensive apartments with high 'location scores.' To address the multicollinearity, the analyst decides to remove 'number of bedrooms' from the regression model. They also notice the overall $R^2$ is around 0.75. Synthesizing the concepts of correlation matrices, scatterplot matrices, the coefficient of determination, and ceteris paribus, what is the MOST likely consequence of this modeling choice?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model will now accurately predict housing prices because multicollinearity has been eliminated, leading to unbiased coefficient estimates, holding all else constant.",
        "B": "The coefficient for 'square footage' will now better reflect the true impact of size on price, as it is no longer confounded by the number of bedrooms.",
        "C": "The model will likely underestimate prices for larger homes with fewer bedrooms (e.g., modern open-concept designs), as the effect of 'square footage' is now being interpreted assuming a 'typical' number of bedrooms for a given square footage, violating ceteris paribus.",
        "D": "The $R^2$ of the model will increase, because number of bedrooms does not add substantial predictive power to a house price model given the inclusion of square footage."
      },
      "correct_answer": "The model will likely underestimate prices for larger homes with fewer bedrooms (e.g., modern open-concept designs), as the effect of 'square footage' is now being interpreted assuming a 'typical' number of bedrooms for a given square footage, violating ceteris paribus.",
      "explanation": "This question integrates correlation matrices (Lec 4.5), scatterplot matrices (Lec 4.6), the coefficient of determination ($R^2$) (Lec 4.8), and ceteris paribus (Lec 4.7). Removing 'number of bedrooms' addresses multicollinearity but creates a bias. Option A is too optimistic; multicollinearity is reduced, but bias is introduced. Option B is partially correct; the coefficient for 'square footage' *changes*, but it doesn't necessarily become *more* accurate overall. Option D is not necessarily true; $R^2$ may decrease slightly. The correct answer (C) pinpoints the key problem: by removing 'number of bedrooms,' the model implicitly assumes a 'typical' relationship between square footage and bedrooms. This means that houses with significantly *fewer* bedrooms than average for their size will be systematically undervalued because the model doesn't account for the premium associated with open-concept layouts. The ceteris paribus condition is violated because the effect of square footage is now being interpreted *without* holding number of bedrooms constant; it's being interpreted *assuming* a certain number of bedrooms.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "scatterplot matrix",
        "multicollinearity",
        "EDA",
        "R-squared",
        "Coefficient of Determination",
        "Ceteris Paribus"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data analyst is exploring factors influencing customer satisfaction scores for a subscription-based service. They create both a correlation matrix and a scatterplot matrix. The correlation matrix shows a strong positive correlation (0.8) between 'number of support tickets resolved' and 'average resolution time,' and a moderate positive correlation (0.5) between 'number of support tickets resolved' and 'customer satisfaction score'. However, the scatterplot matrix reveals that the relationship between 'number of support tickets resolved' and 'customer satisfaction score' is non-linear: up to a certain number of resolved tickets, satisfaction increases, but beyond that point, it plateaus and even decreases. The analyst decides to exclude 'number of support tickets resolved' from a multiple linear regression model predicting customer satisfaction, citing multicollinearity with 'average resolution time'. Synthesizing the concepts of correlation matrices, scatterplot matrices, the coefficient of determination ($R^2$), and the meaning of ceteris paribus in MLR, what is the MOST probable outcome of this decision?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model's $R^2$ will significantly increase because removing a multicollinear variable always improves the model's overall fit and predictive accuracy, ceteris paribus.",
        "B": "The model will become more interpretable and accurate, as eliminating multicollinearity ensures that the coefficients of the remaining variables are unbiased and easier to understand.",
        "C": "The model will likely underestimate customer satisfaction for customers with a moderate number of resolved support tickets, as it fails to capture the initial positive effect of 'number of support tickets resolved,' while potentially overestimating satisfaction for those with very high numbers of resolved tickets.",
        "D": "The model will overestimate the negative impact of 'average resolution time' on customer satisfaction, as the effect of longer resolution times will now be partially confounded with the number of resolved tickets, violating the ceteris paribus condition."
      },
      "correct_answer": "The model will likely underestimate customer satisfaction for customers with a moderate number of resolved support tickets, as it fails to capture the initial positive effect of 'number of support tickets resolved,' while potentially overestimating satisfaction for those with very high numbers of resolved tickets.",
      "explanation": "This question combines correlation matrices (Lec 4.5), scatterplot matrices (Lec 4.6), the coefficient of determination ($R^2$) (Lec 4.8), and ceteris paribus (Lec 4.7). While removing 'number of support tickets resolved' might reduce multicollinearity, it disregards the information from the scatterplot matrix indicating a non-linear relationship. Option A is incorrect; removing a variable doesn't always increase $R^2$. Option B is partially correct; interpretability *might* improve, but accuracy will likely suffer. Option D is plausible but secondary; while the effect of 'average resolution time' might be slightly distorted, the primary issue is the lost information about the *positive* impact of resolving a moderate number of tickets. The correct answer (C) emphasizes that the model will likely misrepresent customer satisfaction. By excluding 'number of support tickets resolved', the model fails to account for the initial positive effect of ticket resolution, leading to underestimation for customers with a moderate number of resolved tickets. Additionally, it may overestimate satisfaction for those with a very *high* number of resolved tickets because it doesn't capture the plateau/decline in satisfaction at high resolution volumes.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "scatterplot matrix",
        "multicollinearity",
        "EDA",
        "R-squared",
        "Coefficient of Determination",
        "Ceteris Paribus"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A financial analyst is building a multiple linear regression model to predict stock prices. The data includes 'company revenue,' 'debt-to-equity ratio,' 'price-to-earnings ratio (P/E),' and 'analyst ratings.' The correlation matrix reveals a strong positive correlation (0.9) between 'company revenue' and 'analyst ratings,' and a moderate positive correlation (0.6) between 'company revenue' and 'stock price.' The scatterplot matrix shows a generally linear relationship between 'company revenue' and 'stock price,' but also reveals that companies with exceptionally *low* debt-to-equity ratios and high revenue tend to have significantly higher stock prices than predicted by the linear trend. The analyst decides to remove 'company revenue' from the model to address the multicollinearity, rationalizing that 'analyst ratings' capture most of the information contained in 'company revenue.' Synthesizing the concepts of correlation matrices, scatterplot matrices, coefficient of determination ($R^2$), and ceteris paribus, what is the MOST likely consequence of this decision?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model's $R^2$ will remain largely unchanged, as 'analyst ratings' effectively proxy for 'company revenue', thus maintaining the model's predictive power.",
        "B": "The model will accurately predict stock prices for most companies, as the multicollinearity issue has been resolved, leading to more reliable coefficient estimates for the remaining variables, ceteris paribus.",
        "C": "The model will likely underestimate the stock prices of companies with exceptionally low debt-to-equity ratios and high revenue, as it loses the ability to capture the synergistic effect of these two factors on stock price.",
        "D": "The model will overestimate the impact of 'analyst ratings' on stock price, because the effect of company revenue will now be partially confounded with analyst ratings, leading to biased coefficient estimates and a violation of the ceteris paribus condition."
      },
      "correct_answer": "The model will likely underestimate the stock prices of companies with exceptionally low debt-to-equity ratios and high revenue, as it loses the ability to capture the synergistic effect of these two factors on stock price.",
      "explanation": "This question synthesizes correlation matrices (Lec 4.5), scatterplot matrices (Lec 4.6), the coefficient of determination ($R^2$) (Lec 4.8), and ceteris paribus (Lec 4.7). Removing 'company revenue' to address multicollinearity is a plausible step, but it overlooks the critical information revealed by the scatterplot matrix: the *synergistic* effect of low debt and high revenue on stock price. Option A is incorrect; while $R^2$ might not change drastically, the *accuracy* for a specific subset of companies will be negatively affected. Option B is too optimistic; while coefficient estimates *might* be more reliable overall, the model will perform poorly for a specific group. Option D is plausible, but secondary. While the coefficient for 'analyst ratings' *might* be inflated, the primary issue is the lost ability to model the combined effect of low debt and high revenue. The correct answer (C) captures the core problem: the model will underestimate the stock prices of companies with this specific combination of factors. The scatterplot showed that these companies are outliers relative to a linear model that doesn't account for this synergy. The analyst made a critical error by assuming a linear model will capture the entire relationship.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_5",
      "tags": [
        "correlation matrix",
        "scatterplot matrix",
        "multicollinearity",
        "EDA",
        "R-squared",
        "Coefficient of Determination",
        "Ceteris Paribus"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing team is analyzing customer churn using both a Scatterplot Matrix and a Correlation Matrix. The Scatterplot Matrix reveals a non-linear, U-shaped relationship between 'customer age' and 'churn probability,' indicating higher churn among both young and old customers. The Correlation Matrix shows a strong negative correlation (-0.7) between 'customer satisfaction score' and 'churn probability,' and a moderate positive correlation (0.5) between 'customer age' and 'years as customer'. The team decides to build a churn prediction model, focusing on 'customer satisfaction score' and 'years as customer' due to their high correlations with churn. Synthesizing the concepts of Scatterplot Matrix, Correlation Matrix, and Ceteris Paribus, what is the MOST significant issue with this approach?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model will likely overestimate churn probability for all customers, as it only considers linear relationships and ignores the non-linear U-shaped relationship with 'customer age'.",
        "B": "The model will accurately predict churn for most customers, as the strong correlation between 'customer satisfaction score' and churn probability captures the primary driver of churn.",
        "C": "The model will likely underestimate churn probability for middle-aged customers, as it misses the higher churn rates within those age groups.",
        "D": "The model will fail to capture the increased churn rates for both young and old customers, as it only considers linear effects and disregards the U-shaped relationship revealed by the scatterplot matrix, violating the ceteris paribus condition."
      },
      "correct_answer": "The model will fail to capture the increased churn rates for both young and old customers, as it only considers linear effects and disregards the U-shaped relationship revealed by the scatterplot matrix, violating the ceteris paribus condition.",
      "explanation": "This question requires understanding the differences between Scatterplot Matrices and Correlation Matrices (Lec 4.6) and how they inform regression modeling, as well as the meaning of Ceteris Paribus (Lec 4.7). The critical flaw is ignoring the non-linear relationship between 'customer age' and 'churn probability'. Option A is incorrect because it assumes *overestimation* of churn for *all* customers, which isn't necessarily true; the model will likely be more accurate for customers near the average age. Option B is incorrect because while 'customer satisfaction score' is important, it doesn't account for the age effect. Option C is plausible but incomplete; it only focuses on *middle-aged* customers, not the entire U-shape. The correct answer (D) identifies the core problem: by focusing solely on linear relationships, the model will miss the increased churn rates for *both* young and old customers. The ceteris paribus condition is being violated in a subtle way. The coefficients for 'customer satisfaction score' and 'years as customer' are being interpreted as *constant* effects *regardless* of age, which is demonstrably false based on the Scatterplot Matrix. A more sophisticated model would need to account for the non-linearity, perhaps through polynomial terms or interaction effects.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_6",
      "tags": [
        "scatterplot matrix",
        "correlation matrix",
        "eda",
        "comparison",
        "MLR",
        "Ceteris Paribus"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is building a model to predict customer spending using multiple linear regression. They use both a Scatterplot Matrix and a Correlation Matrix in their EDA. The Scatterplot Matrix reveals a cluster of high-spending customers with relatively low incomes, while the rest of the customers show a weak or no relationship between income and spending. The Correlation Matrix shows a strong positive correlation (0.7) between 'website visits' and 'customer spending,' and a weak positive correlation (0.2) between 'income' and 'customer spending.' The data scientist decides to build a multiple linear regression model, including only 'website visits' and 'age' as predictors, justifying their decision by stating that 'income' does not appear to be a significant predictor based on the correlation matrix. Synthesizing the concepts of Scatterplot Matrix, Correlation Matrix, and the Coefficient of Determination, what is the MOST likely consequence of this modeling approach?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The $R^2$ of the model will be significantly lower than it could be, as the model fails to capture the spending patterns of the high-spending, low-income customer segment.",
        "B": "The model will accurately predict customer spending for most customers, as the inclusion of 'website visits' captures the primary driver of spending behavior.",
        "C": "The model will overestimate customer spending for low-income customers, as it does not account for the limitations imposed by income.",
        "D": "The model will fail to capture the spending patterns of all customers, as it relies solely on linear relationships and ignores the cluster of high-spending, low-income customers revealed by the scatterplot matrix."
      },
      "correct_answer": "The $R^2$ of the model will be significantly lower than it could be, as the model fails to capture the spending patterns of the high-spending, low-income customer segment.",
      "explanation": "This question requires synthesizing knowledge of Scatterplot Matrices (Lec 4.6), Correlation Matrices (Lec 4.5), and the Coefficient of Determination ($R^2$) (Lec 4.8). The critical error is dismissing 'income' as a predictor based solely on the correlation matrix, while ignoring the cluster of high-spending, low-income customers revealed by the scatterplot matrix. Option B is incorrect because even though 'website visits' may be a good predictor *overall*, it doesn't explain the behavior of the high-spending, low-income segment. Option C is plausible but not the *most* significant issue. While the model *might* overestimate spending for some low-income customers, the bigger problem is missing the high-spending segment. Option D is too broad; the model likely captures *some* spending patterns, just not the ones associated with this specific cluster. The correct answer (A) pinpoints the key issue: the model's overall fit ($R^2$) will be lower because it cannot explain the spending behavior of the high-spending, low-income group. These customers are outliers relative to the linear model the analyst is building. The analyst is making a mistake by relying too heavily on the *average* relationship revealed by the Correlation Matrix and ignoring the *specific* patterns revealed by the Scatterplot Matrix. These customers deviate significantly from the overall trend.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_6",
      "tags": [
        "scatterplot matrix",
        "correlation matrix",
        "eda",
        "comparison",
        "R-squared",
        "Coefficient of Determination"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A health researcher is investigating factors affecting patient recovery time after surgery. They create a Scatterplot Matrix and a Correlation Matrix using data on patient age, BMI, pre-surgery fitness level (VO2 max), and post-operative complications. The Scatterplot Matrix reveals a non-linear relationship between 'patient age' and 'recovery time,' with recovery time increasing sharply after age 65. The Correlation Matrix shows a strong negative correlation (-0.65) between 'pre-surgery fitness level' and 'recovery time,' and a weak positive correlation (0.2) between 'BMI' and 'recovery time.' The researcher decides to build a multiple linear regression model using only 'pre-surgery fitness level' and 'BMI' as predictors, citing the desire for a simple, interpretable model. Synthesizing the concepts of Scatterplot Matrix, Correlation Matrix, and Ceteris Paribus, what is the MOST likely limitation of this approach?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The model will likely underestimate recovery time for all patients, as it only considers linear relationships and ignores the non-linear effect of 'patient age'.",
        "B": "The model will accurately predict recovery time for most patients, as the strong correlation between 'pre-surgery fitness level' and recovery time captures the primary determinant of recovery.",
        "C": "The model will likely overestimate recovery time for younger patients, as it doesn't account for their generally faster recovery rates.",
        "D": "The model will fail to accurately predict recovery time for patients over 65, as it disregards the sharp increase in recovery time for this age group, thus violating the ceteris paribus condition by assuming a constant effect of fitness and BMI regardless of age."
      },
      "correct_answer": "The model will fail to accurately predict recovery time for patients over 65, as it disregards the sharp increase in recovery time for this age group, thus violating the ceteris paribus condition by assuming a constant effect of fitness and BMI regardless of age.",
      "explanation": "This question tests the ability to integrate information from Scatterplot Matrices (Lec 4.6) and Correlation Matrices (Lec 4.5) in the context of multiple linear regression (and implicitly, variable selection). The *critical* flaw is the researcher's decision to ignore the non-linear relationship between 'patient age' and 'recovery time' revealed by the Scatterplot Matrix. Option A is too broad; the model won't necessarily *underestimate* recovery time for *all* patients. Option B is incorrect; 'pre-surgery fitness level' is important, but it doesn't account for the age effect. Option C is plausible, but less significant than the effect on older patients; the *sharp* increase in recovery time after 65 is the dominant feature. The correct answer (D) highlights the key limitation: the model will be inaccurate for patients over 65 because it completely disregards the age effect. By assuming that 'fitness level' and 'BMI' have a *constant* effect on recovery time *regardless* of age, the researcher is violating the ceteris paribus condition (Lec 4.7). A more sophisticated model would need to account for this non-linearity, perhaps through interaction terms or a piecewise regression.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_6",
      "tags": [
        "scatterplot matrix",
        "correlation matrix",
        "eda",
        "comparison",
        "MLR",
        "Ceteris Paribus"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An e-commerce company is using multiple linear regression to predict customer lifetime value (CLTV). They include 'average order value,' 'customer age,' 'number of purchases,' and 'time since last purchase' in their model. The regression output shows a coefficient of 50 for 'average order value.' A junior analyst interprets this as: \"For every dollar increase in average order value, CLTV increases by $50, regardless of other factors.\" Synthesizing the concepts of ceteris paribus and R-squared, which of the following statements BEST describes the validity and limitations of this interpretation?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The interpretation is valid, as the coefficient in a multiple linear regression model directly represents the marginal effect of the predictor variable on the response variable, holding all other variables constant, and R-squared guarantees its accuracy.",
        "B": "The interpretation is valid only if the R-squared of the model is high, indicating that the model explains a significant portion of the variance in CLTV, thus ensuring the reliability of the coefficient estimate.",
        "C": "The interpretation is potentially misleading, as it assumes that other variables in the model are held constant (ceteris paribus), and this assumption might not hold in reality, particularly if there are strong correlations between the predictor variables.",
        "D": "The interpretation is entirely invalid, because the coefficient represents the effect of average order value on CLTV but it may be the result of a spurious correlation, and the R-squared value is irrelevant."
      },
      "correct_answer": "The interpretation is potentially misleading, as it assumes that other variables in the model are held constant (ceteris paribus), and this assumption might not hold in reality, particularly if there are strong correlations between the predictor variables.",
      "explanation": "This question requires a good understanding of ceteris paribus (Lec 4.7) and R-squared (Lec 4.8), and how they relate to interpreting coefficients in multiple linear regression. Option A is incorrect because a high R-squared doesn't guarantee the *validity* of the ceteris paribus assumption; it only indicates the overall fit of the model. Option B is partially correct in that a low R-squared *weakens* the interpretation, but the ceteris paribus assumption is still the core issue. Option D is too strong; the interpretation isn't *entirely* invalid, but it's incomplete and potentially misleading. The correct answer (C) highlights the crucial limitation: the interpretation assumes ceteris paribus, which may not be true, especially if there is multicollinearity. For example, if 'average order value' is strongly correlated with 'number of purchases,' then increasing 'average order value' might *also* influence 'number of purchases,' violating the assumption that all other variables are held constant. Therefore, the $50 increase in CLTV might be due to a combination of the increase in 'average order value' AND the associated increase in 'number of purchases.'",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_7",
      "tags": [
        "MLR",
        "Ceteris Paribus",
        "Partial Slope",
        "R-squared",
        "Coefficient of Determination"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A real estate analyst develops a multiple linear regression model to predict apartment rental prices. The model includes variables such as 'square footage,' 'number of bedrooms,' 'distance to public transportation,' and 'neighborhood safety score.' The model has an R-squared of 0.65. The analyst states, \"Our model explains 65% of the variation in apartment rental prices. Therefore, we can confidently use this model to determine the optimal pricing strategy for new apartment units.\" Synthesizing the concepts of R-squared and Ceteris Paribus, what is the MOST significant limitation of the analyst's conclusion?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The analyst's conclusion is valid, as an R-squared of 0.65 indicates a reasonably good fit, allowing for reliable predictions of rental prices and informed pricing decisions.",
        "B": "The analyst's conclusion is invalid because the R-squared value is not high enough to make reliable pricing decisions.",
        "C": "The analyst's conclusion is potentially flawed because while the model explains 65% of the variation, it doesn't guarantee that the relationships are causal or that the model captures all relevant factors influencing rental prices, and the ceteris paribus condition might not hold.",
        "D": "The analyst's conclusion is potentially flawed because the model does not include any interaction variables."
      },
      "correct_answer": "The analyst's conclusion is potentially flawed because while the model explains 65% of the variation, it doesn't guarantee that the relationships are causal or that the model captures all relevant factors influencing rental prices, and the ceteris paribus condition might not hold.",
      "explanation": "This question tests the understanding of R-squared (Lec 4.8) and ceteris paribus (Lec 4.7). The key issue is that a high R-squared doesn't guarantee a *good* model for all purposes, especially *causal* inference. Option A is incorrect because an R-squared of 0.65, while reasonable, doesn't guarantee reliable pricing decisions, especially without considering other factors. Option B is incorrect because there isn't a hard threshold for a 'high enough' R-squared; it depends on the context. Option D is a plausible concern, but not the *most* significant limitation. The correct answer (C) highlights the core limitations: R-squared only measures the proportion of variance explained, not causality. Furthermore, the model might be missing important variables (e.g., proximity to amenities, school district ratings). The *ceteris paribus* condition is also important: the model assumes that the effect of each variable (e.g., square footage) on rental price is constant, regardless of the values of other variables. However, this assumption might not be true. For example, the effect of 'square footage' on rental price might be different in a luxury apartment building than in a low-income housing complex. The conclusion assumes that the coefficients estimated from the data accurately represent the *true* relationship and will remain constant in a *new* situation (pricing new units), which is a leap of faith. The model is missing *context* and *causality*.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_7",
      "tags": [
        "MLR",
        "Ceteris Paribus",
        "Partial Slope",
        "R-squared",
        "Coefficient of Determination"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An analyst is building a multiple regression model to predict customer churn at a telecom company. They have a dataset with 20 potential predictor variables, including demographics, usage patterns, and customer service interactions. The analyst initially builds a model with all 20 predictors and obtains an R-squared of 0.90. However, they are concerned about overfitting and want to select a more parsimonious model. They are considering two approaches:\n\nApproach A: Select the model with the highest Adjusted R-squared.\nApproach B: Use the Global F-Test to determine if the overall model is significant, and only then consider individual t-tests to identify significant predictors and build a reduced model.\n\nSynthesizing the concepts of Adjusted R-squared and the Global F-Test, which approach is most appropriate in this scenario, and why?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Approach A is most appropriate because Adjusted R-squared directly penalizes the inclusion of irrelevant predictors, leading to a more generalizable model, regardless of the Global F-Test result.",
        "B": "Approach B is most appropriate because the Global F-Test ensures that the overall model has significant explanatory power before considering individual predictors. If the Global F-Test is not significant, no further analysis should be performed, thus preventing overfitting.",
        "C": "Approach A is superior because a high Adjusted R-squared guarantees that the model will have good predictive power on new data, while the Global F-Test only tells us about the significance of the overall model on the training data.",
        "D": "Approach B is superior because it combines the strengths of both approaches: the Global F-Test confirms overall model significance, and then individual t-tests, alongside careful consideration of Adjusted R-squared, guide the selection of a reduced, more parsimonious model that avoids overfitting. This balanced approach is crucial when dealing with a large number of potential predictors."
      },
      "correct_answer": "Approach D is superior because it combines the strengths of both approaches: the Global F-Test confirms overall model significance, and then individual t-tests, alongside careful consideration of Adjusted R-squared, guide the selection of a reduced, more parsimonious model that avoids overfitting. This balanced approach is crucial when dealing with a large number of potential predictors.",
      "explanation": "This is a synthesis question combining the concepts of Adjusted R-squared (Lecture 4) and the Global F-Test (Lecture 4). Option A focuses solely on Adjusted R-squared, neglecting the importance of first confirming overall model significance via the Global F-Test. Option B correctly emphasizes the Global F-Test but doesn't fully leverage Adjusted R-squared for model reduction. Option C incorrectly claims that a high Adjusted R-squared guarantees predictive power, ignoring the potential for overfitting despite the adjustment. The correct answer, D, highlights the need for a two-step process: first, ensure the overall model is significant using the Global F-Test; second, use t-tests and Adjusted R-squared to select a parsimonious model. This prevents interpreting individual coefficients when the overall model lacks explanatory power and helps to avoid overfitting.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_9",
      "tags": [
        "R-squared",
        "Model Comparison",
        "Overfitting",
        "Adjusted R-squared",
        "Global F-Test",
        "Hypothesis Testing"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data scientist is tasked with building a model to predict sales for a retail company. They start with a simple linear regression model using only advertising spend as a predictor, resulting in an R-squared of 0.60. They then add several other predictors, including promotional offers, seasonal indicators, and competitor pricing data, which increases the R-squared to 0.85. The Adjusted R-squared also increases, but only slightly, from 0.58 to 0.62. \n\nThe company's marketing director is excited about the higher R-squared and wants to use the more complex model. However, the data scientist is hesitant. Considering the concepts of R-squared, Adjusted R-squared, and the potential for overfitting, what is the most likely problem with the more complex model, and what is the best course of action?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "The most likely problem is multicollinearity among the predictors, which inflates the R-squared. The best course of action is to remove the predictors with the highest variance inflation factors (VIFs) until the Adjusted R-squared significantly increases.",
        "B": "The most likely problem is overfitting, where the model is capturing noise in the data rather than true relationships. The best course of action is to use the more complex model for in-sample prediction but revert to the simpler model for out-of-sample prediction to avoid poor generalization.",
        "C": "The most likely problem is that some of the added predictors are not truly related to sales, leading to a small increase in Adjusted R-squared despite a larger increase in R-squared. The best course of action is to perform t-tests on the individual coefficients and remove any predictors with p-values above a predetermined significance level (e.g., 0.05).",
        "D": "The most likely problem is heteroscedasticity, where the variance of the error term is not constant across all levels of the predictors. The best course of action is to transform the response variable (e.g., using a logarithmic transformation) and rebuild the model to address the heteroscedasticity, regardless of the R-squared or Adjusted R-squared values."
      },
      "correct_answer": "The most likely problem is that some of the added predictors are not truly related to sales, leading to a small increase in Adjusted R-squared despite a larger increase in R-squared. The best course of action is to perform t-tests on the individual coefficients and remove any predictors with p-values above a predetermined significance level (e.g., 0.05).",
      "explanation": "This question synthesizes R-squared and Adjusted R-squared (Lecture 4) with the concept of individual t-tests (Lecture 4) in the context of model building. While multicollinearity (Option A) and heteroscedasticity (Option D) could be present, they are not the *most likely* problem given the described scenario. Option B suggests reverting to the simpler model for out-of-sample prediction, but this doesn't address the underlying issue of why the more complex model is not performing well. The small increase in Adjusted R-squared, despite a larger increase in R-squared, suggests that some of the added predictors are not contributing meaningful information to the model. Performing t-tests (Option C) allows the data scientist to identify and remove these non-significant predictors, leading to a more parsimonious and generalizable model. The key is to use t-tests to identify the root cause of the R-squared inflation.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_9",
      "tags": [
        "R-squared",
        "Adjusted R-squared",
        "Overfitting",
        "t-test",
        "multicollinearity",
        "heteroscedasticity"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An economist is building a model to predict GDP growth using several macroeconomic indicators, including inflation, unemployment, and interest rates. After building the initial model, the economist finds that the Global F-Test is significant (p < 0.05), indicating that the model as a whole has explanatory power. However, upon examining the individual t-tests for each predictor, they discover that the t-test for 'inflation' has a high p-value (e.g., 0.6), suggesting that inflation is not a significant predictor *given* the other variables in the model.\n\nThe economist is considering two options:\n\nOption A: Remove 'inflation' from the model since its t-test is not significant.\nOption B: Keep 'inflation' in the model because it is a theoretically important variable and its inclusion does not substantially reduce the Adjusted R-squared.\n\nSynthesizing the concepts of the Global F-Test, individual t-tests, and the importance of theoretical considerations, which option is the most appropriate, and why?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Option A is most appropriate because a non-significant t-test definitively indicates that the predictor has no meaningful effect on the response variable, and including it only adds noise to the model.",
        "B": "Option B is most appropriate because theoretical importance should always outweigh statistical significance. Removing theoretically important variables can lead to biased estimates for the remaining predictors.",
        "C": "Option A is most appropriate because, since the Global F-Test is significant, removing a non-significant variable will increase the Adjusted R-squared, leading to a better model.",
        "D": "Option B is most appropriate because the non-significance of the t-test only means that 'inflation' doesn't have a significant effect *given the other predictors in the model*. It might still be important for theoretical reasons or as a control variable, and its inclusion doesn't substantially reduce the Adjusted R-squared. Moreover, economic models often rely on established theoretical relationships, even if statistical significance is marginal."
      },
      "correct_answer": "Option B is most appropriate because the non-significance of the t-test only means that 'inflation' doesn't have a significant effect *given the other predictors in the model*. It might still be important for theoretical reasons or as a control variable, and its inclusion doesn't substantially reduce the Adjusted R-squared. Moreover, economic models often rely on established theoretical relationships, even if statistical significance is marginal.",
      "explanation": "This question synthesizes the Global F-Test (Lecture 4), individual t-tests (Lecture 4), and the interplay between statistical significance and theoretical importance. Option A incorrectly assumes that a non-significant t-test means the predictor has *no* effect. Option C incorrectly assumes that removing a non-significant variable will *always* increase Adjusted R-squared (this is not guaranteed). Option B recognizes that a non-significant t-test result only means the predictor doesn't have a significant effect *given the other predictors in the model*. It highlights the importance of theoretical considerations and the role of variables as controls, even if they are not statistically significant. The edge case here is that the p-value is close to the significance level, suggesting a weak effect; removing 'inflation' *might* slightly improve Adjusted R-squared, but could also bias the model by omitting a theoretically relevant factor. The correct choice is to retain the variable.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_11",
      "tags": [
        "Global F-Test",
        "t-test",
        "Hypothesis Testing",
        "statistical significance",
        "Adjusted R-squared"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst is building a multiple regression model to predict website conversion rates based on various factors such as advertising spend (across different platforms), website design elements (e.g., number of images, button colors), and user demographics. The Global F-Test for the model is highly significant (p < 0.001). However, the analyst notices that the t-test for \"number of website images\" has a very high p-value (e.g., 0.9), indicating it's not a significant predictor *given* the other variables. Furthermore, the analyst suspects that advertising spend and user demographics may be collinear.\n\nGiven these observations, what is the most appropriate next step for the analyst to take?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Remove the \"number of website images\" variable from the model since its t-test is not significant and focus on interpreting the coefficients of the remaining variables.",
        "B": "Conduct a variance inflation factor (VIF) analysis to assess the degree of multicollinearity between advertising spend and user demographics. If VIFs are high (e.g., > 5), remove one of the collinear variables and rerun the regression.",
        "C": "Transform the \"number of website images\" variable (e.g., using a logarithmic transformation) and rerun the regression. A non-linear relationship might exist that is not captured by the linear term.",
        "D": "First, conduct a VIF analysis to address potential multicollinearity. Then, after addressing multicollinearity, reassess the significance of the t-test for \"number of website images\". It is possible that after removing collinear variables, the significance of \"number of website images\" will change."
      },
      "correct_answer": "First, conduct a VIF analysis to address potential multicollinearity. Then, after addressing multicollinearity, reassess the significance of the t-test for \"number of website images\". It is possible that after removing collinear variables, the significance of \"number of website images\" will change.",
      "explanation": "This question synthesizes the concepts of Global F-Test (Lecture 4), individual t-tests (Lecture 4), and multicollinearity. Option A is premature, as the non-significance of the t-test for \"number of website images\" might be due to multicollinearity masking its effect. Option C is also not the most appropriate *first* step. While transformations can sometimes help, addressing multicollinearity should be prioritized. Option D correctly identifies the *causal relationship*: multicollinearity can influence the significance of other variables' t-tests. By first addressing multicollinearity, the analyst can obtain a more accurate assessment of the true effect of \"number of website images\". The analyst must address multicollinearity first (VIF analysis) and then reassess the significance of the number of images. Removing a collinear variable may make \"number of website images\" significant, therefore, the analyst shouldn't remove \"number of website images\" immediately.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_11",
      "tags": [
        "Global F-Test",
        "t-test",
        "multicollinearity",
        "VIF",
        "Hypothesis Testing"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A data analyst is building a multiple regression model to predict customer satisfaction scores based on several factors: product quality, customer service responsiveness, price, and brand reputation. The analyst runs the model and obtains the following results:\n\n*   Global F-Test: p < 0.001 (significant)\n*   t-test for product quality: p < 0.05 (significant)\n*   t-test for customer service responsiveness: p < 0.05 (significant)\n*   t-test for price: p > 0.05 (not significant)\n*   t-test for brand reputation: p > 0.05 (not significant)\n\nUpon further investigation, the analyst discovers that price and brand reputation are highly correlated (correlation coefficient = 0.85). Furthermore, the analyst knows from prior research that both price and brand reputation are theoretically important drivers of customer satisfaction.\n\nGiven this scenario, which of the following actions would be most appropriate?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Remove both 'price' and 'brand reputation' from the model since their individual t-tests are not significant, and multicollinearity is present. This will simplify the model and improve its interpretability.",
        "B": "Remove either 'price' or 'brand reputation' from the model based on theoretical considerations or which variable has a slightly lower p-value. Then, rerun the regression model to see if the remaining variable becomes significant.",
        "C": "Keep both 'price' and 'brand reputation' in the model, despite their non-significance and multicollinearity. Acknowledge the limitations in the model's interpretation due to multicollinearity, but argue that their theoretical importance justifies their inclusion.",
        "D": "Create an interaction term between 'price' and 'brand reputation' and include this interaction term in the model instead of the individual variables. This may capture the combined effect of price and brand on customer satisfaction, and potentially alleviate the multicollinearity issue."
      },
      "correct_answer": "Create an interaction term between 'price' and 'brand reputation' and include this interaction term in the model instead of the individual variables. This may capture the combined effect of price and brand on customer satisfaction, and potentially alleviate the multicollinearity issue.",
      "explanation": "This question requires synthesizing the concepts of t-tests (Lecture 4), multicollinearity, and model building strategies. Option A is incorrect because removing both variables discards potentially important information. Option B is better, but doesn't address the *reason* for the non-significance (multicollinearity). Option C acknowledges the issue but does not attempt to resolve it, which is suboptimal. The core issue is the multicollinearity *between* price and brand reputation. Multicollinearity can obscure the individual effects of these variables. The correct approach (Option D) creates an *interaction term* which is intended to capture the combined effect of the two variables. This addresses the multicollinearity issue by capturing the combined effect of price and brand, rather than trying to separate their individual effects. The interaction term could become significant, even if the individual variables are not, due to the combined effect. This approach acknowledges and addresses the theoretical importance of both variables while accounting for their statistical relationship.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_12",
      "tags": [
        "t-test",
        "multiple regression",
        "statistical significance",
        "p-value",
        "multicollinearity",
        "interaction term"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A nationwide real estate firm, 'Global Homes,' uses multiple regression to predict property values based on square footage, number of bedrooms, and location's school district rating. They built a model based on data from 2022 and 2023. A new luxury condo development is being planned in a previously unrated school district. The developers are highly concerned about accurately predicting the sales price of individual units in this new development to secure financing. The regression model, however, shows heteroscedasticity. Considering the potential impact of heteroscedasticity on prediction intervals and the firm's need to accurately predict individual condo prices, which approach is most appropriate to address this situation and provide realistic estimates for potential investors?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Use the existing regression model, but only report the confidence interval for the *average* sales price of all condos in the new development, ignoring the heteroscedasticity since the CI is less affected than the PI.",
        "B": "Apply a variance-stabilizing transformation (e.g., Box-Cox) to the response variable (sales price) to reduce heteroscedasticity, then calculate a prediction interval for individual condo prices using the transformed model. Back-transform the interval to the original scale for interpretation.",
        "C": "Use the existing regression model and calculate a standard prediction interval for individual condo prices, but increase the confidence level (e.g., from 95% to 99%) to account for the increased uncertainty due to heteroscedasticity.",
        "D": "Segment the data into groups based on school district rating (even though the new district is unrated, assign it to the closest rated district), build separate regression models for each group, and then calculate a prediction interval for the new condo development using the model from the most similar rated district. Ignore the heteroscedasticity within that selected district's model."
      },
      "correct_answer": "Apply a variance-stabilizing transformation (e.g., Box-Cox) to the response variable (sales price) to reduce heteroscedasticity, then calculate a prediction interval for individual condo prices using the transformed model. Back-transform the interval to the original scale for interpretation.",
      "explanation": "This question synthesizes the concepts of prediction intervals (Lecture 4) and heteroscedasticity (and its remedies) from a later lecture on regression diagnostics. The problem is multi-faceted: predicting individual values (PI), the presence of heteroscedasticity violates regression assumptions, and the new development in an unrated district adds uncertainty. Option A only focuses on the CI, which is not the core concern. Option C attempts to compensate for heteroscedasticity by arbitrarily increasing the confidence level, which is statistically unsound. Option D introduces bias by forcing the new district into an existing category and ignores heteroscedasticity. The correct answer addresses the root cause (heteroscedasticity) with a proper transformation, allowing for a valid PI calculation. The back-transformation ensures results are interpretable in the original units. This approach balances statistical rigor with practical interpretability.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting",
        "heteroscedasticity",
        "variance stabilizing transformation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A pharmaceutical company is developing a new drug to lower blood pressure. They conduct a clinical trial with multiple predictor variables (age, weight, dosage, and pre-existing conditions) and blood pressure reduction as the response variable. The company needs to provide a reliable estimate of the blood pressure reduction for a *specific* patient profile to inform doctors about individual treatment expectations. However, they also need to estimate the *average* blood pressure reduction across all patients with that same profile for marketing materials. The regression model exhibits multicollinearity between 'age' and 'pre-existing conditions'. Given the need for both accurate prediction intervals *and* confidence intervals, what is the MOST appropriate strategy for dealing with the multicollinearity in this scenario?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Remove the 'pre-existing conditions' variable from the model because it is likely causing the multicollinearity. This will simplify the model and provide more stable coefficient estimates for both the confidence and prediction intervals.",
        "B": "Calculate both a confidence interval for the *average* blood pressure reduction and a prediction interval for an *individual* patient using the model with multicollinearity present. Acknowledge the potential inflation of standard errors but proceed without modification.",
        "C": "Use Ridge Regression (L2 regularization) to mitigate the effects of multicollinearity on the coefficient estimates, then calculate both the confidence interval for the average blood pressure reduction and the prediction interval for an individual patient using the regularized model.",
        "D": "Transform the 'age' and 'pre-existing conditions' variables into a single interaction term to eliminate the multicollinearity, then calculate both the confidence interval for the average blood pressure reduction and the prediction interval for an individual patient using the transformed model."
      },
      "correct_answer": "Use Ridge Regression (L2 regularization) to mitigate the effects of multicollinearity on the coefficient estimates, then calculate both the confidence interval for the average blood pressure reduction and the prediction interval for an individual patient using the regularized model.",
      "explanation": "This question synthesizes the concepts of confidence intervals and prediction intervals (Lecture 4) with multicollinearity and regularization techniques (from a later lecture). The key is recognizing that both *average* (CI) and *individual* (PI) predictions are important, and multicollinearity affects both. Option A, removing a variable, might introduce bias and lose valuable information. Option B acknowledges the problem but doesn't address it, leading to unreliable intervals. Option D, creating an interaction term, might not fully resolve the multicollinearity and could create interpretability issues. Ridge Regression (Option C) is the most appropriate because it directly addresses multicollinearity by shrinking the coefficients, leading to more stable and reliable estimates for *both* the CI and PI, thus balancing the need for average and individual predictions.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting",
        "multicollinearity",
        "ridge regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An e-commerce company, 'ShopSmart,' uses multiple regression to predict daily sales based on advertising spend (TV, online), website traffic, and seasonality (using dummy variables for each month). They need to forecast sales for the next month to manage inventory. They primarily rely on prediction intervals for individual days to account for daily fluctuations. During a model validation exercise, they discover that the residuals exhibit autocorrelation. Furthermore, the marketing team plans a significant increase in online advertising spend next month, outside the range of historical data. Considering the impact of autocorrelation on prediction intervals and the planned advertising spend increase, which course of action is most appropriate for ShopSmart to ensure accurate sales forecasting?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Ignore the autocorrelation since it primarily affects the standard errors of the coefficients, not the point predictions themselves. Calculate prediction intervals using the existing model, and proceed with inventory management as usual. The increased advertising spend will be accounted for in the model.",
        "B": "Address the autocorrelation by incorporating lagged sales as a predictor variable in the regression model. Then, calculate prediction intervals for the next month, being cautious about extrapolating too far beyond the historical range of advertising spend. Potentially cap the advertising spend at a reasonable level in the forecast.",
        "C": "Switch from using prediction intervals to using confidence intervals for the *average* daily sales next month. Confidence intervals are less sensitive to autocorrelation than prediction intervals. The average will smooth out the daily fluctuations.",
        "D": "Completely rebuild the regression model using only the most recent month's data, assuming that the autocorrelation is due to a recent shift in customer behavior. This will provide the most up-to-date estimates, but historical patterns will be ignored."
      },
      "correct_answer": "Address the autocorrelation by incorporating lagged sales as a predictor variable in the regression model. Then, calculate prediction intervals for the next month, being cautious about extrapolating too far beyond the historical range of advertising spend. Potentially cap the advertising spend at a reasonable level in the forecast.",
      "explanation": "This question integrates prediction intervals (Lecture 4) with autocorrelation (and its remedies) and extrapolation concerns (from later lectures). The problem is complex: daily forecasting needs PIs, autocorrelation violates regression assumptions, and increased advertising spend introduces extrapolation risk. Option A incorrectly dismisses the impact of autocorrelation on prediction intervals. Option C switches to CIs, which are not suitable for daily inventory management. Option D overreacts by discarding historical data, potentially losing valuable seasonal information. The correct answer addresses the autocorrelation by including lagged sales, improving the validity of the PIs. It also acknowledges and mitigates the extrapolation risk by suggesting caution and potentially capping the advertising spend. This approach balances statistical accuracy with practical forecasting considerations.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting",
        "autocorrelation",
        "lagged variables",
        "extrapolation"
      ]
    },
    {
      "type": "mcq",
      "question_text": "A credit risk department at a bank uses multiple regression to predict the probability of loan default based on factors like credit score, income, debt-to-income ratio, and loan amount. They primarily rely on prediction intervals to assess the risk of *individual* loan applicants. The model's residuals are approximately normally distributed, but the bank's regulator requires them to also report confidence intervals for the *average* default probability across different risk segments. The credit risk manager notices that adding more predictor variables (e.g., employment history, homeownership status) slightly improves the model's R-squared but substantially widens the prediction intervals. Considering the regulator's requirement for confidence intervals and the importance of narrow prediction intervals for individual risk assessment, which approach BEST balances model fit with predictive accuracy for individual loan applicants?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Continue adding predictor variables until the R-squared reaches a satisfactory level, even if the prediction intervals widen. The higher R-squared indicates a better overall model fit, which is more important for regulatory compliance.",
        "B": "Remove the variables that contribute the least to the R-squared, even if they are statistically significant. This will narrow the prediction intervals at the expense of a slightly lower R-squared.",
        "C": "Use a model selection technique like stepwise regression or best subsets regression, using a metric like AIC or BIC that penalizes model complexity. This will help find a balance between model fit and the number of predictors, thus optimizing both confidence and prediction intervals. Consider using adjusted R-squared as well.",
        "D": "Ignore the widening of the prediction intervals and focus solely on minimizing the standard error of the regression. This will ensure the most accurate estimates for the average default probability, which is the regulator's primary concern."
      },
      "correct_answer": "Use a model selection technique like stepwise regression or best subsets regression, using a metric like AIC or BIC that penalizes model complexity. This will help find a balance between model fit and the number of predictors, thus optimizing both confidence and prediction intervals. Consider using adjusted R-squared as well.",
      "explanation": "This question synthesizes the concepts of confidence intervals and prediction intervals (Lecture 4) with model selection and complexity penalties (from later lectures). The problem is about balancing competing needs: regulatory reporting (CI) and individual risk assessment (PI), while also managing model complexity. Option A prioritizes R-squared over prediction accuracy, which is inappropriate for individual risk assessment. Option B arbitrarily removes variables, potentially sacrificing predictive power. Option D focuses solely on the average, neglecting individual predictions. The correct answer uses model selection techniques (AIC/BIC) to find the optimal trade-off between model fit and complexity, leading to narrower prediction intervals without sacrificing overall model performance. AIC and BIC are specifically designed to prevent overfitting, which directly addresses the widening of the PIs when more variables are added. Considering adjusted R-squared alongside AIC/BIC provides an additional layer of validation.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting",
        "model selection",
        "AIC",
        "BIC",
        "stepwise regression",
        "best subsets regression"
      ]
    },
    {
      "type": "mcq",
      "question_text": "An agricultural research institute is developing a multiple regression model to predict crop yield (tons/acre) based on rainfall (inches), fertilizer application (lbs/acre), and soil pH. They need to provide farmers with both a confidence interval for the *average* yield across all farms with similar conditions and a prediction interval for the yield of a *specific* farm. After building the initial model, they observe that the variance of the residuals increases with increasing levels of fertilizer application. Additionally, a farming consultant suggests that the relationship between fertilizer and yield is likely non-linear, exhibiting diminishing returns at higher fertilizer levels. Considering the heteroscedasticity, the need for both confidence and prediction intervals, and the potential non-linearity, which approach is MOST appropriate for the institute to improve their model and provide reliable yield forecasts?",
      "visual_type": "None",
      "visual_code": "",
      "alt_text": "",
      "options": {
        "A": "Ignore the heteroscedasticity and non-linearity. Calculate both confidence and prediction intervals using the existing model. The sample size is large, so the Central Limit Theorem will ensure that the intervals are approximately correct.",
        "B": "Apply a variance-stabilizing transformation (e.g., Box-Cox) to the response variable (crop yield) to address the heteroscedasticity. Add a quadratic term for fertilizer application to capture the non-linear relationship. Then, calculate both confidence and prediction intervals using the transformed model.",
        "C": "Calculate separate regression models for different levels of fertilizer application (e.g., low, medium, high). This will address both the heteroscedasticity and the non-linearity. Then, calculate confidence and prediction intervals using the appropriate model for the specific farm.",
        "D": "Remove the fertilizer application variable from the model altogether. This will eliminate both the heteroscedasticity and the non-linearity, resulting in a simpler model with more reliable confidence and prediction intervals, albeit with lower predictive power."
      },
      "correct_answer": "Apply a variance-stabilizing transformation (e.g., Box-Cox) to the response variable (crop yield) to address the heteroscedasticity. Add a quadratic term for fertilizer application to capture the non-linear relationship. Then, calculate both confidence and prediction intervals using the transformed model.",
      "explanation": "This question synthesizes the concepts of confidence and prediction intervals (Lecture 4) with heteroscedasticity, non-linearity, and transformations (from later lectures). The problem is complex, involving heteroscedasticity, a non-linear relationship, and the need for both CI and PI. Option A ignores both problems, leading to unreliable intervals. Option C addresses the problems by creating separate models, which might lead to overfitting and data fragmentation. Option D oversimplifies by removing a key predictor. The correct answer addresses both the heteroscedasticity and non-linearity directly. The variance-stabilizing transformation corrects for unequal variances, ensuring valid interval calculations. The quadratic term captures the non-linear relationship, improving the model's fit and predictive power. This approach balances statistical rigor with practical modeling considerations, leading to more accurate and reliable yield forecasts for both average and individual farms.",
      "difficulty_level": 4,
      "source_flashcard_id": "DAA_lec_4_13",
      "tags": [
        "confidence interval",
        "prediction interval",
        "multiple regression",
        "forecasting",
        "heteroscedasticity",
        "non-linearity",
        "variance stabilizing transformation",
        "quadratic term"
      ]
    }
  ]
}