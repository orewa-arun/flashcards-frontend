{
  "questions": [
    {
      "type": "mcq",
      "question_text": "In a business analytics project, what is the primary objective of building a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "To perfectly fit past data with as many variables as possible",
        "B": "To explain variation in a dependent variable and make useful predictions for decisions",
        "C": "To replace all managerial judgment with automated algorithms",
        "D": "To summarize data into a single average without using predictors"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because regression modeling aims to explain how Y changes with X’s and to generate reliable predictions that support business decisions. A is incorrect because chasing perfect fit with many variables can overfit and hurt prediction quality rather than help it.",
        "step_by_step": [],
        "interpretation": "Regression is a tool for understanding and predicting business outcomes, not just for fitting historical data.",
        "business_context": "For example, a retailer uses regression to predict sales from price and advertising to decide budgets."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_1",
      "tags": [
        "regression model",
        "business decision-making",
        "prediction",
        "dependent variable"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the context of building a regression model, what is the role of the independent variables (X’s)?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "They are the outcomes the model tries to predict",
        "B": "They are random errors that the model cannot explain",
        "C": "They are the inputs used to explain and predict the dependent variable",
        "D": "They are constants that never change across observations"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because independent variables are the predictors used to explain variation in Y and generate predictions. A is wrong because the outcome to be predicted is the dependent variable, not the X’s.",
        "step_by_step": [],
        "interpretation": "Think of X’s as levers (like price or advertising) that help explain and predict a business outcome Y.",
        "business_context": "A bank might use income and credit score (X’s) to predict default risk (Y)."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_1",
      "tags": [
        "independent variables",
        "dependent variable",
        "regression model"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes the process of \"building\" a regression model in business analytics?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It is a one-time calculation where you run software once and accept the output as final.",
        "B": "It is an iterative process of selecting variables, specifying an equation, and refining the model.",
        "C": "It is the process of randomly choosing predictors until R-squared is highest.",
        "D": "It is only about drawing scatter plots without estimating any equation."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because model building is a systematic, iterative process of choosing predictors, specifying the equation, and refining it. A is incorrect because treating model building as a one-shot run ignores the need for checking assumptions and business sense.",
        "step_by_step": [],
        "interpretation": "Building a model is more like tuning a recipe than pressing a single button.",
        "business_context": "Analysts often re-specify models as new data arrive or business conditions change."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_1",
      "tags": [
        "regression model",
        "model building process",
        "business analytics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A retail chain uses regression to relate weekly sales (Y) to advertising spend and store size (X’s). What kind of business question is this model primarily designed to answer?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "How to randomly assign customers to stores",
        "B": "How weekly sales change when advertising and store size change",
        "C": "How to measure the exact error term for each customer",
        "D": "How to compute the median of store sizes without using sales data"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because the model links sales (Y) to predictors to understand and predict how sales respond to changes in advertising and store size. C is incorrect because the model does not aim to measure individual error terms as a business goal.",
        "step_by_step": [],
        "interpretation": "The focus is on quantifying relationships between inputs (X’s) and an outcome (Y).",
        "business_context": "Managers can use such a model to plan advertising budgets and store investments."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_1",
      "tags": [
        "regression model",
        "prediction",
        "business decision-making"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common mistake when building regression models for business decisions?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using an iterative process that combines theory, data, and judgment",
        "B": "Checking whether the model makes business sense before using it",
        "C": "Focusing only on getting a high R-squared and ignoring assumptions and interpretability",
        "D": "Considering whether the goal is explanation or prediction when choosing model complexity"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because chasing high R-squared without checking assumptions or business logic is a well-known pitfall. A and B describe good practice, not mistakes, and D is important because model purpose should influence complexity.",
        "step_by_step": [],
        "interpretation": "A model that fits the data well numerically can still be poor for decision-making if it is not sensible or valid.",
        "business_context": "Overly complex models may look good on paper but fail when used to forecast future sales or risk."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_1",
      "tags": [
        "common mistakes",
        "regression model",
        "R-squared",
        "business decision-making"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is the standard mathematical form of the general linear regression model with p predictors?",
      "question_visual": {
        "type": "latex",
        "plot_type": "formula",
        "params": {
          "expression": "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon"
        }
      },
      "question_visual_type": "latex",
      "options": {
        "A": "Y = β₀ + β₁X₁ + β₂X₂ + … + βₚXₚ + ε",
        "B": "Y = X₁ + X₂ + … + Xₚ",
        "C": "Y = β₀·β₁·…·βₚ·X₁·X₂·…·Xₚ",
        "D": "Y = ε only, without any X’s"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because the general linear regression model expresses Y as an intercept plus a linear combination of predictors plus an error term. B omits coefficients and the error term, and C is multiplicative rather than linear.",
        "step_by_step": [],
        "interpretation": "The model combines predictors additively, each scaled by its own coefficient, plus random unexplained variation.",
        "business_context": "Banks and retailers routinely use this form to predict risk scores, sales, or churn from several predictors."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_2",
      "tags": [
        "general linear model",
        "regression equation",
        "formula recognition"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the linear regression model Y = β₀ + β₁X₁ + … + βₚXₚ + ε, how is the coefficient βᵢ (for i ≥ 1) typically interpreted?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "As the change in Y when Xᵢ increases by one unit, holding all other X’s constant",
        "B": "As the average of all X variables in the dataset",
        "C": "As the total unexplained variation in Y",
        "D": "As the probability that Xᵢ causes Y"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because βᵢ measures the expected change in Y for a one-unit increase in Xᵢ, ceteris paribus. C is wrong because unexplained variation is represented by ε, not by βᵢ.",
        "step_by_step": [],
        "interpretation": "Coefficients describe marginal effects of each predictor while other predictors are held fixed.",
        "business_context": "In a risk score model, β for FICO shows how much risk score changes per extra FICO point, holding income and LTV fixed."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_2",
      "tags": [
        "regression coefficients",
        "interpretation",
        "ceteris paribus"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the general linear regression model, what does the intercept term β₀ represent conceptually?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The predicted value of Y when all Xᵢ are equal to zero",
        "B": "The total sum of all Xᵢ values in the sample",
        "C": "The maximum possible value of Y",
        "D": "The amount of random error in each observation"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because β₀ is the model’s predicted value of Y when all predictors are set to zero. D is incorrect because random error is captured by ε, not by β₀.",
        "step_by_step": [],
        "interpretation": "In some applications X = 0 is not meaningful, so β₀ may not have a useful real-world interpretation.",
        "business_context": "In a loan risk model, a FICO of 0 is unrealistic, so β₀ is mainly a mathematical anchor rather than a literal baseline risk."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_2",
      "tags": [
        "intercept",
        "general linear model",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the model Risk Score = 150 − 0.1·FICO + 0.5·LTV − 0.00001·Income + ε, how should the coefficient −0.1 on FICO be interpreted?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Risk score increases by 0.1 points for each extra FICO point, holding other variables constant",
        "B": "Risk score decreases by 0.1 points for each extra FICO point, holding other variables constant",
        "C": "Risk score changes randomly with FICO and cannot be interpreted",
        "D": "FICO has no effect on risk score because the coefficient is small"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because a negative coefficient means higher FICO scores are associated with lower predicted risk scores, ceteris paribus. A reverses the sign and so misstates the direction of the effect.",
        "step_by_step": [],
        "interpretation": "A one-unit increase in FICO reduces predicted risk by 0.1, all else equal.",
        "business_context": "This supports the intuitive business idea that more creditworthy borrowers are less risky."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_2",
      "tags": [
        "regression coefficients",
        "interpretation",
        "risk score example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the general linear regression model, what is the role of the error term ε?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It captures all variation in Y explained by the X’s",
        "B": "It captures unexplained variation in Y due to unmodeled factors and randomness",
        "C": "It is only the measurement error in X’s and nothing else",
        "D": "It is a constant added to every predicted value"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because ε represents all influences on Y not captured by the included X’s, including randomness and omitted factors. C is too narrow because ε is not limited to measurement error.",
        "step_by_step": [],
        "interpretation": "Even a good model cannot include every possible driver of Y, so ε absorbs the rest.",
        "business_context": "In a loan risk model, ε might reflect borrower-specific shocks or macroeconomic surprises not in the data."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_2",
      "tags": [
        "error term",
        "general linear model",
        "unexplained variation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following statements about components of the linear regression model Y = β₀ + β₁X₁ + … + βₚXₚ + ε are correct? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Y is the dependent variable the model aims to explain or predict",
        "B": "Xᵢ are independent variables that serve as predictors",
        "C": "ε represents all unexplained influences on Y",
        "D": "βᵢ (i ≥ 1) are random errors that change from observation to observation"
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": {
        "text": "A, B, and C are correct: Y is the outcome, Xᵢ are predictors, and ε captures unexplained variation. D is incorrect because βᵢ are fixed (though unknown) parameters, not random errors; randomness is in ε.",
        "step_by_step": [],
        "interpretation": "Distinguishing parameters (β’s), predictors (X’s), and error (ε) is fundamental to understanding regression.",
        "business_context": "In practice, Y could be sales, X’s could be price and ads, β’s are the effects to estimate, and ε captures noise and omitted factors."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_2",
      "tags": [
        "general linear model",
        "MCA",
        "model components"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In multiple regression, what does \"model specification\" primarily refer to?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Choosing the numerical values of the coefficients βᵢ",
        "B": "Identifying which independent variables Xᵢ to include in the regression equation",
        "C": "Deciding which software package to use for estimation",
        "D": "Collecting raw data from surveys and experiments"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because model specification is about deciding which predictors to include in the model structure. A refers to estimation, which happens after the model form has been specified.",
        "step_by_step": [],
        "interpretation": "Specification is like deciding the ingredients for your model before you estimate their exact amounts.",
        "business_context": "A marketing analyst must decide whether to include price, ads, seasonality, and competitor actions in the sales model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_3",
      "tags": [
        "model specification",
        "multiple regression",
        "independent variables"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which description best matches forward selection as a variable selection strategy in regression?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Start with all candidate predictors and remove them one by one",
        "B": "Start with no predictors and add them one by one based on a criterion",
        "C": "Evaluate every possible subset of predictors exhaustively",
        "D": "Rely only on expert judgment and ignore statistical criteria"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because forward selection begins with an empty model and adds predictors stepwise if they improve the model. A instead describes backward elimination, and C describes all subsets regression.",
        "step_by_step": [],
        "interpretation": "Forward selection is a step-by-step building-up process guided by statistical measures.",
        "business_context": "An e-commerce analyst might first add website visits, then add email sends only if they significantly improve adjusted R²."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_3",
      "tags": [
        "forward selection",
        "variable selection",
        "model specification"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "How does backward elimination differ from forward selection in variable selection for regression models?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Backward elimination starts with all predictors and removes them; forward selection starts with none and adds them",
        "B": "Backward elimination and forward selection are identical procedures",
        "C": "Backward elimination only uses expert judgment, while forward selection is purely statistical",
        "D": "Backward elimination evaluates all possible subsets, while forward selection does not"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because backward elimination begins from the full model and drops weak predictors, while forward selection builds up from an empty model. B is wrong because the direction of search is opposite in the two methods.",
        "step_by_step": [],
        "interpretation": "Both methods are stepwise, but one prunes down and the other builds up.",
        "business_context": "A marketing team might start with all variables (price, multiple ad channels, seasonality) and use backward elimination to simplify the model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_3",
      "tags": [
        "backward elimination",
        "forward selection",
        "variable selection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is the main idea behind all subsets regression in model specification?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Consider only the single best predictor and ignore all others",
        "B": "Evaluate every possible combination of predictors using criteria like adjusted R² or Mallows’ Cp",
        "C": "Randomly select a subset of predictors without any evaluation",
        "D": "Include all potential predictors without any selection"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because all subsets regression systematically evaluates every possible model formed from the candidate predictors using chosen criteria. D is incorrect because it refers to using the full model without selection, not to all subsets comparison.",
        "step_by_step": [],
        "interpretation": "All subsets methods trade off thoroughness against computational cost and overfitting risk.",
        "business_context": "With a small set of candidate marketing variables, an analyst might use all subsets to compare models by adjusted R² before choosing one."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_3",
      "tags": [
        "all subsets",
        "model specification",
        "variable selection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are common risks or mistakes when using automated variable selection methods like stepwise or all subsets? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Including spurious variables that have no real business relevance",
        "B": "Omitting important predictors because the algorithm ignores domain knowledge",
        "C": "Overfitting the model when there are many predictors",
        "D": "Ensuring that the model will always generalize perfectly to new data"
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": {
        "text": "A, B, and C are correct: blind use of automated selection can pick spurious variables, drop important ones, and overfit, especially with many predictors. D is incorrect because automated selection does not guarantee perfect generalization.",
        "step_by_step": [],
        "interpretation": "Automated methods should be combined with theory and expert judgment to avoid unstable or misleading models.",
        "business_context": "A purely data-driven model might include a random promotion code variable that appears significant by chance but has no causal link to sales."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_3",
      "tags": [
        "stepwise regression",
        "all subsets",
        "common mistakes",
        "overfitting",
        "MCA"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In ordinary least squares (OLS) regression, what quantity is minimized when estimating the regression coefficients?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The sum of absolute residuals",
        "B": "The sum of squared residuals",
        "C": "The sum of observed Y values",
        "D": "The variance of the X variables"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "OLS chooses coefficient values that minimize the sum of squared residuals, where each residual is the difference between observed and predicted Y. Option A refers to least absolute deviations, not OLS.",
        "step_by_step": [],
        "interpretation": "Squaring the residuals makes large errors count more heavily in determining the best-fitting line.",
        "business_context": "In a risk scoring model, OLS finds coefficients that make the squared differences between actual and predicted risk scores as small as possible across all customers."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_4",
      "tags": [
        "ordinary least squares",
        "sum of squared residuals",
        "objective function"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the context of OLS, what is a residual for a single observation?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The predicted value of Y from the regression model",
        "B": "The difference between the observed Y and the predicted Y",
        "C": "The value of the X variable with the largest effect",
        "D": "The intercept term of the regression line"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "A residual is defined as the observed Y minus the model’s predicted Y for that observation. Option A is just the prediction itself, not the error around it.",
        "step_by_step": [],
        "interpretation": "Residuals measure how far each data point lies above or below the fitted regression line.",
        "business_context": "For each loan in a bank’s dataset, the residual is the difference between the actual risk score and the risk score predicted by the OLS model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_4",
      "tags": [
        "residual",
        "regression basics",
        "ordinary least squares"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why does OLS square the residuals instead of just summing their raw values when fitting a regression line?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because the sum of raw residuals is always very large",
        "B": "Because positive and negative residuals would cancel out if not squared",
        "C": "Because squared residuals are always normally distributed",
        "D": "Because squaring residuals makes OLS the only unbiased estimator"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Squaring residuals prevents positive and negative errors from canceling each other, giving a meaningful measure of overall error size. Option A is incorrect because the sum of raw residuals in OLS is often close to zero, not large.",
        "step_by_step": [],
        "interpretation": "By squaring, OLS focuses on the magnitude of errors rather than their sign and penalizes large errors more heavily.",
        "business_context": "In forecasting sales, squaring residuals ensures that over-predictions and under-predictions both contribute to the total error being minimized."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_4",
      "tags": [
        "ordinary least squares",
        "loss function",
        "residuals"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A bank fits a linear regression model to predict loan risk scores using FICO and income. Using OLS means the bank chooses coefficients that:",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Maximize the correlation between FICO and income",
        "B": "Minimize the number of loans with prediction errors",
        "C": "Minimize the sum of squared differences between actual and predicted risk scores",
        "D": "Maximize the average predicted risk score"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "OLS specifically finds coefficients that minimize the sum of squared differences between observed and predicted Y values. Option B confuses OLS with minimizing classification mistakes rather than squared prediction errors.",
        "step_by_step": [],
        "interpretation": "The chosen line is the one that best fits historical data in a least-squares sense, not necessarily one that perfectly classifies loans.",
        "business_context": "This approach helps the bank build a stable risk model that reflects patterns in historical loan performance."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_4",
      "tags": [
        "ordinary least squares",
        "regression coefficients",
        "business application"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes a common misconception about OLS parameter estimation?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "OLS minimizes squared residuals, giving more weight to large errors.",
        "B": "OLS always requires checking assumptions like linearity and homoscedasticity.",
        "C": "OLS minimizes the sum of absolute differences between observed and predicted values.",
        "D": "OLS is used to estimate regression coefficients in linear models."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "It is a misconception that OLS minimizes absolute differences; it actually minimizes squared differences. Options A and D correctly describe OLS, and B is good practice rather than a misconception.",
        "step_by_step": [],
        "interpretation": "Confusing squared errors with absolute errors leads to misunderstanding how OLS treats large deviations.",
        "business_context": "Analysts who think OLS minimizes absolute errors may be surprised by how strongly outliers can influence the fitted model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_4",
      "tags": [
        "ordinary least squares",
        "common mistakes",
        "loss function"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which OLS assumption states that the error terms for different observations do not systematically move together?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Linearity in parameters",
        "B": "Independence of errors",
        "C": "Normality of errors",
        "D": "No perfect multicollinearity"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Independence of errors means that error terms are uncorrelated across observations. Linearity (A) concerns the form of the model, not the relationship between errors.",
        "step_by_step": [],
        "interpretation": "If errors are correlated, patterns over time or groups can invalidate standard OLS inference.",
        "business_context": "In time series sales data, ignoring autocorrelated errors can lead to misleading confidence intervals for advertising effects."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_5",
      "tags": [
        "OLS assumptions",
        "independence",
        "errors"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What does the homoscedasticity assumption in OLS regression require?",
      "question_visual": {
        "type": "matplotlib",
        "plot_type": "scatter",
        "params": {
          "x_label": "Predicted Y",
          "y_label": "Residual",
          "title": "Residuals vs Predicted Values (Constant Spread)"
        }
      },
      "question_visual_type": "matplotlib",
      "options": {
        "A": "The mean of the residuals is exactly zero for all samples",
        "B": "The variance of the errors is constant across all levels of X",
        "C": "The X variables follow a normal distribution",
        "D": "The residuals are all positive"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Homoscedasticity means the error variance is constant for all values of the predictors. Option A may be approximately true in OLS, but it is not the homoscedasticity assumption.",
        "step_by_step": [],
        "interpretation": "On a residuals-versus-fitted plot, homoscedasticity appears as a roughly uniform vertical spread of points across the range of fitted values.",
        "business_context": "If prediction errors for high-sales months are much more variable than for low-sales months, homoscedasticity is violated and standard errors from OLS may be unreliable."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_5",
      "tags": [
        "OLS assumptions",
        "homoscedasticity",
        "residual plots"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which OLS assumption is most directly needed to justify using t-tests and confidence intervals in small samples?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Normality of errors",
        "B": "No perfect multicollinearity",
        "C": "Linearity in parameters",
        "D": "Independence of X variables"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Normality of errors underpins the exact sampling distributions used for t-tests and confidence intervals in small samples. No perfect multicollinearity (B) is important for estimating coefficients but not specifically for the normal-based inference.",
        "step_by_step": [],
        "interpretation": "Even if coefficients can be estimated without normality, normal errors make the usual inferential procedures valid in small datasets.",
        "business_context": "A marketing analyst using OLS on a small experimental dataset relies on error normality to interpret p-values for campaign effects."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_5",
      "tags": [
        "OLS assumptions",
        "normality",
        "inference"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What does the 'no perfect multicollinearity' assumption rule out in an OLS regression model with several predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Any correlation between X variables",
        "B": "Any correlation between X variables and Y",
        "C": "One X variable being an exact linear combination of other X variables",
        "D": "Nonlinear relationships between X variables and Y"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "No perfect multicollinearity means no predictor is an exact linear combination of others, so each coefficient is identifiable. Option A is too strong; some correlation between predictors is allowed as long as it is not perfect.",
        "step_by_step": [],
        "interpretation": "If two predictors always move together in a fixed proportion, OLS cannot separate their individual effects.",
        "business_context": "If 'total digital spend' equals 'online ads + social media ads' exactly, including all three in one regression may violate this assumption."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_5",
      "tags": [
        "OLS assumptions",
        "multicollinearity",
        "identifiability"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "An analyst fits an OLS model and plots residuals over time. She observes long runs of positive residuals followed by long runs of negative residuals. Which OLS assumption is most clearly violated?",
      "question_visual": {
        "type": "matplotlib",
        "plot_type": "line",
        "params": {
          "x_label": "Time",
          "y_label": "Residual",
          "title": "Residuals Over Time Showing Runs"
        }
      },
      "question_visual_type": "matplotlib",
      "options": {
        "A": "Linearity in parameters",
        "B": "Independence of errors",
        "C": "Normality of errors",
        "D": "No perfect multicollinearity"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Long runs of same-signed residuals suggest that errors are correlated over time, violating independence of errors. Normality (C) concerns the distribution of residuals, not their time pattern.",
        "step_by_step": [],
        "interpretation": "Autocorrelation means that one period’s error helps predict the next, which standard OLS does not allow for.",
        "business_context": "In monthly sales forecasting, such patterns indicate that time-series methods may be more appropriate than simple OLS."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_5",
      "tags": [
        "OLS assumptions",
        "independence",
        "autocorrelation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are standard assumptions of the classical OLS regression model? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The relationship is linear in parameters",
        "B": "Error terms are independent of each other",
        "C": "Predictors are always normally distributed",
        "D": "There is no perfect multicollinearity among predictors"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are core OLS assumptions: linearity in parameters, independence of errors, and no perfect multicollinearity. C is incorrect because OLS does not require the X variables themselves to be normally distributed.",
        "step_by_step": [],
        "interpretation": "These assumptions help ensure that OLS estimators are well-defined and that inference based on them is valid.",
        "business_context": "When modeling demand with several economic indicators, checking these assumptions helps an analyst trust the estimated effects and uncertainty measures."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_5",
      "tags": [
        "OLS assumptions",
        "BLUE conditions",
        "multiple choice multiple answers"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In model building, what is the main idea behind overfitting a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using too few predictors, so important variables are omitted",
        "B": "Using a model that is too complex and captures random noise in the training data",
        "C": "Using only linear relationships between variables",
        "D": "Using cross-validation to evaluate model performance"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Overfitting occurs when a model is so complex that it fits noise rather than the true underlying signal, harming performance on new data. Option A describes underfitting, the opposite problem.",
        "step_by_step": [],
        "interpretation": "An overfitted model looks very accurate on historical data but often fails badly when applied to fresh data.",
        "business_context": "A retailer’s sales model with dozens of arbitrary predictors might perfectly fit last year’s sales but give poor forecasts for this year."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_6",
      "tags": [
        "overfitting",
        "model complexity",
        "generalization"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best illustrates confusing correlation with causation in a regression context?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Noting that two variables move together but refusing to draw any causal conclusion",
        "B": "Claiming that higher ice cream sales cause more sunburns because they are highly correlated",
        "C": "Checking for confounding variables before making a causal claim",
        "D": "Using randomized experiments to identify causal effects"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Interpreting a simple correlation between ice cream sales and sunburns as evidence that one causes the other is a classic correlation–causation error. Options C and D describe correct approaches to causal reasoning, not confusion.",
        "step_by_step": [],
        "interpretation": "Two variables can be strongly associated because of a third factor (like hot weather) without either directly causing the other.",
        "business_context": "A company might incorrectly infer that a cosmetic website redesign caused higher sales just because both changed around the same time."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_6",
      "tags": [
        "correlation vs causation",
        "regression interpretation",
        "pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is a key risk of blindly using stepwise regression to select variables for a model with many potential predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It always produces the smallest possible training error",
        "B": "It may select variables that appear important by chance and drop truly important ones",
        "C": "It forces all coefficients to be exactly zero",
        "D": "It guarantees a causal interpretation of all included variables"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Stepwise regression can latch onto spurious relationships and omit meaningful predictors, especially when many variables are tested. It does not guarantee causal interpretation (D) or zero coefficients (C).",
        "step_by_step": [],
        "interpretation": "Repeated testing across many candidate predictors inflates the chance of including noise variables in the final model.",
        "business_context": "A marketing team using stepwise selection might end up with a model that includes irrelevant metrics like internal email counts while excluding core drivers like price."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_6",
      "tags": [
        "stepwise regression",
        "model selection",
        "overfitting"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A model has an R² of 0.99 on the data used to fit it but performs poorly on new data. Which explanation is most consistent with this outcome?",
      "question_visual": {
        "type": "matplotlib",
        "plot_type": "line",
        "params": {
          "x_label": "Model Complexity",
          "y_label": "Error",
          "title": "Training vs Test Error",
          "lines": [
            "training_error_decreasing",
            "test_error_U_shape"
          ]
        }
      },
      "question_visual_type": "matplotlib",
      "options": {
        "A": "The model is underfitting the training data",
        "B": "The model is overfitting the training data",
        "C": "The predictors have no relationship with the response",
        "D": "The response variable is measured without error"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Very high in-sample R² with poor out-of-sample performance is a hallmark of overfitting. Underfitting (A) would show poor fit even on the training data.",
        "step_by_step": [],
        "interpretation": "The model has likely captured random quirks of the training set that do not generalize to new observations.",
        "business_context": "A sales forecasting model that perfectly fits last year’s data but fails on this year’s likely included too many spurious predictors or complex terms."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_6",
      "tags": [
        "overfitting",
        "R-squared",
        "model evaluation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which practices help reduce the risks of overfitting and misinterpreting regression results? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Evaluating model performance on holdout or cross-validation data",
        "B": "Relying solely on in-sample R² to judge model quality",
        "C": "Using domain knowledge to judge whether selected variables make sense",
        "D": "Treating any statistically significant coefficient as proof of causation"
      },
      "correct_answer": [
        "A",
        "C"
      ],
      "explanation": {
        "text": "A and C help guard against overfitting and causal misinterpretation by checking generalization and using subject-matter insight. B and D are common mistakes: focusing only on in-sample R² and equating significance with causality.",
        "step_by_step": [],
        "interpretation": "Good modeling combines statistical validation with substantive understanding of the business or scientific context.",
        "business_context": "A company might use cross-validation and expert review to ensure its customer churn model is both predictive and plausible before deploying it."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_6",
      "tags": [
        "overfitting prevention",
        "model validation",
        "correlation vs causation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In a regression model of sales (Y) on advertising spend (X), what is the primary role of X as an explanatory variable?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "X is the variable whose values are predicted by Y.",
        "B": "X is the variable hypothesized to help explain or predict Y.",
        "C": "X is a random error term capturing unexplained variation in Y.",
        "D": "X is always a qualitative variable coded with dummies."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Advertising spend is an explanatory (independent) variable because it is used to explain or predict changes in sales, the response variable. It is not the outcome being predicted (A) nor the error term (C), and explanatory variables can be either quantitative or qualitative (so D is too narrow).",
        "step_by_step": [],
        "interpretation": "Explanatory variables are inputs X that we think drive the output Y.",
        "business_context": "In business, variables like price, advertising, and income are typical explanatory variables used to predict sales, profit, or demand."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_7",
      "tags": [
        "explanatory variables",
        "regression basics",
        "response variable"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A retailer finds that the effect of price discounts on sales is much stronger in low-income areas than in high-income areas. Which regression feature best captures this pattern?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "An interaction term between price and income",
        "B": "A quadratic term in sales",
        "C": "A log transformation of the response only",
        "D": "Dropping income from the model"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "An interaction term between price and income allows the effect of price on sales to differ across income levels. A quadratic term in sales (B) changes the shape of Y, and a log transformation of Y (C) mainly addresses scale or variance, not differing effects of X across groups.",
        "step_by_step": [],
        "interpretation": "Interaction terms model situations where the impact of one X depends on the level of another X.",
        "business_context": "Marketing models often include interactions like price × income or price × advertising to reflect different sensitivities in different segments."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_7",
      "tags": [
        "interaction terms",
        "model complexity",
        "business regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why might an analyst include a polynomial term such as price² in a regression of sales on price?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "To force the relationship between price and sales to be strictly linear",
        "B": "To allow the relationship between price and sales to bend or curve",
        "C": "To remove all correlation between price and sales",
        "D": "To convert a qualitative price category into a numeric scale"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "A polynomial term like price² lets the model capture curvature, so the effect of price on sales can increase or decrease in a nonlinear way. It does not force linearity (A) or guarantee zero correlation (C), and it is not used to code qualitative categories (D).",
        "step_by_step": [],
        "interpretation": "Polynomial terms are a simple way to model nonlinear relationships using regression.",
        "business_context": "Businesses may see diminishing returns to discounts or price increases, which can be captured with quadratic terms."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_7",
      "tags": [
        "polynomial terms",
        "nonlinear relationships",
        "price elasticity"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "An analyst models log(Sales) instead of Sales as the response in a regression. Which is a common reason for this transformation?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "To ensure that the error term becomes exactly zero",
        "B": "To stabilize variance and make the relationship with predictors more linear",
        "C": "To convert all explanatory variables into qualitative variables",
        "D": "To remove the need for explanatory variables altogether"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Transforming Sales to log(Sales) can stabilize variance and help linearize relationships with the X’s, improving model assumptions. It does not make the error zero (A), does not change X’s to qualitative (C), and does not eliminate the need for explanatory variables (D).",
        "step_by_step": [],
        "interpretation": "Transformations are tools to make the regression model fit better and satisfy its assumptions.",
        "business_context": "Using log(Sales) often leads to interpreting coefficients as approximate percentage effects, which is useful in elasticity analysis."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_7",
      "tags": [
        "transformations",
        "log transformation",
        "variance stabilization"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common mistake when adding interaction or polynomial terms to a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Checking whether the added terms are meaningful in the business context",
        "B": "Adding such terms mechanically without considering theory or interpretability",
        "C": "Using transformations to stabilize variance in the response",
        "D": "Starting from a simple model and then assessing model fit"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Adding interaction or polynomial terms mechanically, without theoretical or business justification, can create overly complex, multicollinear models. Options A, C, and D describe good modeling practice, not mistakes.",
        "step_by_step": [],
        "interpretation": "Complexity terms should reflect plausible mechanisms, not just be added because software allows it.",
        "business_context": "In business analytics, unjustified complexity can confuse stakeholders and lead to unreliable decisions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_7",
      "tags": [
        "model complexity",
        "common mistakes",
        "interaction terms",
        "polynomial terms"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In a regression of weekly sales (Y) on price (X1) and store size (X2), how should price and store size be classified?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Both are explanatory (independent) variables",
        "B": "Both are response (dependent) variables",
        "C": "Price is the response and store size is explanatory",
        "D": "Store size is the response and price is explanatory"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Price and store size are inputs used to explain or predict weekly sales, so they are explanatory variables. The outcome of interest, weekly sales, is the response variable, not either X.",
        "step_by_step": [],
        "interpretation": "Explanatory variables are the X’s you use to predict Y.",
        "business_context": "Retailers often treat store characteristics and marketing actions as explanatory variables for sales or profit outcomes."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_8",
      "tags": [
        "explanatory variables",
        "response variable",
        "regression basics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which candidate variable is LEAST likely to be useful as an explanatory variable in a business regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "A variable that is grounded in business theory but has very little variation across observations",
        "B": "A variable that varies across observations and is theoretically linked to the outcome",
        "C": "A variable that is measurable, varies, and has a clear causal story in the business context",
        "D": "A variable that represents a marketing action managers can adjust and that differs across stores"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A variable with almost no variation (A) cannot explain differences in the response, even if it is theoretically relevant. Options B, C, and D describe variables that are both relevant and variable, which makes them useful as explanatory variables.",
        "step_by_step": [],
        "interpretation": "For an explanatory variable to help, it must both make sense theoretically and actually vary in the data.",
        "business_context": "For example, a fee that is identical for all customers cannot explain why some customers buy more than others."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_8",
      "tags": [
        "explanatory variables",
        "variable selection",
        "data requirements"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes a key requirement for explanatory variables in regression-based business analysis?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "They must always be under the direct control of the analyst.",
        "B": "They must be clearly distinct from the response variable being modeled.",
        "C": "They must be uncorrelated with each other.",
        "D": "They must be purely qualitative variables coded as dummies."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Explanatory variables should be clearly distinguished from the response; you should not treat the outcome itself as an X. They do not have to be directly controllable (A), uncorrelated (C), or purely qualitative (D).",
        "step_by_step": [],
        "interpretation": "Confusing Y with X undermines the logic and interpretation of the regression model.",
        "business_context": "Using profit as both an explanatory variable and the outcome in the same model would make the results meaningless."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_8",
      "tags": [
        "explanatory variables",
        "response variable",
        "model structure"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A firm includes 'store size' as an explanatory variable in a sales model. Which property makes this a good choice?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Store size is constant across all stores.",
        "B": "Store size is theoretically related to sales and varies across stores.",
        "C": "Store size is actually the same quantity as weekly sales.",
        "D": "Store size is chosen only because it appears in the database, with no theory."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Store size is a good explanatory variable when it varies across stores and is theoretically linked to the capacity to generate sales. A constant variable (A) cannot explain differences, and choosing variables without theory (D) is a common mistake.",
        "step_by_step": [],
        "interpretation": "Good explanatory variables connect business logic (bigger stores can sell more) with variation in the data.",
        "business_context": "Retailers often use store size to normalize or explain sales performance across locations."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_8",
      "tags": [
        "explanatory variables",
        "business analytics",
        "variable properties"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common mistake when selecting explanatory variables for a regression model in business analysis?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Including variables based only on their availability in the dataset, without business rationale",
        "B": "Checking that variables are measurable and have sufficient variation",
        "C": "Ensuring variables are grounded in marketing or economic theory",
        "D": "Distinguishing clearly between explanatory variables and the response variable"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Using variables just because they are present in the dataset, with no theoretical or business justification, is a common error. Options B, C, and D describe good practices in choosing explanatory variables.",
        "step_by_step": [],
        "interpretation": "Data-driven choices should still be guided by domain knowledge to avoid spurious relationships.",
        "business_context": "For example, including a customer’s favorite color as an explanatory variable for credit risk just because it is recorded would be poor practice."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_8",
      "tags": [
        "explanatory variables",
        "common mistakes",
        "variable selection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "When choosing explanatory variables for a regression model of customer churn, what should be the FIRST guiding principle?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Start with variables that show the highest sample correlation with churn, regardless of meaning.",
        "B": "Start from marketing and behavioral theory about what drives churn.",
        "C": "Start with variables that are easiest to obtain from IT systems.",
        "D": "Start by including every available variable and remove them later at random."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Theoretical and domain knowledge about what drives churn should be the starting point for choosing explanatory variables. Purely correlation-based (A) or convenience-based (C) selection risks including irrelevant or spurious predictors.",
        "step_by_step": [],
        "interpretation": "Theory helps you focus on variables that plausibly cause changes in the response.",
        "business_context": "For churn, variables like recency, frequency, and monetary value are guided by well-known RFM marketing frameworks."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_9",
      "tags": [
        "variable selection",
        "theory-driven modeling",
        "customer churn"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A bank is building a model to predict loan default. Which variable choice best reflects the use of domain knowledge in selecting explanatory variables?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Including customer income, past repayment history, and debt-to-income ratio",
        "B": "Including customers’ favorite music genre and favorite color",
        "C": "Including only variables that are free to obtain, regardless of relevance",
        "D": "Using default status itself as an explanatory variable for default"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Income, repayment history, and debt-to-income ratio are standard credit risk factors grounded in financial theory, so they reflect domain knowledge. Favorite music or color (B) has no established link to default, and using default as its own predictor (D) is logically flawed.",
        "step_by_step": [],
        "interpretation": "Good explanatory variables are those that theory suggests are drivers of the outcome.",
        "business_context": "Regulators and risk managers expect default models to use established risk drivers, not arbitrary customer traits."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_9",
      "tags": [
        "domain knowledge",
        "credit risk",
        "explanatory variables"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which situation illustrates the problem of data snooping when selecting explanatory variables?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Choosing variables based on established economic theory",
        "B": "Scanning the dataset for any variable correlated with Y and treating it as a driver without business justification",
        "C": "Consulting domain experts about which factors affect the response",
        "D": "Limiting the model to variables that managers can interpret and act on"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Data snooping involves searching for correlations in the data and declaring them explanatory without theoretical or business rationale. Options A, C, and D describe more disciplined, theory-informed variable selection.",
        "step_by_step": [],
        "interpretation": "Correlation alone does not prove a variable is a meaningful driver of the outcome.",
        "business_context": "A spurious correlation (e.g., ice cream sales and stock prices) may appear in one dataset but will not generalize for decision making."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_9",
      "tags": [
        "data snooping",
        "variable selection",
        "common mistakes"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In modeling customer churn, which candidate variable is most consistent with using theoretical and domain knowledge?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Customer’s preferred website color theme",
        "B": "Time since last purchase (recency)",
        "C": "Randomly generated customer ID number",
        "D": "Server room temperature in the company’s headquarters"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Recency of last purchase is a well-established driver of churn in marketing theory, so it aligns with domain knowledge. The other options have no plausible theoretical link to churn.",
        "step_by_step": [],
        "interpretation": "Domain knowledge points to behavior-related variables rather than arbitrary technical or aesthetic details.",
        "business_context": "RFM models (Recency, Frequency, Monetary value) are standard tools for predicting customer retention and churn."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_9",
      "tags": [
        "theory-driven modeling",
        "customer churn",
        "business context"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following practices reflect GOOD use of theoretical and business-domain knowledge in selecting explanatory variables for regression? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using marketing literature to prioritize recency, frequency, and monetary value as churn predictors",
        "B": "Including variables that are correlated with the response but have no plausible business explanation",
        "C": "Consulting domain experts to identify operational factors that drive the response",
        "D": "Ignoring well-established drivers in the literature because they are harder to measure"
      },
      "correct_answer": [
        "A",
        "C"
      ],
      "explanation": {
        "text": "A and C are correct because they use theory and expert knowledge to guide variable selection. B is data snooping—choosing variables only by correlation—and D discards known drivers, weakening the model’s credibility.",
        "step_by_step": [],
        "interpretation": "Good models start from what is already known about the business, not just from patterns found in a single dataset.",
        "business_context": "Combining academic research and internal expertise helps build models that managers trust and can act upon."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_9",
      "tags": [
        "variable selection",
        "theory-driven modeling",
        "multiple correct",
        "business analytics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In regression modeling, why is it important that an explanatory variable has noticeable variation across observations?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Without variation, its relationship with the response cannot be estimated.",
        "B": "Low variation always increases the statistical significance of the variable.",
        "C": "Variables with little variation are easier to interpret causally.",
        "D": "Lack of variation guarantees the variable is a perfect predictor."
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because if a variable is nearly the same for all observations, the model cannot see how changes in it relate to changes in the response. B and D are wrong because low variation does not improve significance or guarantee prediction; it simply provides no information about differences in the response.",
        "step_by_step": [],
        "interpretation": "To explain differences in Y, X itself must differ across cases.",
        "business_context": "For example, if all customers pay the same fee, that fee cannot explain why their spending differs."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_10",
      "tags": [
        "variability",
        "explanatory variables",
        "regression basics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A company believes 'brand loyalty' affects spending but has no reliable way to measure it for each customer. From a modeling perspective, what is the main problem with using 'brand loyalty' directly as an explanatory variable?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It is conceptually irrelevant to customer spending.",
        "B": "It cannot be consistently recorded, so it is not practically usable.",
        "C": "Explanatory variables must always be binary, and loyalty is not.",
        "D": "Unmeasurable variables automatically have too much variation."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because even if a concept is important, if it cannot be measured consistently, it cannot be included directly in a regression model. A is wrong because brand loyalty may be conceptually relevant; the issue is measurement, not relevance.",
        "step_by_step": [],
        "interpretation": "Practical data availability is as important as theoretical importance.",
        "business_context": "Analysts often replace such concepts with measurable proxies, like repeat purchase rate."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_10",
      "tags": [
        "measurability",
        "data availability",
        "practical modeling"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following variables is LEAST useful as an explanatory variable in a regression model for monthly customer spending, assuming the description is accurate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Customer age in years, recorded for all customers with a wide range.",
        "B": "Number of purchases last month, recorded exactly for each customer.",
        "C": "A flat platform fee that is identical for 99.9% of customers.",
        "D": "Total number of website visits last month, recorded in server logs."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because a variable that is almost identical for everyone has almost no variation and cannot explain differences in spending. A, B, and D all vary across customers and are measurable, so they can potentially explain variation in the response.",
        "step_by_step": [],
        "interpretation": "Near-constant variables add noise but no explanatory power.",
        "business_context": "Flat fees or standard policies rarely help explain why some customers spend more than others."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_10",
      "tags": [
        "variability",
        "data selection",
        "regression practice"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A telecom analyst wants to include 'customer mood' as an explanatory variable for monthly data usage but only has very noisy, sporadic survey responses for a small subset of customers. Which criterion for selecting explanatory variables is most clearly violated?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Measurability and data quality",
        "B": "Linearity of the relationship",
        "C": "Independence of errors",
        "D": "Normality of residuals"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because the variable is poorly measured and not available consistently across customers, violating measurability and availability. B, C, and D are model-assumption issues, not criteria for whether a variable can be used in the first place.",
        "step_by_step": [],
        "interpretation": "If you cannot measure X reliably for most observations, it is a poor choice as an explanatory variable.",
        "business_context": "In practice, analysts often drop such variables or search for better proxies (e.g., behavioral data)."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_10",
      "tags": [
        "measurability",
        "data availability",
        "telecom example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are essential properties of a good explanatory variable in a regression model? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It can be measured consistently for the observations.",
        "B": "It shows meaningful variation across observations.",
        "C": "It is conceptually interesting, even if impossible to measure.",
        "D": "Its value is almost identical for all observations."
      },
      "correct_answer": [
        "A",
        "B"
      ],
      "explanation": {
        "text": "A and B are correct because explanatory variables must be both measurable and variable to help explain differences in the response. C and D describe variables that are either unmeasurable or nearly constant, so they cannot contribute meaningfully to a practical regression model.",
        "step_by_step": [],
        "interpretation": "Both data quality (can we record it?) and variability (does it differ across cases?) matter.",
        "business_context": "Businesses often prioritize variables available in operational systems that differ across customers or time."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_10",
      "tags": [
        "measurability",
        "variability",
        "MCA",
        "foundations"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes the problem of confusing correlation with causation when selecting explanatory variables?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Any variable that is correlated with the response must be omitted from the model.",
        "B": "A correlated variable is always the true cause of changes in the response.",
        "C": "Two variables may move together because of a third factor, not because one causes the other.",
        "D": "Causation and correlation are identical concepts in regression analysis."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because a third, omitted factor can drive both variables, creating correlation without direct causation. A and D are incorrect generalizations, and B wrongly assumes that correlation proves causation.",
        "step_by_step": [],
        "interpretation": "Seeing X and Y move together does not tell you whether X causes Y, Y causes X, or both are driven by some Z.",
        "business_context": "For example, rain can increase both umbrella and soup sales without umbrellas causing soup purchases."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_11",
      "tags": [
        "correlation vs causation",
        "regression pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the context of regression, what is omitted variable bias most closely associated with?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Leaving out a variable that has no relationship with the response.",
        "B": "Leaving out an important variable that is related to both included predictors and the response.",
        "C": "Including too many irrelevant predictors in the model.",
        "D": "Using variables based on theory rather than on the observed data."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because omitted variable bias occurs when an important driver of the response is left out and is also correlated with included predictors, distorting their estimated effects. A describes a harmless omission, and C refers to overfitting, not omitted variable bias.",
        "step_by_step": [],
        "interpretation": "Missing a key driver can make other coefficients look larger or smaller than they truly are.",
        "business_context": "Ignoring weather when modeling umbrella sales can bias the estimated effect of promotions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_11",
      "tags": [
        "omitted variable bias",
        "regression pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is the main risk of including many irrelevant explanatory variables in a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The model will automatically become more causal.",
        "B": "The model may overfit the current data and perform poorly on new data.",
        "C": "The coefficients of important variables will always become zero.",
        "D": "The model’s predictions will be unaffected, only its appearance will change."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because irrelevant variables increase complexity and can cause the model to overfit noise in the current sample, harming generalization. A and C are false claims, and D ignores the impact of unnecessary predictors on prediction accuracy.",
        "step_by_step": [],
        "interpretation": "More variables are not always better; they can make the model too tailored to one dataset.",
        "business_context": "Adding dozens of weak customer attributes may fit historical churn but fail on future cohorts."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_11",
      "tags": [
        "irrelevant variables",
        "overfitting",
        "model selection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which situation best illustrates data snooping when selecting explanatory variables for a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Choosing predictors based on a clear business theory before looking at the data.",
        "B": "Trying many possible predictors and only reporting those that appear significant in the current dataset.",
        "C": "Dropping a variable with a strong theoretical link but poor data quality.",
        "D": "Using a variable that is easy to measure but has no variation."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because data snooping refers to repeatedly mining the same dataset for patterns and selecting variables solely because they look significant in that sample. A is the opposite of data snooping, and C and D describe other issues related to data quality and variability.",
        "step_by_step": [],
        "interpretation": "If variable choice is driven mainly by what looks good in one dataset, the model may not hold up on new data.",
        "business_context": "For example, testing hundreds of time windows and keeping the one that 'works' is classic data snooping."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_11",
      "tags": [
        "data snooping",
        "model selection",
        "overfitting"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are common pitfalls when identifying explanatory variables for a business regression model? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Assuming any statistically significant predictor is causal.",
        "B": "Omitting important variables that influence the response.",
        "C": "Including many variables with no plausible link to the response.",
        "D": "Selecting variables based only on theoretical reasoning and never checking data."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": {
        "text": "A, B, and C are correct because they correspond to correlation–causation confusion, omitted variable bias, and including irrelevant variables that can cause overfitting. D is not a pitfall by itself; theory-driven selection is recommended, provided it is combined with appropriate empirical checking.",
        "step_by_step": [],
        "interpretation": "Good variable selection balances theory with data and avoids overinterpreting patterns from one dataset.",
        "business_context": "Missteps here can lead to misguided marketing campaigns or misallocated resources."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_11",
      "tags": [
        "regression pitfalls",
        "MCA",
        "model building"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the retail-chain weekly sales example, which of the following is the response variable in the regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Store size (square footage)",
        "B": "Weekly advertising spend",
        "C": "Weekly store sales",
        "D": "Distance to the nearest competitor"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because the model is built to forecast weekly store sales, so sales is the response (dependent) variable. Store size, ad spend, and competitor distance are explanatory (independent) variables used to explain or predict sales.",
        "step_by_step": [],
        "interpretation": "The response is the outcome you want to predict; the others are inputs.",
        "business_context": "Managers use predicted weekly sales to plan inventory and staffing."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_12",
      "tags": [
        "response variable",
        "sales forecasting",
        "regression application"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why is 'distance to the nearest competitor' a reasonable explanatory variable in the retail-chain weekly sales model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Closer competitors always increase a store’s sales.",
        "B": "Competitor distance plausibly affects how many customers choose the store.",
        "C": "It is the only variable that can be measured for all stores.",
        "D": "Distance is required in every regression model regardless of context."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because competitor distance can influence customer choice and thus weekly sales, making it a plausible driver. A is wrong in direction, and C and D are false general statements unrelated to the specific business context.",
        "step_by_step": [],
        "interpretation": "Variables are chosen because they can reasonably affect the outcome, not because they are mathematically required.",
        "business_context": "Retailers routinely analyze how nearby competitors impact store performance."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_12",
      "tags": [
        "explanatory variables",
        "competition",
        "retail example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the retail-chain example, which of the following BEST explains why average household income around a store is used as an explanatory variable for weekly sales?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Higher local income is expected to increase customers’ ability to spend.",
        "B": "Income never changes over time, so it stabilizes the model.",
        "C": "Income is unrelated to purchasing power but easy to measure.",
        "D": "Income is used only to replace store size in the model."
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because higher household income typically means greater purchasing power, which can raise weekly sales. B and C misstate the economic meaning of income, and D is incorrect because income complements, rather than replaces, store size as a predictor.",
        "step_by_step": [],
        "interpretation": "Economic and demographic variables are often used because they capture customers’ capacity to buy.",
        "business_context": "Chains often target high-income areas when deciding where to open new stores."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_12",
      "tags": [
        "income",
        "retail example",
        "demand drivers"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following variables from the retail-chain example is MOST clearly under management control and can be adjusted to influence future weekly sales predictions?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Store size (for existing stores)",
        "B": "Local population density",
        "C": "Weekly advertising spend",
        "D": "Distance to the nearest competitor"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because advertising spend can typically be changed week to week by managers to influence sales. For existing stores, size, local population, and competitor distance are largely fixed in the short run.",
        "step_by_step": [],
        "interpretation": "Some explanatory variables are levers managers can move; others are environmental conditions.",
        "business_context": "Forecasting how changes in ad spend affect sales helps optimize marketing budgets."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_12",
      "tags": [
        "controllable variables",
        "marketing",
        "sales forecasting"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "In the retail-chain weekly sales regression, which of the following are used as explanatory variables to help forecast sales? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Store size (square footage)",
        "B": "Local population density",
        "C": "Weekly store sales",
        "D": "Weekly advertising spend"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are correct because store size, population density, and advertising spend are described as predictors of weekly sales. C is the response variable, not an explanatory variable, in this modeling setup.",
        "step_by_step": [],
        "interpretation": "Explanatory variables are inputs; the response is the output being predicted.",
        "business_context": "These predictors allow the chain to compare stores and plan for new locations."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_12",
      "tags": [
        "explanatory variables",
        "MCA",
        "retail forecasting"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst wants to predict monthly sales (a continuous variable) using only advertising spend in TV (a quantitative predictor). Which regression model is most appropriate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Simple Linear Regression",
        "B": "Multiple Linear Regression",
        "C": "ANCOVA",
        "D": "Logistic Regression"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Simple Linear Regression is used when you model a continuous outcome using a single quantitative predictor. Multiple linear regression adds more predictors, ANCOVA adds group comparisons with covariates, and logistic regression is for binary outcomes, not continuous sales.",
        "step_by_step": [],
        "interpretation": "With one quantitative X and a continuous Y, SLR is the standard starting tool.",
        "business_context": "This is like predicting revenue from just one driver such as TV spend, a common first-pass model in marketing analytics."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_13",
      "tags": [
        "simple linear regression",
        "model choice",
        "continuous outcome"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which situation is BEST modeled using logistic regression rather than a linear regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Predicting total annual sales in dollars from price and advertising spend",
        "B": "Predicting customer satisfaction score on a 0–100 scale from service time",
        "C": "Predicting whether a customer will churn (yes/no) from usage variables",
        "D": "Comparing average sales across three regions while adjusting for store size"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Logistic regression is designed for binary outcomes such as churn yes/no. Options A and B describe continuous outcomes suited to linear models, and D describes group mean comparisons with a covariate, which is the role of ANCOVA.",
        "step_by_step": [],
        "interpretation": "Whenever the dependent variable is a 0/1 event, logistic regression is the usual choice.",
        "business_context": "Firms commonly use logistic regression to predict events like churn, default, or purchase conversion."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_13",
      "tags": [
        "logistic regression",
        "binary outcome",
        "model selection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A retailer wants to compare mean weekly sales across four store layouts while controlling for local average income (a quantitative covariate). Which model is most appropriate for this goal?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Simple Linear Regression",
        "B": "Multiple Linear Regression without group indicators",
        "C": "ANCOVA",
        "D": "Logistic Regression"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "ANCOVA is used to compare group means (store layouts) while adjusting for one or more quantitative covariates (income). Simple or multiple linear regression without group indicators does not explicitly compare layouts, and logistic regression is for binary outcomes.",
        "step_by_step": [],
        "interpretation": "ANCOVA combines ANOVA-style group comparisons with regression on covariates.",
        "business_context": "This lets the retailer assess layout effects fairly after accounting for differences in neighborhood income."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_13",
      "tags": [
        "ANCOVA",
        "group comparison",
        "covariate adjustment"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "For comparing two linear regression models with the SAME dependent variable and sample, which statistic is most appropriate to penalize for adding unnecessary predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "R²",
        "B": "Adjusted R²",
        "C": "Hosmer–Lemeshow test statistic",
        "D": "ROC curve AUC"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Adjusted R² is specifically designed to account for the number of predictors, penalizing models that add variables without real improvement. Plain R² always increases when predictors are added, and Hosmer–Lemeshow and AUC are used for logistic, not linear, models.",
        "step_by_step": [],
        "interpretation": "Adjusted R² balances goodness-of-fit against model complexity for linear regression.",
        "business_context": "Analysts use adjusted R² to avoid overfitting when building revenue or cost prediction models."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_13",
      "tags": [
        "model comparison",
        "adjusted R-squared",
        "overfitting"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "An analyst fits a logistic regression model to predict purchase (1 = buy, 0 = not buy). Which of the following is a commonly used performance metric for this type of model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "R²",
        "B": "Adjusted R²",
        "C": "ROC curve and AUC",
        "D": "Durbin–Watson statistic"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "ROC curves and AUC are standard measures of how well a logistic regression discriminates between classes (buy vs not). R² and adjusted R² are for linear models, and Durbin–Watson is used to check autocorrelation in residuals, not classification performance.",
        "step_by_step": [],
        "interpretation": "For binary outcomes, discrimination metrics like ROC/AUC matter more than linear-model R².",
        "business_context": "Marketing teams often compare churn or conversion models using AUC to see which best separates likely from unlikely buyers."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_13",
      "tags": [
        "logistic regression",
        "ROC",
        "AUC",
        "model evaluation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A bank wants to predict customers’ monthly spending (continuous) using income, age, and existing balance as predictors. Which model is most suitable?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Simple Linear Regression",
        "B": "Multiple Linear Regression",
        "C": "Logistic Regression",
        "D": "ANCOVA"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Multiple Linear Regression is appropriate because the outcome is continuous and there are several predictors. Simple linear regression uses only one predictor, logistic regression is for binary outcomes, and ANCOVA is focused on group mean comparisons with covariates.",
        "step_by_step": [],
        "interpretation": "When Y is continuous and you have multiple X’s, MLR is the standard choice.",
        "business_context": "Banks commonly model spending or balances using multiple customer characteristics in an MLR framework."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_14",
      "tags": [
        "multiple linear regression",
        "model selection",
        "continuous outcome"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In choosing between multiple linear regression and logistic regression on the same dataset, what SINGLE feature should primarily guide the choice of model family?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The number of predictors available",
        "B": "The type of the dependent variable",
        "C": "The presence of missing values in predictors",
        "D": "Whether predictors are categorical or quantitative"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "The type of dependent variable is the key: continuous outcomes suggest linear regression, while binary outcomes suggest logistic regression. The number or type of predictors matters for model details, but does not change the basic family as much as Y’s type does.",
        "step_by_step": [],
        "interpretation": "Always start by asking: is Y continuous, binary, or a group mean comparison?",
        "business_context": "In business analytics, the same customer dataset can support different models depending on whether you forecast spend (continuous) or default (binary)."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_14",
      "tags": [
        "model selection",
        "dependent variable",
        "logistic regression",
        "linear regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A bank wants to know which of three marketing campaigns (Email, SMS, Direct Mail) leads to higher average balances, while adjusting for age. Which model best fits this objective?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Multiple Linear Regression without campaign indicators",
        "B": "Logistic Regression with balance as Y",
        "C": "ANCOVA with campaign as factor and age as covariate",
        "D": "Simple Linear Regression of balance on age only"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "ANCOVA is specifically for comparing group means (campaigns) while controlling for a quantitative covariate (age). Ignoring campaign indicators or age would not answer the adjusted group comparison, and logistic regression is not appropriate for a continuous balance outcome.",
        "step_by_step": [],
        "interpretation": "ANCOVA lets you compare campaigns fairly by holding age constant statistically.",
        "business_context": "This helps marketing decide which campaign truly performs better after accounting for customer age differences."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_14",
      "tags": [
        "ANCOVA",
        "campaign analysis",
        "group comparison"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following business questions MOST clearly calls for logistic regression rather than multiple linear regression or ANCOVA?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "How much will each customer spend next month?",
        "B": "Do average balances differ across campaigns after adjusting for age?",
        "C": "What is the probability that a customer will default on a loan?",
        "D": "How does average spending change with income and age?"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Logistic regression is used to estimate probabilities of binary outcomes, such as default vs no default. The other options focus on continuous outcomes or group mean comparisons, which are handled by linear regression or ANCOVA.",
        "step_by_step": [],
        "interpretation": "Probability of an event is the hallmark use case for logistic regression.",
        "business_context": "Credit risk models commonly use logistic regression to estimate default probabilities for lending decisions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_14",
      "tags": [
        "logistic regression",
        "binary outcome",
        "credit risk"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following model choices correctly match the business objective and Y type? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using multiple linear regression to predict monthly spending amount from income and age",
        "B": "Using logistic regression to predict default (yes/no) from credit score and income",
        "C": "Using ANCOVA to compare mean balances across campaigns while controlling for age",
        "D": "Using logistic regression to predict exact purchase amount in dollars"
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": {
        "text": "A, B, and C correctly align model type with the dependent variable and question: continuous Y with MLR, binary Y with logistic, and group mean comparisons with a covariate with ANCOVA. D is incorrect because logistic regression is not used for predicting exact continuous amounts.",
        "step_by_step": [],
        "interpretation": "Always match the regression framework to whether Y is continuous, binary, or a group mean comparison with covariates.",
        "business_context": "Choosing the right model ensures spending forecasts, default predictions, and campaign comparisons are statistically sound and interpretable."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_14",
      "tags": [
        "model selection",
        "multiple linear regression",
        "logistic regression",
        "ANCOVA"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which assumption of linear regression states that the average relationship between the predictors and the response can be represented by a straight-line function of the predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Independence of errors",
        "B": "Linearity",
        "C": "Normality of errors",
        "D": "Homoscedasticity"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "The linearity assumption requires that the expected value of Y given X is a linear function of the predictors. Independence concerns how errors relate to each other, normality is about the distribution of errors, and homoscedasticity concerns the constancy of error variance.",
        "step_by_step": [],
        "interpretation": "Linearity means the trend of Y versus X can be well-approximated by a straight line or linear combination of predictors.",
        "business_context": "If spending increases in a curved rather than straight pattern with income, a simple linear model may be misleading."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_15",
      "tags": [
        "assumptions",
        "linearity",
        "linear regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In a multiple linear regression model, which assumption is directly concerned with predictors being too strongly related to each other, making coefficient estimates unstable?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Normality of errors",
        "B": "No multicollinearity among predictors",
        "C": "Independence of errors",
        "D": "Homoscedasticity"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "The no multicollinearity assumption requires that predictors are not highly correlated with each other; otherwise, coefficients can be unstable and hard to interpret. Normality, independence, and homoscedasticity are properties of the residuals, not of relationships among X’s.",
        "step_by_step": [],
        "interpretation": "If two predictors tell almost the same story, the model struggles to separate their individual effects.",
        "business_context": "For example, heavily overlapping advertising variables (TV and online that always move together) can cause multicollinearity in a sales model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_15",
      "tags": [
        "assumptions",
        "multicollinearity",
        "multiple regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which assumption is MOST directly assessed by examining a residuals-versus-fitted-values plot for a funnel shape (residual spread increasing with fitted values)?",
      "question_visual": {
        "type": "matplotlib",
        "plot_type": "scatter",
        "params": {
          "x_label": "Fitted values",
          "y_label": "Residuals",
          "title": "Residuals vs Fitted: Funnel Shape",
          "pattern": "increasing_spread"
        }
      },
      "question_visual_type": "matplotlib",
      "options": {
        "A": "Linearity",
        "B": "Independence of errors",
        "C": "Normality of errors",
        "D": "Homoscedasticity (constant variance)"
      },
      "correct_answer": [
        "D"
      ],
      "explanation": {
        "text": "A funnel-shaped residual plot indicates changing error variance as the fitted values change, violating homoscedasticity. Linearity is about the mean pattern, independence is about correlation over time or order, and normality is about the overall distribution of residuals.",
        "step_by_step": [],
        "interpretation": "Equal spread of residuals across all fitted values is a visual sign of constant variance.",
        "business_context": "If sales forecast errors grow for larger stores, standard errors may be unreliable and decisions based on them could be risky."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_15",
      "tags": [
        "assumptions",
        "homoscedasticity",
        "diagnostics",
        "residual plots"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why do the standard linear regression assumptions (linearity, independent normal errors with constant variance, and low multicollinearity) matter for business decision-making?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "They guarantee that R² will always be close to 1",
        "B": "They ensure that p-values and confidence intervals from the model are trustworthy",
        "C": "They prevent any outliers from occurring in the data",
        "D": "They make the data automatically follow a normal distribution"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "These assumptions underpin the validity of inferential results like p-values and confidence intervals; if they fail badly, inferences can be misleading. They do not guarantee high R², remove outliers, or force the raw data to be normal.",
        "step_by_step": [],
        "interpretation": "Assumptions are about the reliability of conclusions drawn from the model, not about making the data perfect.",
        "business_context": "Managers rely on these intervals and tests to decide on pricing, marketing, and risk policies, so invalid assumptions can lead to costly errors."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_15",
      "tags": [
        "assumptions",
        "inference",
        "p-values",
        "confidence intervals"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common MISTAKE regarding regression assumptions, especially in large samples?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Believing that normality of errors is less critical with very large samples",
        "B": "Checking residual diagnostics before trusting model inferences",
        "C": "Focusing on whether Y is normal instead of whether the errors are normal",
        "D": "Computing VIFs to detect multicollinearity"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "A common mistake is worrying about normality of Y instead of the normality of the error terms, which is what the theory requires. With large samples, mild non-normality of errors is often less serious, and checking diagnostics and VIFs are good practices, not mistakes.",
        "step_by_step": [],
        "interpretation": "It’s the pattern of residuals, not the raw Y, that matters most for regression assumptions.",
        "business_context": "Misunderstanding this can lead analysts to fix the wrong problem or to ignore serious issues in residuals that affect decisions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_15",
      "tags": [
        "assumptions",
        "common mistakes",
        "diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which assumption is specifically required for logistic regression but refers to a linear relationship on the log-odds scale rather than on the original outcome scale?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Linearity of the logit in the predictors",
        "B": "Normality of the error terms",
        "C": "Constant variance of residuals",
        "D": "Normal distribution of the predictors"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Logistic regression assumes that the logit (log-odds) of the outcome is linearly related to the predictors. It does not require normal errors or constant variance on the original probability scale, and predictors themselves do not need to be normally distributed.",
        "step_by_step": [],
        "interpretation": "The linear pattern is required after transforming probabilities to log-odds, not on the raw probability scale.",
        "business_context": "When modeling default risk, you check whether log-odds of default change roughly linearly with variables like income or credit score."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_16",
      "tags": [
        "logistic regression",
        "assumptions",
        "linearity of logit"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In a logistic regression model predicting loan default (yes/no), what does the independence of observations assumption mean?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Each predictor must be statistically independent of every other predictor.",
        "B": "Each customer’s default outcome should not be systematically linked to another customer’s outcome unless modeled.",
        "C": "The predicted probabilities must sum to 1 across all observations.",
        "D": "The sample must be drawn using simple random sampling."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Independence of observations means one person’s outcome should not depend on another’s in an unmodeled way. It does not require fully independent predictors or a particular sampling design, and probabilities do not need to sum to 1 across customers.",
        "step_by_step": [],
        "interpretation": "If customers are clustered (e.g., by branch or household), that dependence must be modeled or adjusted for.",
        "business_context": "In banking, loans from the same household may violate independence if this clustering is ignored in a default model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_16",
      "tags": [
        "logistic regression",
        "independence",
        "assumptions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes multicollinearity in the context of logistic regression for default prediction?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It occurs when the outcome is perfectly predicted by one predictor.",
        "B": "It occurs when predictors are highly correlated with each other, making coefficient estimates unstable.",
        "C": "It occurs when predicted probabilities are all close to 0.5.",
        "D": "It occurs when residuals have non-constant variance across fitted values."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Multicollinearity means predictors are strongly related to one another, which makes it hard to separate their individual effects and can destabilize coefficient estimates. It is not about prediction perfection, probability levels, or residual variance patterns.",
        "step_by_step": [],
        "interpretation": "High correlation among X-variables inflates standard errors and makes odds ratios less reliable.",
        "business_context": "In a credit model, including both highly overlapping score variables may create multicollinearity and confuse interpretation."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_16",
      "tags": [
        "logistic regression",
        "multicollinearity",
        "assumptions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which assumption of linear regression is explicitly NOT required for logistic regression when modeling a binary outcome like default (yes/no)?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Independence of observations",
        "B": "No severe multicollinearity among predictors",
        "C": "Normality of error terms on the original outcome scale",
        "D": "Linearity in a transformed scale of the outcome"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Logistic regression does not require normally distributed errors on the original outcome or probability scale. It still needs independence, control of multicollinearity, and linearity of the logit (a transformed scale).",
        "step_by_step": [],
        "interpretation": "This is why logistic regression is suitable for binary outcomes even when residuals would not be normal in a linear model.",
        "business_context": "Default models rely on logistic regression partly because the binary nature of default violates linear-model error normality."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_16",
      "tags": [
        "logistic regression",
        "assumptions",
        "comparison with linear regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are key assumptions for applying logistic regression to a rare event outcome such as loan default? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "A sufficiently large sample size to obtain stable coefficient estimates",
        "B": "Linearity of the log-odds of the outcome in the predictors",
        "C": "Normally distributed predictor variables",
        "D": "Independence of observations"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "Logistic regression relies on independence of observations, linearity in the logit, and enough data—especially when events are rare—to stabilize estimates. Predictors themselves do not need to be normally distributed.",
        "step_by_step": [],
        "interpretation": "Large samples help avoid extreme or unstable odds ratios when defaults are infrequent.",
        "business_context": "Credit risk teams ensure enough default cases and check logit-linearity before trusting a default model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_16",
      "tags": [
        "logistic regression",
        "assumptions",
        "rare events"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In choosing between two regression models for customer churn, what does higher predictive power primarily refer to?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The model’s ability to clearly show how each predictor relates to churn",
        "B": "The model’s ability to forecast churn outcomes accurately on new data",
        "C": "The number of statistically significant predictors in the model",
        "D": "The simplicity of the model’s mathematical form"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Predictive power is about how well the model forecasts outcomes for new or unseen data. Clarity of relationships, significance counts, and simplicity relate more to explanatory power or parsimony than to prediction accuracy itself.",
        "step_by_step": [],
        "interpretation": "A model with high predictive power correctly flags future churners, even if it is complex.",
        "business_context": "A telecom might favor a high-prediction model to efficiently target retention offers."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_17",
      "tags": [
        "predictive power",
        "model selection",
        "churn"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is the main feature of a regression model with high explanatory power in a business context?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It always has the highest possible accuracy on a test set.",
        "B": "It uses the largest number of predictors available.",
        "C": "It clearly shows how changes in predictors are associated with changes in the outcome.",
        "D": "It automatically satisfies all statistical assumptions by construction."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Explanatory power is about how clearly the model reveals relationships between predictors and the outcome. It does not guarantee maximum accuracy, more predictors, or automatic satisfaction of assumptions.",
        "step_by_step": [],
        "interpretation": "Such models support \"why\" questions, like how support calls affect churn odds.",
        "business_context": "Managers often prefer interpretable models to justify decisions to executives or regulators."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_17",
      "tags": [
        "explanatory power",
        "interpretability",
        "regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A bank is choosing between a complex churn model with slightly higher prediction accuracy and a simpler logistic regression with clear odds ratios. According to the concept of balancing predictive and explanatory power, when is the simpler model more appropriate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "When the main need is to understand and communicate how each factor affects churn",
        "B": "When no stakeholders need to see or approve the model",
        "C": "When the simpler model has the lowest possible prediction error",
        "D": "When predictive and explanatory power are always identical"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "When the primary goal is understanding and explaining drivers of churn, a more interpretable model is preferred, even if it is slightly less accurate. The other options either ignore stakeholders or misstate the relationship between prediction and explanation.",
        "step_by_step": [],
        "interpretation": "Explanatory models help justify actions, such as why focusing on support quality may reduce churn.",
        "business_context": "Risk committees and regulators often require transparent models they can defend."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_17",
      "tags": [
        "model selection",
        "explanatory power",
        "business decision"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a common mistake when prioritizing predictive power over explanatory power in business modeling?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using a model that stakeholders cannot understand or trust, even for small gains in accuracy",
        "B": "Checking both accuracy and interpretability before choosing a model",
        "C": "Using a simpler model when the business question is mainly about understanding drivers",
        "D": "Considering effect sizes and practical significance of coefficients"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Chasing tiny accuracy improvements with very complex, opaque models can lead to stakeholder distrust and inaction. The other options describe good practice in balancing prediction and explanation.",
        "step_by_step": [],
        "interpretation": "A \"black-box\" model may be technically strong but practically unusable if no one believes or understands it.",
        "business_context": "Executives may reject a complex churn model if they cannot see how it links customer behavior to risk."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_17",
      "tags": [
        "predictive power",
        "explanatory power",
        "common mistakes"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "When selecting a regression model for a business problem, which considerations reflect an appropriate balance between predictive and explanatory power? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Choosing a slightly less accurate model if it offers much clearer interpretation of key drivers",
        "B": "Using the most complex model available regardless of stakeholder needs",
        "C": "Aligning the model choice with whether the business goal is forecasting or understanding mechanisms",
        "D": "Assuming that the model with the highest accuracy is always the best for every purpose"
      },
      "correct_answer": [
        "A",
        "C"
      ],
      "explanation": {
        "text": "It is reasonable to sacrifice a bit of accuracy for much better interpretability and to align the model with whether you need forecasts or explanations. Automatically choosing the most complex or most accurate model ignores the business context and can be counterproductive.",
        "step_by_step": [],
        "interpretation": "Model selection is about fit to the decision problem, not just technical scores.",
        "business_context": "A marketing mix model may emphasize explanation, while a fraud detection model emphasizes prediction."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_17",
      "tags": [
        "model selection",
        "predictive power",
        "explanatory power"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is an example of using the wrong regression model for the dependent variable type in a business setting?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using linear regression to predict monthly sales revenue (a continuous variable)",
        "B": "Using logistic regression to predict whether a transaction is fraudulent (yes/no)",
        "C": "Using linear regression to predict whether a customer will click an ad (yes/no)",
        "D": "Using logistic regression to model default vs. non-default on loans"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "A yes/no outcome like ad click should typically be modeled with logistic regression, not linear regression, to get valid probabilities. The other options match model type to the nature of the outcome correctly.",
        "step_by_step": [],
        "interpretation": "Using linear regression on a binary outcome can produce predictions outside [0,1] and misinterpretation.",
        "business_context": "Click-through rate prediction normally relies on classification models such as logistic regression."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_18",
      "tags": [
        "model choice",
        "dependent variable type",
        "pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why is over-reliance on R-squared (or pseudo R-squared) considered a pitfall when comparing regression models for business use?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because R-squared directly measures the model’s profit impact",
        "B": "Because a high R-squared always indicates overfitting",
        "C": "Because R-squared alone may ignore prediction quality, calibration, and business relevance of effects",
        "D": "Because R-squared cannot be computed for any regression model"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "R-squared only captures variance explained and can miss aspects such as out-of-sample performance, probability calibration, and whether the effects are meaningful in practice. It does not directly measure profit or always imply overfitting.",
        "step_by_step": [],
        "interpretation": "A modest R-squared can still be valuable if it supports good predictions or decisions.",
        "business_context": "In noisy markets, even a model explaining 20–30% of variance can drive profitable targeting strategies."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_18",
      "tags": [
        "R-squared",
        "model comparison",
        "pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A risk analyst finds a regression coefficient with p < 0.001 but the effect corresponds to only a $1 change in average account balance. Which pitfall does this illustrate if the result is treated as highly important for strategy?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Confusing statistical significance with practical significance",
        "B": "Using the wrong model type for the outcome",
        "C": "Ignoring multicollinearity among predictors",
        "D": "Over-reliance on pseudo R-squared"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A very small effect can be statistically significant in large samples but practically unimportant. Treating it as strategically crucial confuses statistical significance with real-world impact.",
        "step_by_step": [],
        "interpretation": "\"Significant\" in statistics does not automatically mean \"big enough to matter\" in business.",
        "business_context": "Banks should focus on variables that meaningfully change risk or revenue, not just those with tiny but significant coefficients."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_18",
      "tags": [
        "statistical significance",
        "practical significance",
        "pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following best describes a consequence of ignoring model assumptions when interpreting regression results in business analytics?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Predictions and inferences may be biased or invalid, leading to misleading decisions",
        "B": "The model will always have a lower R-squared value",
        "C": "The p-values will automatically become smaller",
        "D": "The model will become easier to interpret"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "If key assumptions are violated, coefficient estimates, standard errors, and predictions can become unreliable, which can mislead decisions. This does not systematically lower R-squared, shrink p-values, or improve interpretability.",
        "step_by_step": [],
        "interpretation": "Assumptions are the conditions under which model conclusions can be trusted.",
        "business_context": "Ignoring clustered data or nonlinearity can cause a company to misjudge the effect of price or marketing spend."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_18",
      "tags": [
        "assumptions",
        "regression",
        "pitfalls"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are common pitfalls when comparing and interpreting regression models in business settings? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using linear regression to model a binary yes/no outcome",
        "B": "Evaluating models only by R-squared or pseudo R-squared",
        "C": "Checking whether effect sizes are large enough to matter in practice",
        "D": "Treating any statistically significant coefficient as automatically important for strategy"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "Using the wrong model type for the outcome, relying only on R-squared, and equating statistical with strategic importance are all highlighted pitfalls. Evaluating effect sizes for practical relevance is good practice, not a pitfall.",
        "step_by_step": [],
        "interpretation": "Good model comparison looks at appropriate model form, multiple performance metrics, and business impact.",
        "business_context": "Marketing and risk teams should avoid discarding useful models just because of modest R-squared or chasing tiny but significant effects."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_18",
      "tags": [
        "pitfalls",
        "model comparison",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In a multiple regression with several predictors, what does a high Variance Inflation Factor (VIF) for a predictor X_j primarily indicate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "X_j has a strong nonlinear relationship with the response Y",
        "B": "X_j is highly linearly predictable from the other predictors",
        "C": "The overall model has a very low R-squared",
        "D": "The residuals for X_j are approximately normally distributed"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "A high VIF means X_j can be well explained by the other predictors, indicating strong linear collinearity. It does not directly say anything about nonlinearity with Y, overall R-squared, or residual normality.",
        "step_by_step": [],
        "interpretation": "VIF measures how much the variance of a coefficient is inflated because its predictor overlaps information with other predictors.",
        "business_context": "If advertising spend in one channel has a high VIF, it likely moves closely with other marketing variables, making its separate effect hard to estimate."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_19",
      "tags": [
        "VIF",
        "collinearity",
        "multiple regression",
        "diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "How does severe collinearity between two predictors typically affect the estimated regression coefficients for those predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Coefficients become biased but very precise",
        "B": "Coefficients remain unbiased but have large standard errors",
        "C": "Coefficients become zero by construction",
        "D": "Coefficients are forced to have the same sign and magnitude"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Collinearity does not bias OLS coefficients but inflates their standard errors, making them imprecise. They are not forced to be zero or identical in sign or size.",
        "step_by_step": [],
        "interpretation": "The model can still be correct on average, but each individual coefficient is estimated with much more uncertainty.",
        "business_context": "In a bank model, balance and assets may both matter, but their coefficients will look unstable and imprecise when they move together too closely."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_19",
      "tags": [
        "collinearity",
        "standard errors",
        "OLS properties"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following formulas correctly defines the Variance Inflation Factor (VIF) for predictor X_j in a multiple regression?",
      "question_visual": {
        "type": "latex",
        "expression": "VIF_j = \\frac{1}{1 - R_j^2}"
      },
      "question_visual_type": "latex",
      "options": {
        "A": "VIF_j = 1 - R_j^2",
        "B": "VIF_j = \\dfrac{1}{1 - R_j^2}",
        "C": "VIF_j = 1 + R_j^2",
        "D": "VIF_j = R_j^2"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "The VIF for X_j is defined as 1 divided by (1 minus R_j²), where R_j² comes from regressing X_j on the other predictors. The other expressions do not match the standard definition.",
        "step_by_step": [
          {
            "step": 1,
            "title": "Recall the definition",
            "content": "To compute VIF_j, first regress X_j on all other predictors and get R_j². Then use VIF_j = 1 / (1 − R_j²).",
            "latex": "VIF_j = \\frac{1}{1 - R_j^2}",
            "diagram": null,
            "diagram_type": null
          }
        ],
        "interpretation": "As R_j² gets closer to 1 (stronger collinearity), the denominator shrinks and VIF grows large.",
        "business_context": "Analysts use this formula to detect when a predictor, such as ‘total assets’, is almost a linear combination of other financial variables."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_19",
      "tags": [
        "VIF",
        "formula recognition",
        "collinearity"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In practice, what is a common rule of thumb for flagging a predictor as having potentially problematic collinearity based on its VIF value?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "VIF greater than 1",
        "B": "VIF greater than 2",
        "C": "VIF greater than 5 or 10, depending on context",
        "D": "VIF exactly equal to 0"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Analysts often use thresholds like VIF > 5 or VIF > 10 as practical cutoffs for concerning collinearity, always considering the modeling context. Any VIF above 1 is normal and not automatically problematic.",
        "step_by_step": [],
        "interpretation": "These cutoffs are guidelines, not strict rules, but very large VIFs signal that collinearity is inflating variance.",
        "business_context": "If a real estate model shows VIF ≈ 20 for ‘total square footage’, the analyst may drop or combine that variable due to severe collinearity."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_19",
      "tags": [
        "VIF",
        "rules of thumb",
        "diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A real estate analyst finds that house size and total square footage are nearly perfectly correlated, and the VIF for house size is about 20. Which interpretation is most appropriate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The model’s predictions of house price are necessarily very poor",
        "B": "House size has a biased coefficient and must be removed",
        "C": "Collinearity is causing the house size coefficient to be unstable and imprecise",
        "D": "The regression automatically ignores one of the two variables, so VIF is irrelevant"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "A very high VIF indicates severe collinearity, which makes the coefficient for house size unstable and its standard error large. It does not automatically ruin prediction or force the software to ignore a variable.",
        "step_by_step": [],
        "interpretation": "The overall fit can still be good, but you should be cautious interpreting the separate effect of house size.",
        "business_context": "The analyst might simplify the model by keeping only one of the highly overlapping size measures to gain more stable coefficients."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_19",
      "tags": [
        "collinearity",
        "VIF",
        "interpretation",
        "real estate example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In multiple regression, what is the basic definition of collinearity (multicollinearity) among predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The response variable is independent of all predictors",
        "B": "Two or more predictors are highly linearly correlated with each other",
        "C": "Each predictor is uncorrelated with the response variable",
        "D": "There are more observations than predictors in the dataset"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Collinearity means that one predictor can be well predicted from a linear combination of others, i.e., they are highly linearly correlated. It is not about the response being independent or about sample size.",
        "step_by_step": [],
        "interpretation": "When predictors move together, the model struggles to distinguish their individual contributions.",
        "business_context": "In a marketing model, budget and number of campaigns might be collinear if they always increase and decrease together."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_20",
      "tags": [
        "collinearity",
        "definition",
        "multiple regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes how collinearity affects the overall predictive performance (R²) of a multiple regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Collinearity always makes R² very low",
        "B": "Collinearity often allows R² to be high while individual coefficients are hard to interpret",
        "C": "Collinearity forces R² to be exactly 1",
        "D": "Collinearity has no effect on coefficients or prediction"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "With collinearity, the model can still predict Y well, so R² may be high, but the individual coefficients are unstable and difficult to interpret. It does not force R² to be extreme values.",
        "step_by_step": [],
        "interpretation": "Prediction quality and interpretability of individual effects are different issues; collinearity mainly harms the latter.",
        "business_context": "A retailer’s sales model might forecast well overall but still not reveal which marketing lever is truly effective due to collinearity."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_20",
      "tags": [
        "collinearity",
        "R-squared",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following is a correct statement about the impact of collinearity on ordinary least squares (OLS) coefficient estimates?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Collinearity causes OLS estimates to be systematically biased",
        "B": "Collinearity leaves OLS estimates unbiased but increases their variance",
        "C": "Collinearity forces all coefficients to be statistically significant",
        "D": "Collinearity guarantees that each predictor’s p-value will be small"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Collinearity does not introduce bias into OLS estimates but inflates their variances, making them less precise. It often leads to larger p-values, not guaranteed significance.",
        "step_by_step": [],
        "interpretation": "The estimates still center around the true values, but with much more uncertainty.",
        "business_context": "In a gym revenue model, promotion variables may all be important, yet appear insignificant because collinearity inflates their standard errors."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_20",
      "tags": [
        "collinearity",
        "OLS",
        "variance",
        "p-values"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A retailer includes number of campaigns, marketing budget, and number of flyers as predictors. They move together every week. What is the main difficulty collinearity creates for managers using this model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "They cannot forecast sales at all",
        "B": "They cannot compute residuals from the model",
        "C": "They cannot reliably tell which specific marketing variable drives sales",
        "D": "They cannot estimate an intercept term for the model"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Collinearity makes it hard to separate the individual effect of each correlated predictor, even if the model predicts sales well. It does not prevent forecasting or computing residuals.",
        "step_by_step": [],
        "interpretation": "The model answers “Does marketing as a bundle matter?” but not “Which marketing lever should I change?”.",
        "business_context": "Managers may struggle to decide whether to adjust budget, campaigns, or flyers because the coefficients are unstable and imprecise."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_20",
      "tags": [
        "collinearity",
        "interpretation",
        "marketing example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are common misunderstandings about collinearity in multiple regression? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Believing that collinearity makes the entire model useless for prediction",
        "B": "Assuming highly correlated predictors must each be strongly significant",
        "C": "Recognizing that collinearity mainly harms interpretation of individual coefficients",
        "D": "Thinking that collinearity inflates the variances of coefficient estimates"
      },
      "correct_answer": [
        "A",
        "B"
      ],
      "explanation": {
        "text": "A and B are misunderstandings: collinearity does not automatically destroy prediction, and it can make important predictors look insignificant. C and D are correct descriptions of collinearity’s main effects, so they are not misunderstandings.",
        "step_by_step": [],
        "interpretation": "A clear mental model is: prediction can remain good, but coefficient precision and interpretability suffer.",
        "business_context": "Avoid discarding a well-predicting sales model just because marketing variables are collinear; instead, be cautious about interpreting each coefficient."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_20",
      "tags": [
        "collinearity",
        "common mistakes",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which direct effect does collinearity have on the standard errors of regression coefficients?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It reduces standard errors, making coefficients more precise",
        "B": "It leaves standard errors unchanged but changes coefficient signs",
        "C": "It inflates standard errors, making coefficients less precise",
        "D": "It forces standard errors to be exactly zero"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Collinearity inflates the standard errors of affected coefficients, decreasing precision. It does not reduce them or force them to zero.",
        "step_by_step": [],
        "interpretation": "Larger standard errors mean wider confidence intervals and more difficulty detecting true effects.",
        "business_context": "In a sales model, even important marketing variables may appear non-significant if collinearity has greatly increased their standard errors."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_21",
      "tags": [
        "standard errors",
        "collinearity effects",
        "precision"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "How does collinearity typically affect the p-values of regression coefficients for the collinear predictors?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "P-values usually become smaller, making it easier to detect significance",
        "B": "P-values usually become larger, making it harder to detect significance",
        "C": "P-values are always exactly 0.05 for collinear predictors",
        "D": "Collinearity has no relationship to p-values"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Because collinearity inflates standard errors, test statistics shrink and p-values tend to become larger, reducing power to detect true effects. It does not force a specific p-value.",
        "step_by_step": [],
        "interpretation": "Even genuinely important predictors can appear non-significant when collinearity is strong.",
        "business_context": "Managers might wrongly conclude that no marketing variable matters because each has a large p-value due to collinearity."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_21",
      "tags": [
        "p-values",
        "collinearity effects",
        "hypothesis testing"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is meant by saying that regression coefficients are \"unstable\" in the presence of strong collinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Their values remain exactly the same when predictors are added or removed",
        "B": "They change dramatically with small changes in the data or model specification",
        "C": "They always converge to zero regardless of the data",
        "D": "They are no longer computed by least squares"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Unstable coefficients are highly sensitive; small changes in the dataset or in which predictors are included can cause large shifts in their magnitude or even sign. They are still computed by least squares.",
        "step_by_step": [],
        "interpretation": "Instability signals that the model cannot uniquely assign effects to each collinear predictor.",
        "business_context": "Dropping one marketing variable might flip the sign of another, making it risky to base decisions on those individual coefficients."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_21",
      "tags": [
        "stability",
        "collinearity effects",
        "coefficients"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the retail sales example, the regression of sales on campaigns, budget, and flyers has high R² but very wide confidence intervals for each marketing coefficient. What is the most likely explanation for the wide intervals?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "There are too many observations in the dataset",
        "B": "Collinearity among the marketing variables is inflating standard errors",
        "C": "OLS always produces wide intervals regardless of data",
        "D": "The response variable has no variation"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "When predictors like campaigns, budget, and flyers move together, collinearity inflates their standard errors, which in turn widens confidence intervals. High R² alone does not prevent this.",
        "step_by_step": [],
        "interpretation": "The model predicts sales well overall, but each marketing coefficient is estimated very imprecisely.",
        "business_context": "Managers cannot confidently decide which marketing action to scale up or down because each estimated effect is too uncertain."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_21",
      "tags": [
        "confidence intervals",
        "collinearity effects",
        "example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are typical consequences of strong collinearity for inference about individual predictors? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Larger standard errors for the affected coefficients",
        "B": "Wider confidence intervals for the affected coefficients",
        "C": "Smaller p-values for the affected coefficients",
        "D": "Greater sensitivity of coefficients to small data changes"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are all standard consequences: collinearity inflates standard errors, widens confidence intervals, and makes coefficients unstable. C is incorrect because p-values typically become larger, not smaller.",
        "step_by_step": [],
        "interpretation": "Together, these effects mean you should be cautious in interpreting the size and even the sign of individual collinear coefficients.",
        "business_context": "In practice, analysts often see high VIFs, wide intervals, and coefficients that change a lot when trying slightly different models."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_21",
      "tags": [
        "standard errors",
        "confidence intervals",
        "stability",
        "collinearity effects"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In multiple regression, what does a Variance Inflation Factor (VIF) of 1 for a predictor X_j indicate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "X_j is perfectly predicted by the other predictors",
        "B": "There is no collinearity between X_j and the other predictors",
        "C": "The regression model has a very high R²",
        "D": "The coefficient of X_j must be statistically significant"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "VIF = 1 means that the variance of the coefficient for X_j is not inflated at all by collinearity, so X_j is effectively independent of the other predictors. Option A is wrong because perfect prediction by other predictors would give a very large VIF, not 1.",
        "step_by_step": [],
        "interpretation": "A VIF of 1 is the ideal case: no variance inflation from collinearity.",
        "business_context": "If all predictors had VIF ≈ 1, an analyst could interpret individual effects with more confidence."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_22",
      "tags": [
        "VIF",
        "multicollinearity detection",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "How is the Variance Inflation Factor (VIF) for a predictor X_j computed in a multiple regression model?",
      "question_visual": {
        "type": "latex",
        "plot_type": "formula",
        "params": {
          "expression": "VIF_j = \\frac{1}{1 - R_j^2}"
        }
      },
      "question_visual_type": "latex",
      "options": {
        "A": "By squaring the correlation between X_j and Y",
        "B": "As 1 divided by (1 minus the R² from regressing X_j on the other predictors)",
        "C": "As the square root of the model’s overall R²",
        "D": "As the ratio of the residual sum of squares to the total sum of squares"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "VIF_j is defined as 1 / (1 − R_j²), where R_j² comes from regressing X_j on all the other predictors. Option A confuses this with simple correlation with the response Y, which does not define VIF.",
        "step_by_step": [
          {
            "step": 1,
            "title": "Auxiliary regression",
            "content": "First regress X_j on all the other predictors and obtain the R_j² from this auxiliary regression.",
            "latex": "X_j = \\gamma_0 + \\gamma_1 X_1 + \\dots + \\gamma_{j-1} X_{j-1} + \\gamma_{j+1} X_{j+1} + \\dots + e",
            "diagram": null,
            "diagram_type": "None"
          },
          {
            "step": 2,
            "title": "Compute VIF",
            "content": "Plug R_j² into VIF_j = 1 / (1 − R_j²). A larger R_j² gives a larger VIF.",
            "latex": "VIF_j = \\frac{1}{1 - R_j^2}",
            "diagram": null,
            "diagram_type": "None"
          }
        ],
        "interpretation": "The more X_j can be explained by the other predictors, the larger its VIF.",
        "business_context": "Analysts use this computation to flag predictors whose estimated effects are unstable due to redundancy with other variables."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_22",
      "tags": [
        "VIF",
        "R-squared",
        "diagnostics",
        "formula recognition"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst regresses TV Ad Spend (X1) on Online Ad Spend and In‑store Promotion and gets R₁² = 0.80 from this auxiliary regression. What is the approximate VIF for X1?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "1.25",
        "B": "2.0",
        "C": "5.0",
        "D": "10.0"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "Using VIF₁ = 1 / (1 − R₁²) = 1 / (1 − 0.80) = 1 / 0.20 = 5, so option C is correct. Options B and D are numerically inconsistent with this formula.",
        "step_by_step": [
          {
            "step": 1,
            "title": "Apply the VIF formula",
            "content": "Compute 1 − R₁² = 1 − 0.80 = 0.20.",
            "latex": "1 - R_1^2 = 1 - 0.80 = 0.20",
            "diagram": null,
            "diagram_type": "None"
          },
          {
            "step": 2,
            "title": "Invert to get VIF",
            "content": "VIF₁ = 1 / 0.20 = 5.",
            "latex": "VIF_1 = \\frac{1}{0.20} = 5",
            "diagram": null,
            "diagram_type": "None"
          }
        ],
        "interpretation": "A VIF of 5 is often viewed as a warning sign for notable collinearity.",
        "business_context": "Such a VIF would suggest that the TV Ad Spend effect is estimated with inflated uncertainty due to similarity with other ad measures."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_22",
      "tags": [
        "VIF",
        "calculation",
        "multicollinearity detection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What does a large VIF value (for example, greater than about 5) typically signal about a predictor’s coefficient in a business regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Its estimate has low variance and is very stable",
        "B": "Its estimate is heavily influenced by collinearity with other predictors",
        "C": "The predictor has no relationship with the response",
        "D": "The overall model fit (R²) must be poor"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "A large VIF means the variance of the coefficient is inflated because the predictor is highly collinear with other predictors. Option A is the opposite of what a high VIF implies; it indicates instability, not stability.",
        "step_by_step": [],
        "interpretation": "High VIF warns that the separate effect of that predictor is hard to pin down.",
        "business_context": "Managers should be cautious about making decisions based solely on such a coefficient (e.g., reallocating spend between highly collinear channels)."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_22",
      "tags": [
        "VIF",
        "interpretation",
        "diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following statements about Variance Inflation Factor (VIF) are correct? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "VIF measures how much the variance of a coefficient is increased by collinearity",
        "B": "VIF_j uses the R² from regressing X_j on the other predictors",
        "C": "A VIF of 1 indicates severe multicollinearity",
        "D": "VIF thresholds like 5 or 10 should always be treated as rigid cutoffs, regardless of context"
      },
      "correct_answer": [
        "A",
        "B"
      ],
      "explanation": {
        "text": "A and B are correct: VIF is defined via the R² from an auxiliary regression of X_j on the other predictors and reflects the inflation in variance of its coefficient. C is wrong because VIF = 1 means no collinearity, and D is wrong because thresholds are guidelines, not absolute rules.",
        "step_by_step": [],
        "interpretation": "VIF is both a formula-based and interpretation-based tool, but must be used with judgment.",
        "business_context": "Analysts combine VIF values with domain knowledge to decide whether collinearity is problematic for their specific decision task."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_22",
      "tags": [
        "VIF",
        "definition",
        "common mistakes",
        "multicollinearity detection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why can a multiple regression model with strong collinearity among predictors still have a high R²?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because collinearity always reduces model fit",
        "B": "Because the combined linear effect of the correlated predictors can still track the response very well",
        "C": "Because R² ignores the predictors and depends only on the response",
        "D": "Because each individual coefficient is estimated with very high precision"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Even when predictors are collinear, their linear combination Xβ can follow Y closely, giving a high R². Option A is false: collinearity affects coefficient precision, not necessarily overall fit.",
        "step_by_step": [],
        "interpretation": "Prediction quality (R²) reflects how well the whole set of predictors works together, not how well each one is separated from the others.",
        "business_context": "A retailer may get accurate sales forecasts from a model with collinear marketing variables, even if it cannot isolate each channel’s effect."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_23",
      "tags": [
        "R-squared",
        "collinearity",
        "prediction vs interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the presence of high collinearity, what typically happens to the estimated coefficients of individual predictors in a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "They become more precise and stable across model specifications",
        "B": "They become imprecise and can change a lot when the model is modified",
        "C": "They always become exactly zero",
        "D": "They no longer contribute to prediction in any way"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "High collinearity makes individual coefficients unstable: they have large standard errors and can change markedly if you add or remove predictors. Option A is the opposite of what happens; precision decreases rather than increases.",
        "step_by_step": [],
        "interpretation": "Collinearity undermines the reliability of interpreting single coefficients, even when overall prediction is good.",
        "business_context": "This is why executives may see coefficient signs or sizes flip when analysts tweak a collinear marketing model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_23",
      "tags": [
        "collinearity",
        "coefficient stability",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A retailer’s regression model for weekly sales has R² = 0.93, but two marketing predictors are highly collinear with unstable coefficients. For which purpose is this model more appropriate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Forecasting total weekly sales under similar marketing conditions",
        "B": "Determining the exact causal effect of each marketing channel separately",
        "C": "Testing whether the response variable is normally distributed",
        "D": "Estimating the variance of the random error term only"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "With high R² and collinear predictors, the model is well-suited for predicting total sales but not for isolating each channel’s causal effect. Option B requires precise, stable coefficients, which collinearity undermines.",
        "step_by_step": [],
        "interpretation": "Use such a model for prediction, not for fine-grained policy recommendations about individual predictors.",
        "business_context": "Managers could use it to plan inventory for expected sales levels, but not to decide which single channel to cut or expand based on coefficient sizes."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_23",
      "tags": [
        "forecasting",
        "prediction vs interpretation",
        "business decisions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which situation best illustrates that high collinearity can separate predictive power from interpretive power in a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Low R² and all coefficients are statistically insignificant",
        "B": "High R², small prediction errors, but coefficients for correlated predictors have wide confidence intervals",
        "C": "High R² and all predictors are uncorrelated with each other",
        "D": "Low R² but all coefficients are very stable across models"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "High R² with small prediction errors shows strong predictive power, while wide confidence intervals for collinear predictors indicate weak interpretive power for individual effects. Option C describes a case without collinearity, where interpretation would generally be easier.",
        "step_by_step": [],
        "interpretation": "Collinearity can hide inside models that predict well, undermining coefficient interpretation rather than overall fit.",
        "business_context": "Executives may trust the forecasts but should be cautious about reading too much into any single coefficient in such models."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_23",
      "tags": [
        "collinearity",
        "R-squared",
        "confidence intervals"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "In a regression model with high collinearity but high R², which of the following statements are typically true? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The model can still produce good forecasts if future conditions are similar",
        "B": "Individual coefficients for collinear predictors may be unreliable for policy decisions",
        "C": "High R² guarantees that each predictor’s causal effect is accurately estimated",
        "D": "The model is better suited to prediction than to explaining which specific predictor is most effective"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are correct: collinear models can predict well but have unstable coefficients, so they are more reliable for forecasting than for attributing effects to specific predictors. C is incorrect because high R² does not ensure accurate causal estimates of individual predictors under collinearity.",
        "step_by_step": [],
        "interpretation": "Prediction and explanation are different tasks, and collinearity mainly harms the latter.",
        "business_context": "This distinction guides whether a firm should use such a model to forecast sales or to decide which marketing lever to adjust."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_23",
      "tags": [
        "prediction vs interpretation",
        "collinearity",
        "business decisions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which strategy for dealing with high collinearity directly follows the approach used in the RetailX example with TV and Online Ad Spend?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Dropping both TV and Online Ad Spend from the model",
        "B": "Combining TV and Online Ad Spend into a single Total Ad Spend variable",
        "C": "Adding more polynomial terms of TV and Online Ad Spend",
        "D": "Ignoring collinearity as long as p-values are small"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "In the RetailX example, the analyst reduces collinearity by creating a composite Total Ad Spend variable from TV and Online Ad Spend. Option A throws away information, while C often worsens collinearity and D is poor practice.",
        "step_by_step": [],
        "interpretation": "Combining highly collinear variables can both reduce VIFs and align with a business view of a single overall lever.",
        "business_context": "Managers typically think in terms of total advertising budget, matching the composite variable strategy."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_24",
      "tags": [
        "collinearity remedies",
        "composite variables",
        "RetailX example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "When might simply removing one of two highly correlated predictors be an appropriate way to handle collinearity in a regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "When both predictors measure essentially the same thing and one is not important for business decisions",
        "B": "When the goal is causal interpretation and both predictors are theoretically essential",
        "C": "When VIFs are low (around 1–2) for all predictors",
        "D": "When pairwise correlations are all exactly zero"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "If two variables are redundant and one is not crucial for decision making, dropping it can reduce collinearity with little loss of information. Option B describes a situation where you generally would not want to drop a theoretically important variable.",
        "step_by_step": [],
        "interpretation": "Variable removal is a pragmatic fix when redundancy is high and one variable is expendable.",
        "business_context": "For example, if two internal metrics both track almost the same customer visits but only one is used in reporting, the other may be safely removed."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_24",
      "tags": [
        "collinearity remedies",
        "variable selection",
        "business relevance"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In models that include polynomial or interaction terms (e.g., X and X²), why is centering a predictor (subtracting its mean) often recommended as a collinearity remedy?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It eliminates all collinearity among distinct predictors",
        "B": "It reduces non-essential collinearity created by the polynomial or interaction structure",
        "C": "It guarantees that all VIFs will be exactly 1",
        "D": "It increases the correlation between X and X² to stabilize estimates"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Centering mainly helps with the artificial collinearity that arises when you include powers or interactions of the same variable. Option A is incorrect because centering does not fix fundamental collinearity between different predictors.",
        "step_by_step": [],
        "interpretation": "Centering is a technical adjustment for model structure, not a cure-all for all forms of collinearity.",
        "business_context": "Analysts using squared terms (e.g., advertising spend and its square) often center spend first to make coefficients easier to interpret and more stable."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_24",
      "tags": [
        "centering",
        "polynomial terms",
        "collinearity remedies"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following best describes when using methods like Ridge or Lasso regression is an appropriate strategy for dealing with collinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "When you must keep many correlated predictors for prediction purposes",
        "B": "When you want to drop all predictors with any correlation above 0.2",
        "C": "When the primary goal is exact causal interpretation of each coefficient",
        "D": "When there is only a single predictor in the model"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Ridge and Lasso are regularization methods that stabilize coefficients when you keep many correlated predictors, prioritizing prediction. Option C is misleading because regularization shrinks coefficients and is not ideal for exact causal interpretation.",
        "step_by_step": [],
        "interpretation": "Regularization trades some bias for lower variance, producing more stable predictions in collinear settings.",
        "business_context": "This is common in high-dimensional marketing or customer analytics where many overlapping metrics must be used to predict outcomes."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_24",
      "tags": [
        "ridge regression",
        "lasso",
        "collinearity remedies",
        "prediction"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are sensible strategies for handling high collinearity in a multiple regression model? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Combine two highly collinear predictors into a composite index that matches the business concept",
        "B": "Collect more data to increase independent variation among predictors",
        "C": "Mechanically drop all variables with high VIF, without considering business relevance",
        "D": "Use Ridge or Lasso regression when many correlated predictors must be retained for prediction"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are recommended approaches: combining variables, gathering more data, or using regularization can all mitigate collinearity issues. C is a common mistake because it ignores the business importance of variables and can discard valuable information.",
        "step_by_step": [],
        "interpretation": "Effective collinearity remedies balance statistical diagnostics with practical business needs.",
        "business_context": "For example, a retailer might combine overlapping marketing measures, run more experiments, or use Ridge regression to keep many related predictors while improving prediction stability."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_24",
      "tags": [
        "collinearity remedies",
        "composite variables",
        "data collection",
        "ridge regression",
        "common mistakes"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the RetailX case, which statistical symptom most clearly suggested that TV Ad Spend and Online Ad Spend were affected by collinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Both predictors had high VIF values but non‑significant p‑values despite a high overall R²",
        "B": "Both predictors had low VIF values and highly significant p‑values",
        "C": "The overall R² of the model was very low",
        "D": "Sales were uncorrelated with any advertising variables"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "TV and Online Ad Spend showed high VIFs and non‑significant p‑values even though the model’s R² was high, a classic sign of multicollinearity. Option B describes a well‑behaved model, not a collinearity problem.",
        "step_by_step": [],
        "interpretation": "Collinearity can hide the individual effects of predictors even when the model seems to fit well overall.",
        "business_context": "Managers might falsely think advertising has no effect if they only look at individual p‑values without checking VIFs."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_25",
      "tags": [
        "RetailX example",
        "VIF",
        "p-values",
        "multicollinearity detection"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "How did RetailX mainly mitigate the collinearity between TV Ad Spend and Online Ad Spend in their regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "They dropped both TV and Online Ad Spend from the model",
        "B": "They replaced TV Ad Spend with a random noise variable",
        "C": "They combined TV and Online Ad Spend into a single Total Ad Spend predictor",
        "D": "They increased the sample size without changing the predictors"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "RetailX summed TV and Online Ad Spend into a Total Ad Spend variable, reducing collinearity and clarifying the overall ad effect. Simply dropping variables (A) or adding noise (B) does not address the underlying business question.",
        "step_by_step": [],
        "interpretation": "Combining highly related predictors into a meaningful aggregate can both stabilize estimates and match how managers think about budgets.",
        "business_context": "Total Ad Spend aligns with how executives typically set overall advertising budgets."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_25",
      "tags": [
        "feature engineering",
        "total ad spend",
        "collinearity mitigation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "After RetailX created the Total Ad Spend variable, what happened to its VIF and coefficient in the revised model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "VIF stayed high and the coefficient remained non‑significant",
        "B": "VIF dropped to around 1.2 and the coefficient became statistically significant",
        "C": "VIF increased to above 10 but the coefficient became non‑significant",
        "D": "VIF became undefined and the model could not be estimated"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "The Total Ad Spend variable had a low VIF (about 1.2) and a significant, stable coefficient, indicating reduced collinearity and clearer interpretation. Option A would mean the transformation failed to fix the problem, which is opposite of what happened.",
        "step_by_step": [],
        "interpretation": "Low VIF and significant coefficients suggest more reliable estimates for decision‑making.",
        "business_context": "This allowed RetailX to quantify how much extra sales come from an additional million in total advertising."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_25",
      "tags": [
        "VIF reduction",
        "coefficient significance",
        "model diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "From a business perspective, what key decision did the improved RetailX model help support after addressing collinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Whether to shut down all advertising activities",
        "B": "How changes in total advertising budget affect sales",
        "C": "The exact daily schedule of TV commercials",
        "D": "The optimal price for each individual product SKU"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "The revised model quantified the impact of Total Ad Spend on sales, helping managers decide on the overall advertising budget. It did not provide detailed scheduling (C) or pricing (D) decisions.",
        "step_by_step": [],
        "interpretation": "Regression results are most useful when they answer questions managers actually face, like budget levels.",
        "business_context": "RetailX could more confidently decide whether to increase or decrease total ad spending next quarter."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_25",
      "tags": [
        "business decision-making",
        "budget allocation",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the RetailX example, which analyst action would have been a common but potentially harmful mistake when facing collinearity between TV and Online Ad Spend?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Dropping one ad variable purely because its p‑value was non‑significant",
        "B": "Checking VIF values before changing the model",
        "C": "Creating a combined Total Ad Spend measure",
        "D": "Discussing how management actually budgets for advertising"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "Dropping a predictor only because its p‑value is non‑significant can discard useful information when collinearity is present. Options B, C, and D are good practices for diagnosing and addressing collinearity.",
        "step_by_step": [],
        "interpretation": "Collinearity can hide real effects, so p‑values alone are not a safe basis for dropping variables.",
        "business_context": "Misinterpreting p‑values could lead RetailX to underinvest in an effective advertising channel."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_25",
      "tags": [
        "common mistakes",
        "variable selection",
        "p-values",
        "collinearity"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement best describes multicollinearity in a multiple regression model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It is a strong linear relationship between a predictor and the response variable",
        "B": "It is a situation where predictors are highly linearly related to each other",
        "C": "It occurs only when there is exactly one predictor in the model",
        "D": "It refers to having many observations in the dataset"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Multicollinearity is about strong linear relationships among predictors, not between predictors and the response. Option A confuses predictor–response correlation with collinearity among predictors.",
        "step_by_step": [],
        "interpretation": "The problem is that predictors move together, making it hard to separate their individual effects.",
        "business_context": "For example, price discounts and advertising may move together in campaigns, causing collinearity."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_26",
      "tags": [
        "multicollinearity definition",
        "multiple regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why does high multicollinearity make regression coefficients harder to use for business decisions, even when R² is high?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because it always lowers the R² of the model",
        "B": "Because it makes coefficient estimates unstable and their standard errors large",
        "C": "Because it forces all coefficients to be exactly zero",
        "D": "Because it prevents the model from making any predictions"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "High multicollinearity inflates the variances of coefficient estimates, making them unstable and hard to interpret, even if R² is high. It does not necessarily reduce R² (A) or stop prediction (D).",
        "step_by_step": [],
        "interpretation": "Unstable coefficients can change sign or size with small data changes, undermining trust in specific effects.",
        "business_context": "A telecom firm might see the estimated effect of discounts on revenue swing wildly from one sample to another."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_26",
      "tags": [
        "multicollinearity impact",
        "standard errors",
        "business interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the telecom revenue example, what sign indicated that multicollinearity was a problem for interpreting the discount percentage effect?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The discount coefficient remained stable across different samples",
        "B": "The discount coefficient’s sign and size changed a lot when a few months of data were added or removed",
        "C": "The model’s R² stayed near zero regardless of the predictors",
        "D": "The number of predictors in the model decreased over time"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Large swings in the discount coefficient’s sign and magnitude with small data changes are typical of multicollinearity. Stability (A) would suggest the opposite.",
        "step_by_step": [],
        "interpretation": "If coefficients are that sensitive, they are not reliable for planning pricing strategy.",
        "business_context": "Managers could make poor pricing decisions if they trust such unstable estimates."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_26",
      "tags": [
        "coefficient stability",
        "diagnostics",
        "telecom example"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following statements confuses multicollinearity with a different concept?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "\"Multicollinearity refers to strong relationships among predictor variables.\"",
        "B": "\"High R² guarantees that all individual regression coefficients are reliable.\"",
        "C": "\"Multicollinearity can inflate the variance of coefficient estimates.\"",
        "D": "\"Predictors that move together can make it hard to separate their individual effects.\""
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "High R² does not guarantee reliable individual coefficients when multicollinearity is present; they can still be unstable. Options A, C, and D correctly describe aspects of multicollinearity.",
        "step_by_step": [],
        "interpretation": "R² measures overall fit, not the trustworthiness of each separate coefficient.",
        "business_context": "A marketing model might look good overall but still give misleading channel‑specific effects."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_26",
      "tags": [
        "common mistakes",
        "R-squared",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are typical business consequences of high multicollinearity in a regression model? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Coefficient estimates for related predictors become unstable across samples",
        "B": "Managers lose confidence in using specific coefficients for pricing or budget decisions",
        "C": "The regression model can no longer compute any predictions at all",
        "D": "Hypothesis tests for individual predictors become less reliable"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are correct because multicollinearity makes coefficients unstable, undermines manager confidence, and weakens individual hypothesis tests. C is incorrect; models with multicollinearity can usually still make predictions, though interpretation is harder.",
        "step_by_step": [],
        "interpretation": "The main issue is not prediction, but how much you can trust each predictor’s estimated effect.",
        "business_context": "Unreliable coefficients can misguide decisions on discount levels or marketing spend allocation."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_26",
      "tags": [
        "multicollinearity consequences",
        "business decision-making",
        "MCA"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which condition about predictors is required by the OLS regression assumption on (multi)collinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "There must be no linear relationship at all among any predictors",
        "B": "There must be no exact linear relationship that makes one predictor a perfect combination of others",
        "C": "All predictors must be completely uncorrelated with the response",
        "D": "Each predictor must be normally distributed"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "OLS requires no perfect multicollinearity: no predictor can be an exact linear combination of others. Some correlation among predictors (A) is allowed, and normality of predictors (D) is not the collinearity assumption.",
        "step_by_step": [],
        "interpretation": "Perfect linear dependence makes the normal equations unsolvable for unique coefficients.",
        "business_context": "If Total Ads = TV + Online exactly, including all three as separate predictors violates this assumption."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_27",
      "tags": [
        "OLS assumptions",
        "no perfect multicollinearity"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is a likely technical problem when a model includes a predictor that is an exact linear combination of other predictors (perfect multicollinearity)?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The software may fail to estimate the model or automatically drop one predictor",
        "B": "The model’s R² is forced to be exactly zero",
        "C": "All estimated coefficients must be equal to 1",
        "D": "The residuals become perfectly normally distributed"
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "With perfect multicollinearity, the design matrix is singular, so software may refuse to fit the model or drop one of the collinear variables. R² and residual normality are not forced into the patterns described in B and D.",
        "step_by_step": [],
        "interpretation": "The math cannot uniquely assign effects when one predictor is an exact combination of others.",
        "business_context": "In a marketing mix model, including both total spend and the exact sum of channel spends can trigger this issue."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_27",
      "tags": [
        "perfect multicollinearity",
        "software behavior",
        "design matrix"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "How does high but imperfect multicollinearity (e.g., VIF > 10) typically affect coefficient estimates in OLS regression?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "It makes coefficient estimates very precise with tiny standard errors",
        "B": "It leaves coefficient precision unchanged but alters their signs",
        "C": "It makes coefficient estimates imprecise, with large standard errors",
        "D": "It forces the model to have a low R² value"
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "High multicollinearity inflates the variances of coefficient estimates, leading to large standard errors and imprecise estimates. It does not necessarily reduce R² (D) and certainly does not make estimates more precise (A).",
        "step_by_step": [],
        "interpretation": "Imprecise estimates make it harder to say whether a predictor truly matters.",
        "business_context": "Managers may hesitate to adjust spending if the estimated effect of a channel is too uncertain."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_27",
      "tags": [
        "high multicollinearity",
        "VIF",
        "precision of estimates"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the marketing mix example, why is including both 'total advertising spend' and the sum of 'TV + online + print spend' as separate predictors problematic?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because both variables are categorical, not numeric",
        "B": "Because one variable is nearly an exact linear combination of the others",
        "C": "Because OLS cannot handle more than two predictors",
        "D": "Because advertising variables must always be standardized"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "Total advertising is essentially the sum of its components, so including both creates near‑perfect multicollinearity. OLS can handle many predictors (C); the issue is the linear dependence, not the number of variables.",
        "step_by_step": [],
        "interpretation": "When one predictor is (almost) redundant information, the model cannot separate their effects cleanly.",
        "business_context": "This can cause software warnings and unreliable estimates of channel‑specific effects."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_27",
      "tags": [
        "near-perfect multicollinearity",
        "marketing mix",
        "model specification"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement about the OLS assumption on multicollinearity reflects a common misunderstanding?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "\"The assumption only rules out perfect multicollinearity, but high multicollinearity can still harm inference.\"",
        "B": "\"As long as the software returns coefficients, multicollinearity is not a concern.\"",
        "C": "\"High VIF values can be used as a warning sign of problematic multicollinearity.\"",
        "D": "\"Some correlation among predictors is common and does not automatically violate OLS assumptions.\""
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is a misunderstanding; software can return coefficients even when high multicollinearity makes them very unreliable. A, C, and D correctly describe nuances of the multicollinearity assumption.",
        "step_by_step": [],
        "interpretation": "Model diagnostics must go beyond \"the software ran\" to assess whether estimates are trustworthy.",
        "business_context": "Relying only on software output could lead to overconfident but misleading marketing or pricing decisions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_27",
      "tags": [
        "common mistakes",
        "OLS assumptions",
        "diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the context of multiple regression, which statement best describes a common misconception about collinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "High collinearity means predictors are strongly correlated with each other, not necessarily with the response.",
        "B": "High collinearity can make individual coefficient estimates unstable even if overall prediction is good.",
        "C": "A high R² automatically means that collinearity is not a problem in the model.",
        "D": "Collinearity refers to having too few predictors in the regression model."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because a high R² measures how well predictors jointly explain the response, not whether they are highly correlated with each other. A and B correctly describe aspects of collinearity, while D is unrelated to the definition of collinearity.",
        "step_by_step": [],
        "interpretation": "Good overall fit (high R²) does not guarantee that individual coefficients are reliable when predictors are collinear.",
        "business_context": "In business analytics, relying only on R² may hide problems when you need to interpret separate effects, such as price vs. advertising impact."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_28",
      "tags": [
        "collinearity",
        "R-squared",
        "misconceptions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What is a potential consequence of dropping a predictor only because it has a high VIF, without considering theory or business importance?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The model will always have a lower R².",
        "B": "The model may suffer from omitted variable bias.",
        "C": "The remaining predictors will become uncorrelated with each other.",
        "D": "The error variance will automatically decrease to zero."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because removing a theoretically important predictor can omit a relevant variable, biasing the coefficients of the remaining predictors. A is not guaranteed, C is false because collinearity among remaining predictors may persist, and D is impossible in practice.",
        "step_by_step": [],
        "interpretation": "High VIF alone is not a sufficient reason to remove a meaningful variable; doing so can distort your inferences.",
        "business_context": "For example, dropping ‘support calls’ from a churn model just because it has high VIF can cause you to understate the effect of poor service on churn."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_28",
      "tags": [
        "VIF",
        "omitted variable bias",
        "model specification"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which statement correctly distinguishes collinearity from predictor–response correlation?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Collinearity is about correlation among predictors, while predictor–response correlation is about how a predictor relates to Y.",
        "B": "Collinearity is about how strongly each predictor is correlated with Y.",
        "C": "Predictor–response correlation is irrelevant once collinearity is present.",
        "D": "Collinearity only occurs when predictors have zero correlation with Y."
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because collinearity refers to relationships among the X’s, whereas predictor–response correlation describes each X’s relationship with Y. B confuses the two concepts, and C and D are incorrect statements about collinearity.",
        "step_by_step": [],
        "interpretation": "You can have strong predictor–response correlations and still have severe collinearity among predictors.",
        "business_context": "In a marketing model, advertising spend and price discount may both relate to sales (Y) and also be highly related to each other (collinearity)."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_28",
      "tags": [
        "collinearity",
        "correlation",
        "definitions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why can a regression model with severe collinearity still show a very high R²?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because collinearity always reduces the model’s ability to fit the data.",
        "B": "Because R² measures overall fit, not the stability of individual coefficient estimates.",
        "C": "Because R² automatically adjusts downward when VIF values are large.",
        "D": "Because high R² implies that predictors are uncorrelated with each other."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because R² captures how well the predictors jointly explain Y, even if their individual effects are hard to separate. A, C, and D misstate the relationship between R² and collinearity.",
        "step_by_step": [],
        "interpretation": "High R² can coexist with unstable, hard-to-interpret coefficients in the presence of collinearity.",
        "business_context": "A churn model might predict churn well but still give unreliable estimates for how much each customer behavior contributes separately."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_28",
      "tags": [
        "R-squared",
        "collinearity",
        "interpretation"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are common misconceptions about collinearity in regression models? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Believing that a high R² means there is no collinearity problem.",
        "B": "Treating a high VIF as proof that a variable is unimportant.",
        "C": "Thinking that collinearity is only a concern when we care about interpreting individual effects.",
        "D": "Assuming that good prediction automatically means coefficients are reliable for decision-making."
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are correct because they reflect misunderstandings: R² does not diagnose collinearity, high VIF signals estimation difficulty not unimportance, and good prediction does not guarantee stable coefficients. C is actually true: collinearity is most problematic when we want to interpret separate effects, so it is not a misconception.",
        "step_by_step": [],
        "interpretation": "Misreading R² and VIF can lead analysts to ignore serious collinearity or to drop important predictors.",
        "business_context": "These misconceptions can cause flawed policy or pricing decisions when managers rely on misinterpreted regression results."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_28",
      "tags": [
        "collinearity",
        "VIF",
        "misconceptions",
        "MCA"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What does the Variance Inflation Factor (VIF) for a predictor X_j directly measure?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "How strongly X_j is correlated with the response variable Y.",
        "B": "How much the variance of the coefficient of X_j is increased due to multicollinearity.",
        "C": "The probability that X_j is an unimportant predictor.",
        "D": "The change in R² when X_j is removed from the model."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because VIF quantifies the inflation of the variance of the estimated coefficient of X_j caused by its correlation with other predictors. A confuses predictor–response correlation with collinearity, and C and D are not what VIF measures.",
        "step_by_step": [],
        "interpretation": "Higher VIF means less precise, more unstable estimates for that predictor’s coefficient.",
        "business_context": "In a spending model, a high VIF for ‘website visits’ warns that its estimated effect is noisy because it overlaps with other engagement measures."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_29",
      "tags": [
        "VIF",
        "multicollinearity",
        "definitions"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Suppose you regress predictor X_j on all the other predictors and obtain R_j² = 0.80. What is the VIF for X_j (approximately) and how would you describe the level of multicollinearity?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "VIF ≈ 1.25; no collinearity.",
        "B": "VIF ≈ 2; low collinearity.",
        "C": "VIF ≈ 5; moderate to serious collinearity.",
        "D": "VIF ≈ 10; extremely high collinearity."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because VIF = 1 / (1 − 0.80) = 1 / 0.20 = 5, which is typically viewed as indicating potentially serious multicollinearity. The other numerical values do not follow from the formula.",
        "step_by_step": [
          {
            "step": 1,
            "title": "Apply the VIF formula",
            "content": "Use VIF_j = 1 / (1 − R_j²). With R_j² = 0.80, VIF_j = 1 / (1 − 0.80) = 1 / 0.20 = 5.",
            "latex": "VIF_j = \\frac{1}{1 - R_j^2} = \\frac{1}{1 - 0.80} = \\frac{1}{0.20} = 5",
            "diagram": null,
            "diagram_type": "None"
          }
        ],
        "interpretation": "A VIF around 5 suggests that the variance of the coefficient is inflated fivefold by collinearity, a noteworthy issue.",
        "business_context": "An analyst might reconsider how to model overlapping variables, such as combining similar engagement metrics into a single index."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_29",
      "tags": [
        "VIF",
        "calculation",
        "R-squared"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "What does VIF = 1 for a predictor X_j indicate about that predictor in the model?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "X_j has no relationship with the response variable.",
        "B": "X_j is perfectly predicted by the other predictors.",
        "C": "There is no linear collinearity between X_j and the other predictors.",
        "D": "The coefficient of X_j must be exactly zero."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because VIF = 1 means R_j² = 0, so X_j cannot be linearly predicted from the other predictors and there is no collinearity. A and D refer to the relationship with the response, and B would correspond to an infinite VIF, not 1.",
        "step_by_step": [],
        "interpretation": "When VIF is 1, the variance of the coefficient is not inflated by collinearity with other predictors.",
        "business_context": "If ‘email clicks’ has VIF = 1, its effect is estimated without interference from other marketing variables in the model."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_29",
      "tags": [
        "VIF",
        "interpretation",
        "multicollinearity"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "An analyst computes VIF = 9 for a predictor. Which interpretation is most appropriate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The predictor is unimportant and should be removed from the model.",
        "B": "The variance of the predictor’s coefficient estimate is inflated ninefold due to collinearity.",
        "C": "The predictor has a very weak relationship with the response.",
        "D": "There is no evidence of collinearity involving this predictor."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because a VIF of 9 indicates that the variance of the coefficient estimate is multiplied by 9 relative to the no-collinearity case. A and C confuse estimation difficulty with substantive importance, and D contradicts the high VIF value.",
        "step_by_step": [],
        "interpretation": "High VIF warns about unstable coefficient estimates, not about whether the predictor is valuable conceptually.",
        "business_context": "A high VIF for ‘app sessions’ might mean its effect overlaps with ‘minutes watched’, not that app sessions are irrelevant to spending."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_29",
      "tags": [
        "VIF",
        "interpretation",
        "common mistakes"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following correctly describe properties or interpretation of VIF? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "VIF is computed using R_j² from regressing X_j on all other predictors.",
        "B": "VIF = 1 indicates no linear collinearity between X_j and the other predictors.",
        "C": "High VIF means the unique effect of X_j is hard to estimate precisely.",
        "D": "High VIF proves that X_j has no relationship with the response variable."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": {
        "text": "A, B, and C are correct: VIF_j uses R_j² from the auxiliary regression, equals 1 when there is no collinearity, and large values signal inflated variance for the coefficient. D is wrong because VIF is about X–X correlation, not about whether X_j is related to Y.",
        "step_by_step": [],
        "interpretation": "VIF is a diagnostic for multicollinearity, not a direct test of substantive importance or predictor–response strength.",
        "business_context": "Analysts should treat high-VIF variables carefully, possibly re-specifying the model rather than automatically dropping them."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_29",
      "tags": [
        "VIF",
        "auxiliary regression",
        "interpretation",
        "MCA"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "To compute the VIF for a predictor X_j, which regression must you run to obtain the required R_j²?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Regress the response Y on X_j alone.",
        "B": "Regress Y on all predictors except X_j.",
        "C": "Regress X_j on all the other predictors in the model.",
        "D": "Regress the residuals of Y on the residuals of X_j."
      },
      "correct_answer": [
        "C"
      ],
      "explanation": {
        "text": "C is correct because the auxiliary regression for VIF uses X_j as the dependent variable and all other predictors as regressors. A and B involve Y rather than X_j, and D is not the standard VIF procedure.",
        "step_by_step": [],
        "interpretation": "The key idea is to see how well X_j can be predicted from the other X’s; that goodness of fit is captured by R_j².",
        "business_context": "For ‘online ad spend’, you regress it on TV and radio spend to evaluate how redundant it is with other channels."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_30",
      "tags": [
        "VIF computation",
        "auxiliary regression",
        "process"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A marketing analyst wants the VIF for ‘online_ad_spend’ in a sales model with predictors online_ad_spend, TV_ad_spend, and radio_ad_spend. Which is the correct formula for the VIF once she has obtained R² = 0.75 from the appropriate auxiliary regression?",
      "question_visual": {
        "type": "latex",
        "plot_type": "formula",
        "params": {
          "expression": "VIF_{online} = \\frac{1}{1 - R_{online}^2}"
        }
      },
      "question_visual_type": "latex",
      "options": {
        "A": "VIF_online = 1 − R² = 1 − 0.75",
        "B": "VIF_online = 1 / (1 − R²) = 1 / (1 − 0.75)",
        "C": "VIF_online = R² / (1 − R²) = 0.75 / 0.25",
        "D": "VIF_online = 1 / R² = 1 / 0.75"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because the VIF formula is VIF_j = 1 / (1 − R_j²). Options A, C, and D use different, incorrect transformations of R².",
        "step_by_step": [
          {
            "step": 1,
            "title": "Recall the VIF formula",
            "content": "For predictor j, VIF_j = 1 / (1 − R_j²), where R_j² is from regressing X_j on the other predictors.",
            "latex": "VIF_j = \\frac{1}{1 - R_j^2}",
            "diagram": null,
            "diagram_type": "None"
          }
        ],
        "interpretation": "Plugging in R² = 0.75 gives VIF = 1 / 0.25 = 4, indicating moderate multicollinearity.",
        "business_context": "This tells the analyst that uncertainty in the estimated effect of online ads is four times larger than if it were uncorrelated with TV and radio spend."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_30",
      "tags": [
        "VIF",
        "formula",
        "auxiliary regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In the process of computing VIF for a predictor X_j, what does a very high R_j² (close to 1) in the auxiliary regression indicate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "X_j can be almost perfectly predicted from the other predictors.",
        "B": "X_j has almost no relationship with the other predictors.",
        "C": "The response variable is being predicted very accurately.",
        "D": "There is no need to compute VIF because collinearity is absent."
      },
      "correct_answer": [
        "A"
      ],
      "explanation": {
        "text": "A is correct because a high R_j² means that X_j is largely explained by a linear combination of the other predictors, signaling strong collinearity. B, C, and D contradict what a high R_j² in the auxiliary regression implies.",
        "step_by_step": [],
        "interpretation": "When R_j² is high, VIF becomes large and the coefficient of X_j in the main model will be imprecisely estimated.",
        "business_context": "If ‘website visits’ is almost determined by ‘time on site’ and ‘products viewed’, then its separate effect on spending is hard to distinguish."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_30",
      "tags": [
        "VIF computation",
        "R-squared",
        "multicollinearity"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which of the following best summarizes the step-by-step procedure to compute the VIF for a single predictor X_j?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Regress Y on X_j, square the correlation, and subtract from 1.",
        "B": "Regress X_j on all other predictors, obtain R_j², and compute 1 / (1 − R_j²).",
        "C": "Regress Y on all predictors, take the overall R², and compute 1 / (1 − R²).",
        "D": "Regress X_j on Y, obtain R², and multiply by the number of predictors."
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because VIF_j is defined using the R_j² from regressing X_j on the other predictors, then applying VIF_j = 1 / (1 − R_j²). A, C, and D use the wrong response variable or the wrong R².",
        "step_by_step": [],
        "interpretation": "The auxiliary regression isolates how redundant X_j is given the rest of the predictors.",
        "business_context": "This procedure lets a marketing analyst see whether ‘online spend’ is largely redundant with ‘offline spend’ and other channels."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_30",
      "tags": [
        "VIF computation",
        "process",
        "auxiliary regression"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are common mistakes when computing or interpreting VIF using auxiliary regressions? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Using the response Y instead of X_j as the dependent variable in the auxiliary regression.",
        "B": "Regressing X_j on only one other predictor instead of all other predictors.",
        "C": "Interpreting a high VIF as evidence that X_j is unimportant rather than hard to estimate.",
        "D": "Computing a separate VIF for each predictor using its own auxiliary regression."
      },
      "correct_answer": [
        "A",
        "B",
        "C"
      ],
      "explanation": {
        "text": "A, B, and C are correct because they reflect procedural and interpretive errors: wrong dependent variable, omitting other X’s from the auxiliary regression, and misreading high VIF as unimportance. D is actually correct practice—each predictor has its own VIF—so it is not a mistake.",
        "step_by_step": [],
        "interpretation": "Correct VIF computation requires the right auxiliary setup and careful interpretation of what high VIF really means.",
        "business_context": "Avoiding these mistakes helps ensure that diagnostics for multicollinearity in revenue or churn models are trustworthy."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_30",
      "tags": [
        "VIF computation",
        "common mistakes",
        "auxiliary regression",
        "MCA"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "In a multiple regression model used for business decision-making, what does a high Variance Inflation Factor (VIF) for a predictor mainly indicate?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "The predictor has a very strong and reliable effect on the response",
        "B": "The predictor is highly correlated with other predictors, inflating the variance of its coefficient",
        "C": "The overall model has poor fit, as shown by a low R-squared",
        "D": "The predictor has many missing values, increasing the standard error"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because a high VIF means the predictor is strongly correlated with other predictors, which inflates the variance (and standard error) of its coefficient and makes it unstable. A is wrong because high VIF does not guarantee a reliable effect; it actually warns that the estimate is less trustworthy.",
        "step_by_step": [],
        "interpretation": "High VIF is a red flag about multicollinearity, not about the size or importance of the effect itself.",
        "business_context": "If a key budget variable has high VIF, managers should be cautious about trusting its individual coefficient when making allocation decisions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_31",
      "tags": [
        "VIF interpretation",
        "multicollinearity",
        "regression diagnostics"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Why does a very high VIF make it harder for a business analyst to interpret an individual regression coefficient?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Because high VIF automatically makes the coefficient equal to zero",
        "B": "Because high VIF means the coefficient’s standard error is inflated, so its estimate and p-value are unstable",
        "C": "Because high VIF forces the analyst to change the response variable",
        "D": "Because high VIF means the model violates the assumption of linearity"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because VIF directly reflects how much the variance (and thus standard error) of a coefficient is inflated by multicollinearity, making estimates and p-values unstable. A is incorrect since VIF does not set coefficients to zero; it just makes them noisy and harder to trust.",
        "step_by_step": [],
        "interpretation": "Unstable coefficients mean the estimated impact of a predictor could change a lot with new data, reducing confidence in business interpretations.",
        "business_context": "When planning marketing spend, unstable coefficients for media channels make it risky to reallocate budgets based on those numbers."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_31",
      "tags": [
        "standard error",
        "VIF interpretation",
        "p-values"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "A company finds that TV and online advertising spend each have very high VIF values, but a combined 'total advertising spend' variable has a low VIF. What is the most appropriate interpretation?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "TV and online spend are nearly uncorrelated with each other",
        "B": "The separate channel effects are hard to estimate, but the overall advertising effect is more stable",
        "C": "The regression model must be completely discarded",
        "D": "Total advertising spend cannot be used for prediction"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because high VIF for TV and online indicates multicollinearity between channels, while the low VIF for total spend means the combined effect is estimated more precisely. A is wrong because high VIF actually suggests strong correlation, not independence.",
        "step_by_step": [],
        "interpretation": "When channels move together, it may be easier and more reliable to interpret their joint impact than their separate impacts.",
        "business_context": "Managers might base budget decisions on the total advertising effect rather than trying to over-interpret noisy channel-specific coefficients."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_31",
      "tags": [
        "model selection",
        "VIF interpretation",
        "marketing mix"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mcq",
      "question_text": "Which best describes how VIF should be used in model selection for business analysis?",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Always drop any variable with VIF above a fixed cutoff, regardless of its business importance",
        "B": "Use VIF as one diagnostic, balancing multicollinearity concerns with the strategic relevance of predictors",
        "C": "Ignore VIF entirely if the sample size is large",
        "D": "Use VIF only to decide the value of the intercept term"
      },
      "correct_answer": [
        "B"
      ],
      "explanation": {
        "text": "B is correct because VIF is a diagnostic tool that should be interpreted alongside business priorities and model goals, not used mechanically. A is incorrect because automatically dropping high-VIF variables can remove strategically important predictors and introduce bias.",
        "step_by_step": [],
        "interpretation": "Good model selection blends statistical diagnostics with subject-matter judgment.",
        "business_context": "A key pricing variable with high VIF might be kept and handled carefully rather than blindly dropped, because it is central to revenue decisions."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_31",
      "tags": [
        "model selection",
        "business decisions",
        "VIF usage"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    },
    {
      "type": "mca",
      "question_text": "Which of the following are reasonable actions when you detect high VIFs for some predictors in a business regression model? (Select all that apply)",
      "question_visual": null,
      "question_visual_type": "None",
      "options": {
        "A": "Consider combining highly correlated predictors into a single composite variable",
        "B": "Evaluate whether the affected predictors are essential for the business question before dropping them",
        "C": "Automatically delete all predictors with high VIF without any further consideration",
        "D": "Recognize that if your primary goal is prediction, you may tolerate somewhat higher VIFs than for causal interpretation"
      },
      "correct_answer": [
        "A",
        "B",
        "D"
      ],
      "explanation": {
        "text": "A, B, and D are correct: combining variables can reduce VIF; checking business importance prevents dropping critical predictors; and prediction-focused models can sometimes live with higher VIF than explanatory models. C is incorrect because blindly deleting high-VIF variables ignores strategic relevance and can harm the model.",
        "step_by_step": [],
        "interpretation": "VIF suggests where multicollinearity is a problem, but the remedy must respect the model’s purpose and business context.",
        "business_context": "For channel attribution (explanatory), you may need lower VIFs; for forecasting total sales, a model with higher VIFs can still be very useful."
      },
      "difficulty_level": 1,
      "source_flashcard_id": "DAA_lec_5_31",
      "tags": [
        "VIF interpretation",
        "model goals",
        "predictive vs explanatory"
      ],
      "visual_type": "None",
      "visual_code": "",
      "alt_text": ""
    }
  ]
}